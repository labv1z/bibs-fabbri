@TechReport{Fua95f,
author =      "P. Fua and C. Brechbuehler",
title =       {Imposing Hard Constraints on Soft Snakes},
institution = "Artificial Intelligence Center, SRI International",
year        = "1995",
type        = "Tech Note",
number      = "553",
month       = "October",
keywords    = "snakes, deformable models, constrained optimization, consistency",
windexkey   = "sri aic technical note notes",
url         = "ftp://ftp.ai.sri.com/pub/tech-notes/1995/aic-tn-553.ps.gz"
}

@TechReport{Fua94g,
author =      "P. Fua",
title =       {Reconstructing Complex Surfaces  from Multiple Stereo Views},
institution = "Artificial Intelligence Center, SRI International",
year        = "1994",
number      = "550",
month       = "November",
keywords    = "stereo, shape from shading, multiple images, data fusion, object-centered representations, surface reconstruction",
url         = "ftp://ftp.ai.sri.com/pub/tech-notes/1993/aic-tn-550.ps.gz"
}
		  


@TECHREPORT{Kameyama94b:Stressed,
AUTHOR="Kameyama, Megumi",
TITLE="Stressed and Unstressed Pronouns: Complementary Preferences",
TYPE="Technical Note",
INSTITUTION="SRI",
ADDRESS="Menlo Park, CA",
MONTH=aug,
YEAR=1994,
NUMBER=545,
ABSTRACT="I present a unified account of interpretation preferences of stressed
and unstressed pronouns in discourse. The central intuition is the
Complementary Preference Hypothesis that predicts the interpretation
preference of a stressed pronoun from that of an unstressed pronoun
in the same discourse position. The base preference must be computed
in a {\em total} pragmatics module including commonsense preferences.
The focus constraint in Rooth's theory of semantic focus is
interpreted to be the salient subset of the domain in the local
attentional state in the discourse context independently motivated for
other purposes in the centering theory.",
ANNOTE="In Bosch,
Peter and Rob van der Sandt, eds., the Proceedings of the Focus and
NLP Conference, Germany, June, 1994 (one error corrected 3--95)",
URL="ftp://ftp.ai.sri.com/pub/tech-notes/1994/aic-tn-545.ps.gz",
}

@TECHREPORT{Kameyama94:Indefeasible,
AUTHOR="Kameyama, Megumi",
TITLE="Indefeasible Semantics and Defeasible Pragmatics",
TYPE="Technical Note",
INSTITUTION="SRI",
ADDRESS="Menlo Park, CA",
MONTH=aug,
YEAR=1994,
NUMBER=544,
ABSTRACT="An account of utterance interpretation in discourse needs to face the
issue of how the discourse context controls the space of interacting
preferences. 
In the framework of a general discourse processing model that
integrates both the grammar and pragmatics subsystems, I propose
a fine structure of the preferential interpretation in pragmatics in
terms of defeasible rule interactions. The pronoun interpretation
preferences that serve as the empirical ground draw from the survey
data specifically obtained for the present purpose. A logical
implementation of the preferential rule interactions is proposed using
prioritized circumscription, a nonmonotonic reasoning formalism in AI.",
URL="ftp://ftp.ai.sri.com/pub/tech-notes/1994/aic-tn-544.ps.gz",
ANNOTE="Also a CWI Technical Report CS-R9441",
}

		  
@techreport{AIC-TN-1993:535,
keywords={Stereo, Surface Reconstruction},
type =		"Technical Note",
number = 	535,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Object-Centered Surface Reconstruction:  Combining Multi-Image Stereo and Shading",
author = 	"Pascal Fua and Yvan G. Leclerc",
month = 	"SEPTEMBER",
year = 		1993,
abstract = 	"Our goal is to reconstruct both the shape and reflectance properties of surfaces from multiple images.  We argue that an object-centered representation is most appropriate for this purpose becasue it naturally accomodates multiple sources of data, multiple images (including motion sequences of a rigid object), and self-occlusions. We then present a specific object-centered reconstruction method and its implementation.  The method begins with an initial estimate of surface shape provided by triangulating the result of conventional stereo or other means.  The surface shape and reflectance properties are then iteratively adjusted to minimize an objective function that combines information from multiple input images.  The objective function is a weighted sum of ``Stereo,'' shading, and smoothness components, where the weight varies over the surface.  For example, the stereo componenet is weighted more strongly where the surface projects onto highly textured areas in the images, and less strongly otherwise. Thus, each component has its greatest influence where its accuracy is likely to be greatest.  Experimental results on both synthetic and real images are presented. "
}

@techreport{AIC-TN-1993:534,
keywords={SOCAP, SIPE-2},
type =		"Technical Note",
number = 	534,
price = 	"$12.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Applying An Ai Planner To Military Operations Planning",
author = 	"David E. Wilkins and Roberto V. Desimone",
month = 	"JANUARY",
year = 		1993,
abstract = 	"This paper describes a prototype system for quickly developing joint military courses of action.  The system, SOCAP (System for Operations Crisis Action Planning), combines a newly extended version of an AI planning system, SIPE-2 (System for Interactive Planning and Execution), with a color map display and applies this technology to military operations planning.  This paper describes the SOCAP problem domain, how SIPE-2 was used to address this problem, and the strengths and weaknesses of our approach. "
}

@techreport{AIC-TN-1993:533,
keywords={AI Planner, Military Operations},
type =		"Technical Note",
number = 	533,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"A Multivalued Logic Approach To Integrating Planning Control",
author = 	"Alessandro Saffiotti and Kurt Konolige and Enrique Ruspini",
month = 	"JUNE",
year = 		1993,
abstract = 	"This paper describes a prototype system for quickly developing joint military courses of action.  The system, SOCAP (System for Operations Crisis Action Planning and Execution), with a color map display and applies this technology to military operations planning.  This paper describes the Socap problem domain, how SIPE-2 was used to address this problem, and the strengths and weaknesses of our approach. "
}

@techreport{AIC-TN-1993:532,
keywords={Plan Generation, Reactive Execution},
type =		"Technical Note",
number = 	532,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"A Common Knowledge Representation For Plan Generation  and Reactive Execution",
author = 	"David E. Wilkins",
month = 	"JUNE",
year = 		1993,
abstract = 	"This paper describes the ACT formalism, which is designed to encode the knowledge required to support both the generation of complex plans and reactive execution of those plans in dynamic environments.  ACT is an heuristically adequate representation that is useful in practical applications, and serves as an interlingua for Artificial Intelligence (AI) technologies in planning and reactive control.  The design of the formalism is discussed and example uses from practical applications are presented.  These applications show that the ACT representational constructs have reasonable computational properties as well as being adequately expressive. "
}

@techreport{AIC-TN-1993:531,
keywords={Evidential Problems},
type =		"Technical Note",
number = 	531,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Reframing Evidential Problems",
author = 	"Leonard Wesley",
month = 	"APRIL",
year = 		1993,
abstract = 	"Real world problems are rarely solved in one fell swoop.  Rather, problem solving is an iterative process that involves acquiring and interpreting environmental information, then deciding what to do next.  One alternative action might involve adjusting one's problem solving questions which a system must answer to find an acceptable solution. The desirability of pursuing such an alternative follows from the view that solving a problem often requires being able to identify and answer the ``right'' questions.  In evidential reasoning (ER) systems, frames of discernments represent possible answers to the questions at hand.  Asking a new question is equivalent to modifying an existing frame of discernment or constructing a new one, called ``reframing evidential problems'' here.  In this paper, we describe a novel approach to deciding how to reframe problems based on minimizing an entropy characterization of the current situational knowledge. Currently, evidential knowledge-based systems (KDS) employ static frames; a user must modify frames if they are inadequate for the desired domain of application.  Having some automated capability to make even simple adjustments to frames is expected to provide increased autonomy to KBS.  An example of how the approach might be used in the domain of high-level computer vision is presented. "
}

@techreport{AIC-TN-1993:530,
keywords={Locative Reasoning},
type =		"Technical Note",
number = 	530,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Toward A Theory Of Evidential-Based Locative Reasoning",
author = 	"Leonard Wesley",
month = 	"APRIL",
year = 		1993,
abstract = 	"We describe efforts to develop a novel evidential theory of autonomous locative reasoning that is intended to address the computational burden of using Cartesian maps, and reduce the need to instrument environments.  An example of a set of experiments being conducted and results that form the basis of our theory is provided. "
}

@techreport{AIC-TN-1993:529,
keywords={Flakey, Fuzzy Controller},
type =		"Technical Note",
number = 	529,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"A Fuzzy Controller For Flakey, An Autonomous Mobile Robot",
author = 	"Alessandro Saffiotti and Enrique Ruspini and Kurt Konolige",
month = 	"APRIL",
year = 		1993,
abstract = 	"Controlling the movement of an autonomous mobile robot in real-world unstructured environments requires the ability to pursue strategic goals under conditions of uncertainty, incompleteness, and imprecision.  We describe a fuzzy controller for such a mobile robot that can take multiple strategic goals into consideration.  Through the use of fuzzy logic, goal-oriented behavior (e.g., trying to reach a given location) and reactive behavior (e.g., avoiding previously unknown obstacles on the way) are smoothly blended into one sequence of control actions.  The fuzzy controller has been implemented on the SRI robot Flakey, and its performance demonstrated in several different environments, including the first AAAI robotic competition, where Flakey placed second. "
}

@techreport{AIC-TN-1993:528,
keywords={Flakey, AAAI Robot Competition},
type =		"Technical Note",
number = 	528,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Flakey In Action:  The 1992 Aaai Robot Competition",
author = 	"Kurt Konolige and Karen Myers and Enrique Ruspini and Alessandro Saffiotti",
month = 	"APRIL",
year = 		1993,
abstract = 	"Flakey is an autonomous mobile robot developed at the Artificial Intelligence Center of SRI International.  This document describes Flakey's architecture, control mechanisms and perceptual capabilities at one particular milestone:  Flakey's participation in the first AAAI International Robotic Competition in 1992.  Much of Flakey's software was designed for general navigation tasks in office-buildings but was readily adapted for use in the competition.  While most competitors modified the environment in some way to simplify the problems of self-localization and object recognition, Flakey also distinguished itself through its ability to navigate gracefully while avoiding obstacles and people, employing techniques based on fuzzy logic.  The effectiveness of Flakey's software design was validated by second place finishes in both stages of the competition. "
}

@techreport{AIC-TN-1993:527,
keywords={Gemini, Spoken Language},
type =		"Technical Note",
number = 	527,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Gemini:  A Natural Language System For Spoken-Language Understanding",
author = 	"John Dowding and Jean Mark Gawron and Doug Appelt and
		  John Bear and Lynn Cherny and Robert Moore and Douglas Moran",
month = 	"APRIL",
year = 		1993,
abstract = 	"Gemini is a natural language understanding system developed for spoken language applications.  The paper describes the architecture of Gemini, paying particular attention to resolving the tension between robustness and overgeneration.  Gemini features a broad-coverage unification-based grammar of English, fully interleaved syntactic and semantic processing in an all-paths, bottom-up parser, and an utterance-level parser to find interpretations of sentences that might not be analyzable as complete sentences.  Gemini also includes novel components for recognizing and correcting grammatical disfluencies, and for doing parse preferences.  This paper presents a component-by-component view of Gemini, providing detailed relevant measurements of size, efficiency, and performance. "
}

@techreport{AIC-TN-1993:526,
keywords={Scene Representation, Image Understanding},
type =		"Technical Note",
number = 	526,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Building and Using Scene Repesentations In Image Understanding",
author = 	"H. Harlyn Baker",
month = 	"SEPTEMBER",
year = 		1993,
abstract = 	"The task of having computers able to understand their enviornments
through direct imaging has proved to formidable.  With its beginnings
about 30 years ago, the field of computer vision has grown as a major
part fot he pursuit for artificial intelligence.  Most elements of
this pursuit - language understanding, reasoning and planning, speech
- are very difficult challenges, but vision, with its high
dimensionality of space, time, scale, color,dynamics, and so forth,
may be the most challenging.  Early attempts to develop computer
sivion focused on restricted situations in which it was feasible to
provide the computer with fairly complete descriptions of what it
would encounter.  In such cases, single images provided the sensory
information for analysis.  As the domains of application grew, the
requirements for more competent descriptions of the world increased.
Dealing with three-dimensional (3D) dynamic structures (the real
world) from 3D dynamic platforms (we humans) calls for greater
capabilities on both the analysis and synthesis sides of the issue.
The anlaysis side is the processing of sensory data for such tasks as
recognition and navigation, and a number of techniques are discussed
here for dealing with these two-, three-, and higher-dimensional data.
The synthesis side is the construction of ``internal'' descriptions of
what they may be used subsequently for the above tasks.  This latter
issue is the underlying theme we pose in this paper - developing
representations from vision that will later enable effective automated
operation in our 3D dynamic environments."
}

@techreport{AIC-TN-1993:525,
keywords={Deduction and Abduction, Meta-Interpretation},
type =		"Technical Note",
number = 	525,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Upside-Down Meta-Interpretation Of The Model Elimination Theorem-Proving Procedure For Deduction and Abduction",
author = 	"Mark Stickel",
month = 	"APRIL",
year = 		1993,
abstract = 	"Typical bottom-up, forward-chaining reasoning systems such as hyperresolution lack goal-directedness while typical top-down, backward-chaining reasoning systems like Prolog or model elimination repeatedly solve the same goals.  Reasoning systems that are goal-directed and avoid repeatedly solving the same goals can be constructed by formulating the top-down methods metatheoretically for execution by a bottom-up reasoning system (hence, we use the term upside-down meta-interpretation).  This formulation also facilitates the use of flexible search strategies, such as merit-ordered search that are common to bottom-up reasoning systems.  The model elimination theorem-proving procedure, its extension by an assumption rule for abduction, and its restriction to Horn clauses are adapted here for such upside-down meta-interpretation.  This work can be regarded as an extension of the magic-sets or Alexander method for query evaluation in deductive databases to both non-Horn clauses and abductive reasoning. "
}

@techreport{AIC-TN-1993:524,
keywords={Hybrid Reasoning, Universal Attachment},
type =		"Technical Note",
number = 	524,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Hybrid Reasoning Using Universal Attachment",
author = 	"Karen Myers",
month = 	"MARCH",
year = 		1993,
abstract = 	"Hybrid representation frameworks provide a powerful basis for constructing intelligent systems. Universal attachement is a domain-independent mechanism for integrating diverse representation and reasoning methods into hybrid frameworks that contain a subsystem based on deduction over logical formulas.  Although based on the same principles as previous attachment methods, universal attachment provides a much broader range of connections between general-purpose deduction and specialized representation and reasoning techniques.  This paper defines a formal inference rule of universal attachment and discusses the properties of soundness, completeness and correctness for this rule.  The relationship between universal attachment and other integration techniques is explored.  Finally, policies based on experimentation with an implemented universal attachment system are presented that lend guidance in exploiting the expanded representational and interential capabilities that hybrid systems provide.   "
}

@techreport{AIC-TN-1993:523,
keywords={Deduction!Model Generation},
type =		"Technical Note",
number = 	523,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"\=Automated Theorem-Proving Research In The Fifth Generation Computer Systems Project:  Model Generation Theorem Provers",
author = 	"Mark E. Stickel",
month = 	"MARCH",
year = 		1993,
abstract = 	"One of the successful outcomes of the Fifth Generation Computer Systems Project is the development of Model Generation Theorem Provers (MGTPs).  MGTPs have solved previously open problems in finite algebra, produced rapid proofs of condensed detachment problems, and are providing an inferential infrastructure for knowledge-processing research at ICOT.  They successfully exploit the Fifth Generation Project's KL1 logic programming language and parallel inference machines to achieve high performance and parallel speedup.  This paper describes some of the key properties of MGTPs, reasons for their successes, and possible areas for future improvement.  "
}

@techreport{AIC-TN-1993:522,
keywords={Speech!Repairs},
type =		"Technical Note",
number = 	522,
price = 	"$7.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"A System For Labeling Self-Repairs In Speech",
author = 	"John Bear and John Dowding and Elizabeth Shriberg",
month = 	"FEBRUARY,",
year = 		1993,
abstract = 	"This document outlines a system for labeling self-repairs in spontaneous speech.  The system marks the location and extent of a repair, as well as relevant words in the region of the repair. Together these labels determine the relationship between the ``error'' and the hypothesized ``correction.''  The system is designed to be able to capture distinctions among different repair patterns while remaining easy to learn, apply, and integrate into existing transcription formats.  Although the system was originally developed to aid our research on automatic detection and correction of repairs (Shriberg, Bear, Dowding, 1992; Bear, Dowding, Shriberg, 1992), we hope that it may also prove useful for annotation of spontaneous speech data in related fields. "
}

@techreport{AIC-TN-1992:521,
keywords={GRASPER-CL, Graph Management System},
type =		"Technical Note",
number = 	521,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Grasper-Cl Graph Management System",
author = 	"Peter D. Karp, John D. Lowrance, Thomas M. Strat, and David E. Wilkins",
month = 	"JANUARY,",
year = 		1992,
abstract = 	"Grasper-CL [5] is a COMMON LISP system for manipulating and displaying graphs, and for building graph-based user interfaces for application programs.  The system represents a significant advance over previous COMMON LISP graphers because each level of the Grasper-CL  architecture---from the core graph data structures to the interactive display module---has been fully developed and articulated, and is accessible to application programmers.  We call this system organization an open architecture.  In our experience, several different classes of graph-based user interfaces exist.  For example, one class produces static drawings of graphs, whereas another class requires extensive user interaction with graph drawings.  The open architecture of Grasper-CL supports the development of all classes of interfaces, whereas previous graphers support only one or two classes of interfaces.  Grasper-CL graphics operations are implemented using CLIM, the COMMON LISP Interface Manager. "
}

@techreport{AIC-TN-1992:520,
keywords={Knowledge Representation!Frame Representation Systems, Frame Knowledge Representation Systems},
type =		"Technical Note",
number = 	520,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Design Space Of Frame Knowledge Representation Systems",
author = 	"Peter Karp",
month = 	"OCTOBER,",
year = 		1992,
abstract = 	"In the past 20 years, AI researchers in knowledge representation (KR) have implemented over 50 frame knowledge representation systems (FRSs).  KR researchers have explored a large space of alternative FRS designs.  This paper surveys the FRS design space in search of design principles for FRSs.  The FRS design space is defined by the set of alternative features and capabilities -- such as the representational constructs -- that an FRS designer might choose to include in a particular FRS, as well as the alternative implementations that might exist for a particular feature.  The paper surveys the architectural variations explored by different system designers for the frame, the slot, the knowledge base, for access-oriented programming, and for object-oriented programming.  We find that few design principles exist to guide an FRS designer as to how particular design decisions will affect qualities of the resulting FRS, such as its worst-case and average-case theoretical complexity, its actual performance on real-world problems, the expressiveness and succinctness of the representation language, the runtime flexibility of the FRS, the modularity of the FRS, and the effort required to implement the FRS. "
URL="ftp://ftp.ai.sri.com/pub/papers/karp-freview.ps.Z",
}

@techreport{AIC-TN-1992:519,
keywords={FASTUS, Natural Language!FASTUS, Natural Language!Information Extraction},
type =		"Technical Note",
number = 	519,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Fastus:  A System For Extracting Information From Natural-Language Text",
author = 	"Jerry R. Hobbs",
month = 	"NOVEMBER,",
year = 		1992,
abstract = 	"FASTUS is a system for extracting information from free text in English, and potentially other languages as well, for entry into a database, and potentially for other applications.  It works essentially as a cascaded, nondeterministic finite state automaton. There are four steps in the operation of FASTUS.  In Step 1 sentences are scanned for certain trigger words to determine whether further processing should be done.  In Step 2 noun groups, verb groups, and prepositions and some other particles are recognized.  The input to Step 3 is the sequence of phrases recognized in Step 2; patterns of interest are identified in Step 3 and corresponding ``incident structures'' are built up.  In Step 4 incident structures that derive from the same incident are identified and merged, and these are used in generating database entries.  FASTUS is an order of magnitude faster than any comparable system; it can process a news report in an average of less than eleven seconds.  This translates directly into fast development time.  In the three and a half weeks between its first use and the MUC-4 evaluation in May 1992, we were able to build up its domain knowledge to a point where it was among the leaders in the evaluation.  "
}

@techreport{AIC-TN-1992:518,
keywords={Speech!Repairs},
type =		"Technical Note",
number = 	518,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Detection and Correction Of Repairs In Human-Computer Dialog",
author = 	"John Bear and John Dowding and Elizabeth Shriberg",
month = 	"MAY",
year = 		1992,
abstract = 	"We have analyzed 607 sentences of spontaneous human-computer speech data containing repairs, drawn from a total corpus of 10,718 sentences.  We present here criteria and techniques for automatically detecting the presence of a repair, its location, and making the appropriate correction.  The criteria involve integration of knowledge from several sources:  pattern matching, syntactic and semantic analysis, and acoustics. "
}

@techreport{AIC-TN-1992:517,
keywords={Planning!Use of Plans},
type =		"Technical Note",
number = 	517,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Use Of Plans",
author = 	"Martha Pollack",
month = 	"MARCH",
year = 		1992,
abstract = 	"I argue that, contrary to the challenges raised in the work of Brooks and others, intelligent agents are essentially planning agents.  I claim that philosophical and commonsense psychological theorizing about the process of planning can provide useful insights into the question of agent design.  The theories I discuss are not restricted to how agents can form plans, but also concern the ways in which agents use their plans.  Specifically, I describe the use of plans to guide action as well as to control reasoning and to enable inter-agent coordination.  These uses of plans make possible intelligent behavior in complex, dynamic, multiagent environments. This paper is a slightly revised transcription of a lecture presented upon receipt of the Computers and Thought Award at the 12th International Joint Conference on Artificial Intelligence in Sydney, Australia, on August 26, 1991. "
}

@techreport{AIC-TN-1991:516,
keywords={Natural Language!Japanese, JLE},
type =		"Technical Note",
number = 	516,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Syntax and Semantics Of The Japanese Language",
author = 	"Megumi Kameyama",
month = 	"MAY",
year = 		1991,
abstract = 	"The paper describes the basic syntactic rules in the Japanese Language Engine (JLE), a unification-based Japanese grammar used as part of a spoken Japanese interpretation system.  The underlying assumption in this grammar is the Nondefeasibility Thesis --- the rules of grammar are nondefeasible, and all defeasible rules belong to pragmatics.  In an overall understanding of an utterance in discourse, such a grammar module can have a meaningful functionality by giving each utterance a set of context-independent underspecified interpretations under the assumption that the module of pragmatic inferences is responsible for further elaborations of the interpretations.  The JLE grammar developed with this perspective is robust and exception-free.  The major syntactic phenomena of Japanese discussed include: argument omissibility, flexible word order, relativization, topicalization, and quantifier floating. "
URL="ftp://ftp.ai.sri.com/pub/tech-notes/1992/aic-tn-516.ps.gz",
}

@techreport{AIC-TN-1992:515,
type =		"Technical Note",
number = 	515,
price = 	"$7.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Overview Of The Sri Cartographic Modeling Environment",
author = 	"Andrew J. Hanson and Lynn Quam",
month = 	"JANUARY",
year = 		1992,
abstract = 	"The SRI Cartographic Modeling Environment has been created to support research on interactive, semiautomated, and automated computer-based cartographic activities.  The underlying image manipulation capabilities are provided by the SRI ImagCalc$^{TM}$ system.  The cartographic features and data that can be entered include multiple images, camera models, digital terrain elevation data, point, line, and area cartographic features, and a wide assortment of three-dimensional objects.  Interactive capabilities include free-hand feature entry, altering features while constraining them to conform to the terrain and lighting geometry, adjustment of feature parameters, and the adjustment of the camera model to display the scene features from arbitrary viewpoints.  Cartographic features are depictable either as wire-frame sketches for interactive purposes or as texture-mapped renderings for realistic scene synthesis.  High-quality simulated scenes are created by texture-mapped images onto terrain data and adding renderings of cartographic features using depth-buffering and antialiasing techniques.  Motion sequences can be created by choosing a series of camera models and rendering the simulated appearance of the scene from each viewpoint. "
}

@techreport{AIC-TN-1991:514,
keywords={Natural Language!As an Interface, Interface!Natural Language, Shoptalk},
type =		"Technical Note",
number = 	514,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Role Of Natural Language In A Multimodal Interface",
author = 	"Philip R. Cohen",
month = 	"NOVEMBER",
year = 		1991,
abstract = 	"Although graphics and direct manipulation are effective interface technologies for some classes of problems, they are limited in many ways.  In particular, they provide little support for identifying objects not on the screen, for specifying temporal relations, for identifying and operating on large sets and subsets of entities and for using the context of interaction.  On the other hand, these are precisely strengths of natural language.  This paper presents an interface that blends natural language processing and direct manipulation technologies, using each for their characteristic advantages.  Specifically, the paper shows how to use natural language to describe objects and temporal relations, and how to use direct manipulation for overcoming hard natural language problems involving the establishment and use of context and pronominal reference.  This work has been implemented in SRI's Shoptalk system, a prototype information and decision-support system for manufacturing. "
}

@techreport{AIC-TN-1991:513,
keywords={Theorem Proving|see{Deduction}, Deduction!Model Elimination, Deduction!Caching},
type =		"Technical Note",
number = 	513,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Caching and Lemmaizing In Model Elimination Theorem Provers",
author = 	"Owen L. Astrachan and Mark E. Stickel",
month = 	"DECEMBER",
year = 		1991,
abstract = 	"Theorem provers based on the model elimination theorem-proving procedure have exhibited extremely high inference rates but have lacked a redundancy control mechanism such as subsumption.  In this paper we report on work done to modify a model elimination theorem prover using two techniques, caching and lemmaizing, that have reduced by more than an order of magnitude the time required to find proofs of several problems and that have enabled the prover to prove theorems previously unobtainable by top-down model elimination theorem provers. "
}

@techreport{AIC-TN-1991:511,
keywords={TACITUS!MUC-3},
type =		"Technical Note",
number = 	511,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"The Tacitus System: The Muc-3 Experience",
author = 	"Jerry Hobbs, Douglas E. Appelt, John S. Bear, Mabry Tyson, and David Magerman",
month = 	"OCTOBER",
year = 		1991,
abstract = 	"It is often assumed that when natural language processing meets the real world, the idea of aiming for complete and correct interpretations has to be abandoned.  However, our experience with TACITUS, especially in the MUC-3 evaluation, has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with methods for achieving robustness.  We describe and evaluate a method for dealing with unknown words and a method for filtering out sentences irrelevant to the task.  We describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser, a recovery technique for failed parses, and a new technique called terminal sub-string parsing.  For pragmatics processing, we describe how the method of abductive inference is inherently robust, in that an interpretation is always possible, so that in the absence of the required world knowledge, performance degrades gracefully. Each of these techniques has been evaluated, and the results of the evaluations are presented.  "
}

@techreport{AIC-TN-1991:510,
keywords={Logic!Autoepistemic},
type =		"Technical Note",
number = 	510,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Quantification In Autoepistemic Logic",
author = 	"Kurt Konolige",
month = 	"SEPTEMBER",
year = 		1991,
abstract = 	"Quantification in modal logic is interesting from a technical and philosophical stand-point.  Here we look at quantification in autoepistemic logic, which is a modal logic of self-knowledge.  We propose several different semantics, all based on the idea that having beliefs about an individual amounts to having a belief using a certain type of name for the individual. "
}

@techreport{AIC-TN-1991:509,
keywords={Game-playing!Chess, PARADISE},
type =		"Technical Note",
number = 	509,
price = 	"$15.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Working Notes On Paradise Chess Patterns",
author = 	"David E. Wilkins",
month = 	"AUGUST",
year = 		1991,
abstract = 	"This report contains the patterns used in PARADISE, a knowledge-based chess program written at Stanford University in the late 1970s.  This report contains primarily data and is intended for people writing pattern-based game-playing programs who wish to know the details of the patterns in PARADISE. "
}

@techreport{AIC-TN-1991:508,
keywords={Natural Language!Abduction|see{Reasoning!Abduction}, Natural Language!Metaphor, Natural Language!Abduction, Natural Language!Interpretation},
type =		"Technical Note",
number = 	508,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Metaphor and Abduction",
author = 	"Jerry R. Hobbs",
month = 	"AUGUST",
year = 		1991,
abstract = 	"In this paper a recent approach to inference in text understanding based on abduction is applied to the problem of metaphor interpretation.  The fundamental ideas in the ``interpretation as abduction'' approach are outlined.  A succinct characterization of interpretation is given, along with a brief example and a description of the principal features of a weighted abduction scheme that is used. The approach is shown to lead to an elegant integration of syntax, semantics, and pragmatics.  Three examples of metaphor interpretation are then analyzed within the abductive framework to determine what problems arise.  The examples are a conventionalized metaphor schema, a standard category metaphor contextually interpreted, and a novel metaphor.  The primary problem that arises for metaphor interpretation in the abductive framework is dealing with the fact that metaphors are not literally true.  Two perspectives on this difficulty are offered. "
}

@techreport{AIC-TN-1991:507,
keywords={Shoptalk, Simulation!Interfaces, Decision Support!Interfaces, Interface!Integrated},
type =		"Technical Note",
number = 	507,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Integrated Interfaces For Decision-Support With Simulation",
author = 	"Philip R. Cohen",
month = 	"JUNE",
year = 		1991,
abstract = 	"A major limitation of graphical user interfaces for simulation is that users such as managers and decision makers need to know too much.  We examine the weaknesses inherent in graphical user interfaces to support these users of simulation for short-term situation assessment and scenario evaluation, a style of problem solving characteristic of military and factory command-and-control. Then, we present Shoptalk, a factory command-and-control system with an interface integrating direct manipulation and natural-language processing and demonstrate how the Shoptalk style of interaction can overcome these limitations. "
}

@techreport{AIC-TN-1991:506,
keywords={Vision!Feature Extraction, Vision!Cartography},
type =		"Technical Note",
number = 	506,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Sri Image Understanding Research Cartographic Feature Extraction",
author = 	"Lynn H. Quam",
month = 	"JUNE",
year = 		1991,
}

@techreport{AIC-TN-1991:505,
keywords={Reasoning!Abduction!vs Closure},
type =		"Technical Note",
number = 	505,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Abduction Vs. Closure In Causal Theories",
author = 	"Kurt Konolige",
month = 	"APRIL",
year = 		1991,
abstract = 	"There are two distinct formalizations for reasoning from observations to explanations, as in diagnostic tasks.  The consistency based approach treats the task as a deductive one, in which the explanation is deduced from a background theory and a minimal set of abnormalities.  In the other treatment, based on abduction, the explanations are considered to be sentences that, when added to the background theory, account for the observations.  We show that there is a close connection between these two formalizations.  Starting with a causal theory, explanations can be generated either by abductive reasoning, or by adding closure axioms and minimizing causation within a deductive framework.  The latter method is strictly stronger than the former, but requires full knowledge of causation in a domain. "
}

@techreport{AIC-TN-1991:504,
keywords={Multiple Agents!Teamwork},
type =		"Technical Note",
number = 	504,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Teamwork",
author = 	"Phil Cohen",
month = 	"MARCH",
year = 		1991,
abstract = 	"What is involved when a group of agents decide to do something together?  Joint action by a team appears to involve more than just the union of simultaneous individual actions, even when those actions are coordinated.  We would not say that there is any teamwork involved in ordinary automobile traffic, even though the drivers act simultaneously and are coordinated (one hopes) by the traffic signs and rules of the road.  But when a group of drivers decide to do something together, such as driving somewhere as a convoy, it appears that the group acts more like a single agent with beliefs, goals, and intentions of its own, over and above the individual ones.  But given that actions are performed by individuals, and that it is individuals who ultimately have the beliefs and goals that engender action, what motivates agents to form teams and act together?  In some cases, the answer is obviously the inherent value in doing something together, such as playing tennis, performing a duet, or dancing.  These are examples of activities that simply cannot be performed alone.  But in many cases, team activity is only one way among many of achieving the goals of the individuals.  What benefits do agents expect to derive from their participation in a group effort?  In this paper, we attempt to provide an answer to these questions.  In particular, we argue that a joint activity is one that is performed by individuals sharing certain specific mental properties.  We show how these properties affect and are affected by properties of the participants.  Regarding the benefits of teamwork, we show that in return for the overhead involved in participating in a joint activity, an agent expects to be able to share the load in achieving a goal in a way that is robust against certain possible failures and misunderstandings. "
}

@techreport{AIC-TN-1991:503,
keywords={Program Synthesis!Fundamentals},
type =		"Technical Note",
number = 	503,
price = 	"$3.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Fundamentals Of Deductive Program Synthesis",
author = 	"Richard Waldinger",
month = 	"FEBRUARY",
year = 		1991,
abstract = 	"An informal tutorial is presented for program synthesis, with an emphasis on deductive methods.  According to this approach, to construct a program meeting a given specification, we prove the existence of an object meeting the specified conditions.  The proof is restricted to be sufficiently constructive, in the sense that, in establishing the existence of the desired output, the proof is forced to indicated a computational method for finding it.  That method becomes the basis for a program that can be extracted from the proof. The exposition is based on the deductive-tableau system, a theorem-proving framework particularly suitable for program synthesis. The system includes a nonclausal resolution rule, facilities for reasoning about equality, an a well-founded induction rule. "
}

@techreport{AIC-TN-1991:502,
keywords={Simpson's paradox, Reasoning!Simpson's Paradox},
type =		"Technical Note",
number = 	502,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Simpson's Paradox:  A Maximum Likelihood Solution",
author = 	"Mathhias P. Klay and Leonard P. Wesley",
month = 	"JANUARY,",
year = 		1991,
abstract = 	"INTRODUCTION:  Simpson's paradox exemplifies a class of problems that can arise when the logic used to reason about the semantics of propositional sentences does not adequately capture certain dependencies between sentences of interest.  This paradox has been known as early as 1903 [YUL03], and has been discussed extensively in the statistical literature [SIM51, DAW79, BLY73, CHU42].  The phenomena that typically give rise to Simpson's paradox can occur in cases such as destructive testing (e.g., determining the breaking strength of materials in orthogonal directions), and identifying the composition of complex alloys.  It has also been reported to occur in ``real-life'' several times since its discovery [KNA85, WAG82].  One such occurrence received wide attention in 1973 over the appearance of a sex bias in the admission policy for graduate students at the University of Berkeley [BIC75].  Given that automated systems will be expected to recognize and cope with the underlying phenomena of this paradox, it is important to develop effective methods for dealing with them, particularly as it impacts the choice of logics that systems must use to reason about real world problems.  Only recently, however, has there been any significant indication that Simpson's paradox merits serious attention by the AI community [PEA88]. "
}

@techreport{AIC-TN-1990:501,
keywords={Evidential Reasoning!Understanding, Reasoning!Uncertain|see{Evidential Reasoning}},
type =		"Technical Note",
number = 	501,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Understanding Evidential Reasoning",
author = 	"Enrique Ruspini and John D. Lowrance and Thomas M. Strat",
month = 	"DECEMBER",
year = 		1990,
abstract = 	"We address recent criticisms of evidential reasoning, an approach to the analysis of imprecise and uncertain information that is based on the Dempster-Shafer calculus of evidence.  We show that evidential reasoning can be interpreted in terms of classical probability theory and that the Dempster-Shafer calculus of evidence may be considered to be a form of generalized probabilities reasoning based on the representation of probabilistic ignorance by intervals of possible values.  In particular, we emphasize that it is not necessary to resort to nonprobabilistic or subjectivist explanations to justify the validity of the approach. "
}

@techreport{AIC-TN-1990:500,
keywords={Reasoning!Real-time, PRS, Reasoning!Deliberation},
type =		"Technical Note",
number = 	500,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Managing Deliberation and Reasoning In Real-Time Ai Systems",
author = 	"Michael P. Georgeff and Francois Felix Ingrand",
month = 	"DECEMBER",
year = 		1990,
abstract = 	"This paper describes some recent research on architectures
for situated (embedded) systems that need to deliberate and reason in
real time.  One of the most difficult problems in the design of such
architectures is how to manage the reasoning performed by such a
system while still meeting the real-time constraints of the problem
domain.  We present an architecture, based on the Procedural Reasoning
System (PRS), that provides mechanisms for the management and control
of deliberation and reasoning in real-time domains.  In particular, we
show how deliberation and reasoning strategies can be represented in
the form of metalevel plans, and describe an interpreter that selects
and executes these in a way that retains bounded reaction time.  In
addition, this approach allows us to represent different types of
situated systems by varying the metalevel deliberation strategies.
Finally, we provide some statistical measures of performance for one
such type of situated system applied to a complex real-time
application."
}

@techreport{AIC-TN-1990:499,
keywords={Reasoning!Abduction!Weighted, TACITUS!Abduction, Natural Language!Interpretation},
type =		"Technical Note",
number = 	499,
price = 	"$15.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Interpretation As Abduction",
author = 	"Jerry R. Hobbs and Mark Stickel and Douglas Appelt and Paul Martin",
month = 	"DECEMBER",
year = 		1990,
abstract = 	"Abduction is inference to the best explanation.  In the TACITUS project at SRI we have developed an approach to abductive inference, called ``weighted abduction,'' that has resulted in a significant simplification of how the problem of interpreting texts is conceptualized.  The interpretation of a text is the minimal explanation of why the text would be true.  More precisely, to interpret a text, one must prove the logical form of the text from what is already mutually known, allowing for coercions, merging redundancies where possible, and making assumptions where necessary. It is shown how such ``local pragmatics'' problems as reference resolution, the interpretation of compound nominals, the resolution of syntactic ambiguity and metonymy, and schema recognition can be solved in this manner.  Moreover, this approach of ``interpretation as abduction'' can be combined with the older view of ``parsing as deduction'' to produce an elegant and thorough integration of syntax, semantics, and pragmatics, one that spans the range of linguistic phenomena from phonology to discourse structure and accommodates both interpretation and generation.  Finally, we discuss means for making the abduction process efficient, possibilities for extending the approach to other pragmatics phenomena, and the semantics of the weights and costs in the abduction scheme. "
}

@techreport{AIC-TN-1990:497,
keywords={Reasoning!Practical, Reasoning!Overloading intentions},
type =		"Technical Note",
number = 	497,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Overloading Intentions For Efficient Practical Reasoning",
author = 	"Martha Pollack",
month = 	"NOVEMBER",
year = 		1990,
abstract = 	""
}

@techreport{AIC-TN-1990:496,
keywords={Natural Language!Dialogue!Interpreted Telephone Dialogues, Speech!Interpretation in Telephone Dialogues, Natural Language!Spoken Language},
type =		"Technical Note",
number = 	496,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Spoken Language In Interpreted Telephone Dialogues",
author = 	"Sharon L. Oviatt, Philip R. Cohen and Ann Podlozny",
month = 	"OCTOBER",
year = 		1990,
abstract = 	"This research outlines the predominant dialogue and performance characteristics of three-person interpreted telephone speech during service-oriented dialogues, in comparison with those of two-person noninterpreted dialogues.  An empirical study was conducted in which 12 native English speakers each made one telephone call through an experienced telephone interpreter to a Japanese confederate who did not speak English, and a second call to a Japanese confederate fluent in English.  In total, 24 dialogues were collected, each one containing two successfully completed service tasks, or 48 tasks total.  This paper reports on comparisons performed between three-person interpreted and two-person noninterpreted speech, based on the same pool of tasks and English subjects.  The unique characteristics of interpreted telephone dialogues are outlined, including structural and referential features, miscommunications and other performance characteristics, confirmatory language, and linguistic indirection.  In addition, an analysis is presented of interpreters' strategic management of turn shifts, and of the content, sequencing, and chunking of information passed among speakers.  The long-term goal of this exploratory research is the modeling of human speech, and the specification of preliminary target requirements for future automatic systems. "
}

@techreport{AIC-TN-1990:494,
keywords={Program verification!Rule-based Systems, Expert Systems},
type =		"Technical Note",
number = 	494,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Proving Properties Of Rule-Based Systems",
author = 	"Richard J. Waldinger and Mark E. Stickel",
month = 	"DECEMBER",
year = 		1990,
abstract = 	"Rule-based systems are being applied to tasks of increasing responsibility.  Deductive methods are being applied to their validation, to detect flaws in these systems and enable us to use them with more confidence.  Each system of rules is encoded as a set of axioms that define the system theory.  The operation of the rule language and information about the subject domain are also described in the system theory.  Validation tasks, such as establishing termination, unreachability, or consistency, or verifying properties of the system, are all phrased as conjectures.  If we succeed in establishing the validity of the conjecture in the system theory, we have carried out the corresponding validation task.  A method for the gradual formulation of specifications based on the attempted proof of a series of conjectures has been found to be suitable for rule-based systems.  Such a specification can serve as the basis for a reengineering of the system using conventional software technology. "
}

@techreport{AIC-TN-1990:493,
keywords={Natural Language!Anaphora},
type =		"Technical Note",
number = 	493,
price = 	"$23.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Syntactic Constraints On Anaphoric Binding",
author = 	"Mary E. Dalrymple",
month = 	"JULY",
year = 		1990,
abstract = 	"CONCLUSION:  One of the primary advantages of the treatment of syntactic constraints on anaphoric binding that has been presented here is that it provides a framework for stating binding constraints in a precise manner.  The framework makes explicit predictions as to which noun phrases will be considered when anaphoric binding constraints are applied.  In this sense, anaphoric binding constraints are different from relations of conference.  If two elements are coreferent, this may satisfy some positive constraint or violate some negative constraint; coreference between the two might also be irrelevant to any binding constraint.  These cases can be easily distinguished in this framework.  Stating constraints in terms of binding equations makes it easy to express interactions between binding constraints; such interactions are even predicted in some cases.  In the absence of such a framework, it is difficult even to frame the question of what interactions can occur; given an explicit statement of the binding constraints, such interactions are easy to observe.  "
}

@techreport{AIC-TN-1990:492,
keywords={Reasoning!Approximate, Reasoning!Uncertain, Possible Worlds},
type =		"Technical Note",
number = 	492,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Approximate Reasoning:  Past, Present, Future",
author = 	"Enrique H. Ruspini",
month = 	"JUNE",
year = 		1990,
abstract = 	"This note presents a personal view of the state of the art in the representation and manipulation of imprecise and uncertain information by automated processing systems.  To contrast their objectives and characteristics with the sound deductive procedures of classical logic, methodologies developed for that purpose are usually described as relying on Approximate Reasoning.  Using a unified descriptive framework, we will argue that, far from being mere approximations of logically correct procedures, approximate reasoning methods are also sound techniques that describe the properties of a set of conceivable states of a real-world system.  This framework, which is based on the logical notion of possible worlds, permits the description of the various approximate reasoning methods and techniques and simplifies their comparison.  More importantly, our descriptive model facilitates the understanding of the fundamental conceptual characteristics of the major methodologies.  We examine first the development of approximate reasoning methods from early advances to the present state of the art, commenting also on the technical motivation for the introduction of certain controversial approaches.  Our unifying semantic model is then introduced to explain the formal concepts and structures of the major approximate reasoning methodologies: classical probability calculus, the Dempster-Shafer calculus of evidence, and fuzzy (possibilistic) logic.  In particular, we discuss the basic conceptual differences between probabilistic and possibilistic approaches.  Finally, we take a critical look at the controversy about the need and utility for diverse methodologies, and assess requirements for future research and development.  "
}

@techreport{AIC-TN-1990:491,
keywords={Reasoning!Abduction!Weighted, Natural Language!Plan inference},
type =		"Technical Note",
number = 	491,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Weighted Abduction For Plan Ascription",
author = 	"Douglas E. Appelt and Martha E. Pollack",
month = 	"MAY",
year = 		1990,
abstract = 	"We describe an approach to abductive reasoning called weighted abduction, which uses inference weights to compare competing explanations for observed behavior.  We present an algorithm for computing a weighted-abductive explanation, and sketch a model-theoretic semantics for weighted abduction.  We argue that this approach is well suited to problems of reasoning about mental state. In particular, we show how the model of plan ascription developed by Konolige and Pollack can be recast in the framework of weighted abduction, and we discuss the potential advantages and disadvantages of this encoding. "
}

@techreport{AIC-TN-1990:490,
keywords={Natural Language!Interpretation!Incremental, Natural Language!Context, CANDIDE},
type =		"Technical Note",
number = 	490,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Incremental Interpretation",
author = 	"Fernando Pereira and Martha E. Pollack",
month = 	"MAY",
year = 		1990,
abstract = 	"We present a system for the incremental interpretation of natural-language utterances in context.  The main goal of the work is to account for the influences of context on interpretation, while preserving compositionality to the extent possible.  To achieve this goal, we introduce a representational device, conditional interpretations, and a rule system for constructing them.  Conditional interpretations represent the potential contributions of phrases to the interpretation of an utterance.  The rules specify how phrase interpretations are combined and how they are elaborated with respect to context.  The control structure defined by the rules determines the points in the interpretation process at which sufficient information becomes available to carry out specific inferential interpretation steps, such as determining the plausibility of particular referential connections or modifier attachments.  We have implemented these ideas in Candide, a system for interactive acquisition of procedural knowledge. "
}

@techreport{AIC-TN-1990:489,
keywords={Tileworld, Reasoning!Meta-level},
type =		"Technical Note",
number = 	489,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Introducing The Tileworld:  Experimentally Evaluating Agent  Architectures",
author = 	"Martha E. Pollack and Marc Ringuette",
month = 	"MAY",
year = 		1990,
abstract = 	"We describe a system called Tileworld, which consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable.  Both the agent and the environment are highly parameterized, enabling one to control certain characteristics of each.  We can thus experimentally investigate the behavior of various meta-level reasoning strategies by tuning the parameters of the agent, and can assess the success of alternative strategies in different environments by tuning the environmental parameters.  Our hypothesis is that the appropriateness of a particular meta-level reasoning strategy will depend in large part upon the characteristics of the environment in which the agent incorporating that strategy is situated.  We describe our initial experiments using Tileworld, in which we have been evaluating a version of the meta-level reasoning strategy proposed in earlier work by one of the authors [Bratman et al., 1988]. "
}

@techreport{AIC-TN-1990:488,
keywords={TACITUS!Task specification},
type =		"Technical Note",
number = 	488,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Domain-Independent Task Specification In The Tacitus Natural Language  System",
author = 	"Mabry Tyson and Jerry R. Hobbs",
month = 	"MAY",
year = 		1990,
abstract = 	"Many seemingly very different application tasks for natural language systems can be viewed as a matter of inferring the instance of a prespecified schema from the information in the text and the knowledge base.  We have defined and implemented a schema specification and recognition language for the TACITUS natural language system.  This effort entailed adding operators sensitive to resource bounds to the first-order predicate calculus accepted by a theorem-prover.  We give examples of the use of this schema language in a diagnostic task, an application involving data base entry from messages, and a script recognition task, and we consider further possible developments. "
}

@techreport{AIC-TN-1990:487,
keywords={Natural Language!Dialogue, Multiple Agents!Joint intention},
type =		"Technical Note",
number = 	487,
price = 	"$7.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Task-Oriented Dialogues As A Consequence Of Joint Activity",
author = 	"Philip R. Cohen",
month = 	"MAY",
year = 		1990,
abstract = 	"We argue that current plan-based theories of discourse do not by themselves explain even simple task-oriented dialogues.  The purpose of this paper to show how a number of difficult-to-explain features of these dialogues follow from the joint or team nature of the underlying task.  Specifically, the paper formally defines the concept of a joint intention and we argue that the conversants in a task-oriented dialogue jointly intend to accomplish the task.  From this basis, the paper derives the goals underlying the pervasive use of confirmations, elaborations, requests for clarification, and temporal discourse markers observed in a recent experiment.  We conclude with a discussion on generalizing the analysis presented to characterize dialogue itself as a joint activity. "
}

@techreport{AIC-TN-1990:486,
keywords={Natural Language!Performatives, Natural Language!Speech act theory},
type =		"Technical Note",
number = 	486,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Performatives In A Rationally Based Speech Act Theory",
author = 	"Philip R. Cohen and Hector J. Levesque",
month = 	"MAY",
year = 		1990,
abstract = 	"A crucially important adequacy test of any theory of speech acts is its ability to handle performatives.  This paper provides a theory of performatives as a test case for our rationally based theory of illocutionary acts.  We show why ``I request you...'' is a request, and ``I lie to you that p'' is self-defeating.  The analysis supports and extends earlier work of theorists such as Bach and Harnish [1] and takes issue with recent claims by Searle [10] that such performative-as-declarative analyses are doomed to failure. "
}

@techreport{AIC-TN-1990:485,
keywords={Actions!Joint|see{Multiple Agents}, Multiple Agents!Committing to a common goal},
type =		"Technical Note",
number = 	485,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"On Acting Together",
author = 	"Hector J. Levesque, Philip R. Cohen and Jose H. T. Nunes ",
month = 	"MAY",
year = 		1990,
abstract = 	"Joint action by a team does not consist merely of simultaneous and coordinated individual actions; to act together, a team must be aware of and care about the status of the group effort as a whole.  We present a formal definition of what it could mean for a group to jointly commit to a common goal, and explore how these joint commitments relate to the individual commitments of the team members. We then consider the case of joint intention, where the goal in question involves the team performing some action.  In both cases, the theory is formulated in a logical language of belief, action, and time previously used to characterize individual commitment and intention. An important consequence of the theory is the types of communication among the team members that it predicts will often be necessary. "
}

@techreport{AIC-TN-1990:484,
keywords={Reasoning!Abduction!Machine Translation by, Natural Language!Machine Translation, Natural Language!Abduction},
type =		"Technical Note",
number = 	484,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Translation By Abduction",
author = 	"Jerry R. Hobbs and Megumi Kameyama",
month = 	"MAY",
year = 		1990,
abstract = 	"We propose an approach to machine translation using abduction.  This approach overcomes the common machine translation bottleneck by allowing mapping of information from the source to the target language at a variety of levels --- from the most superficial to levels requiring deep interpretation and access to knowledge about the world, the context, and the speech act situation.  We illustrate the approach with a translation of an English sentence into Japanese, and show how we might handle outstanding hard problems in a translation between this language pair. "
}

@techreport{AIC-TN-1990:483,
keywords={DIALOGIC, Natural Language!DIALOGIC, Natural Language!Parse Preference},
type =		"Technical Note",
number = 	483,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Two Principles Of Parse Preference",
author = 	"John Bear and Jerry R. Hobbs",
month = 	"APRIL",
year = 		1990,
abstract = 	"%NOTE: NO ABSTRACT - First paragraph:  The DIALOGIC system for syntactic analysis and semantic translation has been under development for over ten years, and during that time it has been used in a number of domains in both database interface and message-processing applications.  In addition, it has been tested on a number of sentences of linguistic interest.  Built into the system are facilities for ranking parses according to syntactic and selectional considerations, and over the years, as various kinds of ambiguity have become apparent, heuristics have been devised for choosing the preferred parses.  Our aims in this paper is first to present a compendium of many of these heuristics and secondly to propose two principles that seem to underlie the heuristics.  The first aim will be useful to researchers engaged in building grammars of similarly broad coverage.  The second is of psychological interest and may be a guide for estimating parse preferences for newly discovered ambiguities for which we lack the experience to decide among on a more empirical basis. "
}

@techreport{AIC-TN-1990:482,
keywords={Phonology!Backwards, Natural Language!Phonology},
type =		"Technical Note",
number = 	482,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Backwards Phonology",
author = 	"John Bear",
month = 	"APRIL",
year = 		1990,
abstract = 	"This paper constitutes an investigation into the generative capabilities of two-level phonology with respect to unilevel generative phonological rules.  Proponents of two-level phonology have claimed, but not demonstrated, that two-level rules and grammars of two-level rules are reversible and that grammars of unilevel rules are not.  This paper makes ``reversibility'' explicit and demonstrates by means of examples from Tunica and Klamath that two-level phonology does have certain desirable capabilities that are not found in grammars of unilevel rules. "
}

@techreport{AIC-TN-1990:481,
keywords={Information!Integration, Inference techniques, Deduction!Integration of Information, Reasoning!Uncertain},
type =		"Technical Note",
number = 	481,
price = 	"$10.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"A Survey Of Ai Approaches To The Integration Of Information",
author = 	"Thomas D. Garvey",
month = 	"MAY",
year = 		1990,
abstract = 	"The integration of information is a central issue for Artificial Intelligence research and development.  The inference process in AI is the fundamental mechanism for combining information, and a significant aspect of most AI systems is the means by which they manage their overall workload by focusing processing attention and controlling which inferences are drawn and when it is appropriate to draw them.  Several perspectives on the control of inferential processes and their access to information have evolved.  One view of the problem treats the task as a goal-driven perceptual process, where specific information is explicitly sought from the world through selected sensor modalities, translated into a common ``vocabulary,'' fused with other relevant information, and finally translated back into an understanding of critical aspects of the environment.  Another view, centers on a flexible structure known as the blackboard architecture for enforcing control and communication activities.  In this paper, we first review briefly a variety of AI inference techniques, focusing primarily on logical inference and uncertain reasoning methods.  We conclude with a survey of approaches used to control inference processes, to mediate their access to real world information, and to schedule their activities. "
}

@techreport{AIC-TN-1990:480,
keywords={Speech Understanding!Prosodic Information, Natural Language!Prosodic Information, Natural Language!Grammar for Speech},
type =		"Technical Note",
number = 	480,
price = 	"$5.00",
institution =	"AI Center, SRI International",
address =	"333 Ravenswood Ave., Menlo Park, CA 94025",
title = 	"Prosody, Syntax and Parsing",
author = 	"John Bear",
month = 	"APRIL",
year = 		1990,
abstract = 	"We describe the modification of a grammar to take advantage of prosodic information provided by a speech recognition system.  This initial study is limited to the use of relative duration of phonetic segments in the assignment of syntactic structure, specifically in ruling out alternative parses in otherwise ambiguous sentences.  Taking advantage of prosodic information in parsing can make a spoken language system more accurate and more efficient, if prosodic-syntactic mismatches, or unlikely matches, can be pruned.  We know of no other work that has succeeded in automatically exacting speech information and using it in a parser to rule out extraneous parses. "  
}

@techreport{aic-tn-1990:479,
number=479,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Decision-Making In An Embedded Reasoning System",
author={Michael P. Georgeff and Francois Felix Ingrand},
month="February",
year=1990,
keywords={PRS, Reasoning!Real-time},
abstract={
The development of reasoning systems that can reason and plan in a
continuously changing environment is emerging as an important area of
research in Artificial Intelligence.  This paper describes some of the
features of a Procedural Reasoning System (PRS) that enables it to
operate effectively in such environments.  The basic system design is
first described and it is shown how this architecture supports both
goal-directed reasoning and the ability to react rapidly to
unanticipated changes in the environment.  The decision-making
capabilities of the system are then discussed and it is indicated how
the system integrates these components in a manner that takes account
of the bounds on both resources and knowledge that typify most
real-time operations.  The system has been applied to handling
malfunctions on the space shuttle, threat assessment, and the control
of an autonomous robot.}
}

@techreport{aic-tn-1990:478,
number=478,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Real-Time Reasoning: The Monitoring and Control Of Spacecraft Systems",
author={Michael. P. Georgeff and Francois Felix Ingrand},
month="January",
year=1990,
keywords={PRS, Reasoning!Real-time},
abstract={
This paper describes research  concerned with
automating the monitoring and control of spacecraft systems.  In
particular, the paper examines the application of SRI's Procedural
Reasoning System (PRS) to the handling of malfunctions in the Reaction
Control System (RCS) of NASA's space shuttle.  Unlike traditional
monitoring and control systems, PRS is able to reason about and
perform complex tasks in a very flexible and robust manner, somewhat
in the manner of a human assistant.  Using various RCS malfunctions as
examples (including sensor faults, leaking components, multiple
alarms, and regulator and jet failures), it is shown how PRS manages
to combine both goal-directed reasoning and the ability to react
rapidly to unanticipated changes in its environment.  In conclusion,
some important issues in the design of PRS are reviewed and future
enhancements are indicated.}
}

@techreport{aic-tn-1989:476,
number=476,
price="$8.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Localized Search",
author={Lode Missiaen},
month="November",
year=1989,
keywords={GEMPLAN, Multiple Agents!Planning},
abstract={
  In this report, we describe the search algorithm of the
GEMPLAN multiagent planning system.  The search algorithm is based
upon the GEMPLAN domain description and its localized constraint
representation.  The problem domain is structured into regions of
activity, and each region has its own set of local constraints.  The
search is a constraint-satisfaction process; it tries to find a plan
in each region by satisfying the region's constraints.  Therefore, the
search space is subdivided into regional search trees.  Unfortunately,
these search trees cannot be searched independently.  However, the
situation is much better than global search because GEMPLAN's
constraint localization, together with the domain structure, precisely
define when the search in one region can affect another region, and
hence how control must shift from one search tree to another.

	To avoid any confusion, this report does not describe GEMPLAN,
but only its generic localized search algorithm.  We only explain and
abstract features of GEMPLAN on which this search algorithm is based.
As a result, this algorithm is applicable to any other
constraint-satisfaction problem with characteristics similar to
GEMPLAN.}
}

@techreport{aic-tn-1989:475,
number=475,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="On The Semantics Of Fuzzy Logic",
author={Enrique H. Ruspini},
month="December",
year=1989,
keywords={Fuzzy Logic|see{Logic, Fuzzy}, Logic!Fuzzy},
abstract={
This note presents a formal semantic characterization of the major
concepts and constructs of fuzzy logic in terms of notions of
distance, closeness, and similarity between pairs of possible worlds.
The formalism is a direct extension (by recognition of multiple
degrees of accessibility, conceivability, or reachability) of the
major modal logic concepts of possible and necessary truth.

Given a function that maps pairs of possible worlds into a number
between 0 and 1, generalizing the conventional concept of an
equivalence relation, the major constructs of fuzzy logic (i.e.,
conditioned and uncondition possibility distributions) are defined in
terms of this generalized similarity relation using familiar concepts
from the mathematical theory of metric spaces.  This interpretation is
different in nature and character from the typical, chance-oriented,
meanings associated with probabilistic concepts, which are grounded on
the mathematical notion of set measure.  The similarity structure
defines a topological notion of continuity in the space of possible
worlds (and in that of its subsets, i.e., propositions) that allow a
form of logical ``extrapolation'' between possible worlds.  

This logical extrapolation operation corresponds to the major
deductive rule of fuzzy logic ---the compositional rule of inference or
generalized modus ponens of Zadeh---an inferential operation that
generalizes its classical counterpart by virtue of its ability to be
utilized when propositions representing available evidence only match
approximately the antecedents of conditional propositions.  The
relations between the similarity-based interpretation of the role of
conditional possibility distributions and the approximate inferential
procedures of Baldwin are also discussed.  

A straightforward extension of the theory to the case where the
similarity scale is symbolic rather than numeric is described.  The
problem of generating similarity functions from a given set of
possibility distributions, with the latter interpreted as defining a
number of (graded) discernibility relations and the former as the
result of combining them into a joint measure of distinguishability
between possible worlds, is briefly discussed.}
}

@techreport{aic-tn-1989:474,
number=474,
price="$3.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Perspective On Multiagent Planning",
author={Amy L. Lansky},
month="September",
year=1989,
keywords={GEMPLAN, Multiple Agents!Planning},
abstract={
  This report presents an informal review of current research trends in
planning and, in particular, my own views on multiagent planning.  A short
description of the GEMPLAN research project is provided, including both the
current state of the system and my future research plans.}
}

@techreport{aic-tn-1989:473,
number=473,
price="$6.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Path-Indexing Method For Indexing Terms",
author={Mark E. Stickel},
month="October",
year=1989,
keywords={Path-indexing, Deduction!Indexing},
abstract={
  The path-indexing method for indexing first-order predicate calculus
terms is a refinement of the standard coordinate-indexing method.  Path
indexing offers much faster retrieval at a modest cost in space.  Path indexing
is compared with discrimination-net and codeword indexing.  While
discrimination-net indexing may often be the preferred method for maximum
speed, path indexing is an effective alternative if discrimination-net indexing
requires too much space or in certain cases in which discrimination-net
indexing performs particularly poorly.}
}

@techreport{aic-tn-1989:472,
number=472,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Decision Analysis Using Belief Functions",
author={Thomas M. Strat},
month="September",
year=1989,
keywords={Evidential Reasoning!Decision analysis, Decision analysis},
abstract={
  A primary motivation for reasoning under uncertainty is to derive
decisions in the face of inconclusive evidence.  Shafer's theory of
belief functions, which explicitly represents the underconstrained
nature of many reasoning problems, lacks a formal procedure for making
decisions.  Clearly, when sufficient information is not available, no
theory can prescribe actions without making additional assumptions.
Faced with this situation, some assumption must be made if a clearly
superior choice is to emerge.  In this paper we offer a probabilistic
interpretation of a simple assumption that disambiguates decision
problems represented with belief functions.  We prove that it yields
expected values identical to those obtained by a probabilistic analysis
that makes the same assumption.  We maintain a strict separation between
evidence that carries information about a situation and assumptions that
may be made for disambiguation of choices.  In addition, we show how
the decision analysis methodology frequently employed in probabilistic
reasoning can be extended for use with belief functions.  This
generalization of decision analysis allows the use of belief functions
within the familiar framework of decision trees.}
}

@techreport{aic-tn-1989:471,
number=471,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Toward A Foundation For Evaluating Ai Planners",
author={Nabil A. Kartam and David E. Wilkins},
month="August",
year=1989,
keywords={Planning!Evaluating, NOAH, NONLIN, SIPE, TWEAK},
abstract={
  There exists a large body of Artificial Intelligence (AI) research on
generating plans, i.e., linear or nonlinear sequences of actions, to transform
an initial world state to some desired goal state.  However, much of the
planning research to date has been complicated, ill-understood, and unclear.
Only a few of the developers of these planners have provided a thorough
description of their research products, and those descriptions that exist are
usually unrealistically favorable since the range of applications for which the
systems are tested is limited to those for which they were developed.  As a
result, it is difficult to evaluate these planners and to choose the best
planner for a different domain.  To make a planner applicable to different
planning problems, it should be domain independent.  However, one needs to know
the circumstances under which a general planner works so that one can determine
its suitability for a specific domain.

This paper presents criteria for evaluating AI planners; these criteria fall
into three categories: (1) performance issues, (2) representational issues, and
(3) communication issues.  This paper also assesses four nonlinear AI planners
(NOAH, NONLIN, SIPE, and TWEAK).}
}

@techreport{aic-tn-1989:470,
number=470,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Fast Parallel Surface Interpolation With Applications To Digital Cartography",
author="Richard Szeliski",
month="June",
year=1989,
keywords={Vision!Cartography, Parallel relaxation},
abstract={
 The manipulation of two dimensional elevation maps is an important
part of digital cartography.  In many situations, these maps are computed by
interpolating sparse data such as isolated elevation points obtained from
stereo matching.  In this paper, we present a surface interpolation algorithm
based on variational splines which is well suited to massively parallel
computers.  Using multiresolution parallel relaxation, we can efficiently
compute the interpolated surface and also have local control over its
continuity and smoothness.  We apply this technique to sparse elevation data
and to elevation contours, and show how to add realistic fractal detail through
stochastic relaxation.  We also present a multiresolution decomposition
algorithm and a fast parallel 3-D rendering algorithm.}
}

@techreport{aic-tn-1989:469,
number=469,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Concepts Of Information: Comparative Axiomatics",
author={David Israel},
month="June",
year=1989,
}

@techreport{aic-tn-1989:468R,
number=468R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Can Ai Planners Solve Practical Problems?",
author={David E. Wilkins},
month="November",
year=1989,
keywords={Planning!Practical, SIPE-2},
abstract={
   While there has been recent interest in research on planning and
reasoning about actions, nearly all research results have been theoretical.  We
know of no previous examples of a planning system that has made a significant
impact on a problem of practical importance.  One of the primary goals during
the development of the SIPE-2 planning system has been the balancing of
efficiency with expressiveness and flexibility.  With a major new extension,
SIPE-2 has begun to address practical problems.  This paper describes
this new extension and the new applications of the planner.  One of
these applications is the problem of producing products from raw
materials on process lines under production and resource constraints.
This is a problem of commercial importance and SIPE-2's application to
it is described in some detail.}
}

@techreport{aic-tn-1989:467,
number=467,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Calculus For Semantic Composition and Scoping",
author={Fernando C.N. Pereira},
month="May",
year=1989,
keywords={Natural Language!Composition, Natural Language!Scoping},
abstract={
  Certain restrictions on possible scopings of quantified noun phrases
in natural language are usually expressed in terms of formal constraints on
binding at a level of logical form.  Such reliance on the form rather than the
content of semantic interpretations goes against the spirit of
compositionality.  I will show that those scoping restrictions follow from
simple and fundamental facts about functional application and abstraction, and
can be expressed as constraints on the derivation of possible meanings for
sentences rather than constraints of the alleged forms of those meanings.}
}

@techreport{aic-tn-1989:466,
number=466,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Semantic-Head-Driven Generation Algorithm For Unification-Based Formalisms",
author="Stuart M. Shieber and Gertjan Van Noord and Robert C. Moore and Fernando C.N. Pereira",
month="May",
year=1989,
keywords={Unification grammar},
abstract={
  We present an algorithm for generating strings from logical form
encodings that improves upon previous algorithm in that it places fewer
restrictions on the class of grammars to which it is applicable.  In
particular, unlike an Earley deduction generator (Shieber, 1988), it allows use
of semantically nonmonotonic grammars, yet unlike top-down methods, it also
permits left-recursion.  The enabling design feature of the algorithm is its
implicit traversal of the analysis tree for the string being generated in a
semantic-head-driven fashion.}
}

@techreport{aic-tn-1989:465,
number=465,
price="$3.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Objective Functions For Feature Discrimination",
author={Pascal Fua and Andrew J. Hanson},
month="May",
year=1989,
keywords={Vision!Feature discrimination},
abstract={
  We propose and evaluate a class of objective functions that rank
hypotheses for feature labels.  Our approach takes into account the
representation cost and quality of the shapes themselves, and balances the
geometric requirements against the photometric evidence.  This balance is
essential for any system using underconstrained or generic feature models.  We
introduce examples of specific models allowing the actual computation of the
terms in the objective function, and show how this framework leads naturally to
control parameters that have a clear semantic meaning.  We illustrate the
properties of our objective functions on synthetic and real images.}
}

@techreport{aic-tn-1989:464,
number=464,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Prolog Technology Theorem Prover: A New Exposition and Implementation In Prolog",
author="Mark Stickel",
month="June",
year=1989,
keywords={Deduction!PTTP, PTTP, Prolog}	,
abstract={
  A Prolog technology theorem prover (PTTP) is an extension of Prolog
that is complete for the full first-order predicate calculus.  It differs from
Prolog in its use of unification with the occurs check for soundness,
depth-first iterative-deepening search instead of unbounded depth-first search
to make the search strategy complete, and the model elimination reduction rule
that is added to Prolog inferences to make the inference system complete.  This
paper describes a new Prolog-based implementation of PTTP.  It uses three
compile-time transformations to translate formulas into Prolog clauses that
directly execute, with the support of a few run-time predicates, the model
elimination procedure with depth-first iterative-deepening search and
unification with the occurs check.  Its high performance exceeds that of
Prolog-based PTTP interpreters, and it is more concise and readable than the
earlier Lisp-based compiler, which makes it superior for expository purposes.
Examples of inputs and outputs of the compile-time transformations provide an
easy and quite precise way to explain how PTTP works.  This Prolog-based
version makes it easier to incorporate PTTP theorem-proving ideas into Prolog
programs.  Some suggestions are made on extensions to Prolog that could be used
to improve PTTP's performance.}
}

@techreport{aic-tn-1989:463,
number=463,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Recognizing Objects In A Natural Environment:  A Contextual Vision (CVS)",
author="Martin A. Fischler and Thomas M. Strat",
month="March",
year=1989,
keywords={Vision!Contextual, CVS|see(Contextual Vision System), Contextual Vision System},
abstract={
Existing machine vision techniques are not competent to reliably
recognize objects in unconstrained views of natural scenes.  In this
paper we identify a number of weaknesses in current recognition
systems, including an inability to solve the partitioning problem or
to effectively use context and other types of knowledge beyond that of
immediate object appearance.  We propose specific mechanisms for
dealing with some of these problems and describe the design of a
vision system that incorporates these new mechanisms.  The system has
been partially implemented and we include some experimental results
indicative of its operation and performance.}
}

@techreport{aic-tn-1989:462,
number=462,
price="$3.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Morphology With Two-Level Rules and Negative Rule Features",
author={John Bear},
month="March",
year=1989,
keywords={Natural Language!Morphology},
abstract={
  Two-level phonology, as currently practiced, has two severe
limitations.  One is that phonological generalizations are generally expressed
in terms of transition tables of finite-state automata, and these tables are
cumbersome to develop and refine.  The other is that lexical idiosyncrasy is
encoded by introducing arbitrary diacritics into the spelling of a morpheme.
This paper describes how to use phonological rules instead of transition
tables, and describes a more elegant way of expressing phonological
irregularity than with arbitrary diacritics, making use of the fact that
generalizations are expressed with rules instead of automata.}
}

@techreport{aic-tn-1989:461,
number=461,
price="$3.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Generation and Recognition Of Inflectional Morphology",
author={John Bear},
month="March",
year=1989,
keywords={Natural Language!Morphology, Natural Language!Unification grammar},
abstract={
  Koskenniemi's two-level morphological analysis system can be improved
upon by using a PATR-like unification grammar for handling the morphosyntax
instead of continuation classes, and by incorporating the notion of negative
rule feature into the phonological rule interpreter.  The resulting system can
be made to do generation and recognition using the same grammars.}
}

@techreport{aic-tn-1989:460,
number=460,
price="$16.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Parsing and Type Inference For Natural and Computer Languages",
author={Stuart M. Shieber},
month="March",
year=1989,
keywords={Natural Language!Parsing, Natural Language!Type inference},
abstract={
  Computational and theoretical linguists and computer scientists
interested in the computer processing of natural language have converged on a
class of grammar formalisms for describing the well-formedness conditions of
natural languages.  This class is distinguished by its reliance on systems of
declarative constraints to explicate natural-language syntax axiomatically,
rather than generatively or procedurally.  Intuition suggests that these
various efforts from a broad range of disciplines form a natural methodological
class; still, there is no general foundation on which to ground this
impression.  We provide a method for abstractly and uniformly characterizing a
class of formalisms based on logical constraints, and use the uniformity to
define and prove correct a parsing algorithm that applies to any formalism in
the class.  We discuss the applicability of these techniques to computer
languages as well, specifically to type inference.  In so doing, extensions of
the simple formalism instances typically considered in natural-language
analysis are motivated.

This dissertation thus provides a mathematical and computational foundation
for, and extensions of, current practices in theoretical and computational
linguistics, as well as initiating a rapprochement between the techniques for
describing well-formedness of natural and computer languages.}
}

@techreport{aic-tn-1992:459,
number=459,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Representation Space Paradigm Of Concurrent Evolving Object Descriptions",
author={Aaron F. Bobick and Robert C. Bolles},
month="February",
year=1992,
keywords={Vision!Object Descriptions},
abstract={
A representation paradigm for instantiating and refining multiple,
concurrent descriptions of an object from a sequence of imagery is
presented.  This paradigm is designed to be used by the perception
system of an autonomous robot that $1$)
 needs to describe many types
of objects, $2$) initially detects objects at a distance and gradually
acquires higher resolution data, and $3$) continuously collects
sensory input.  We argue that multiple, concurrent descriptions of an
object are necessary because different perceptual tasks are best
performed using different representations and because different types
of descriptions require different quality data to support their
computation.  Since the data change significantly over time, the
paradigm supports the evolution of descriptions, progressing from
crude $2$-D ``blob'' descriptions to complete semantic models, such as
bush, rock, and tree.  To control this accumulation of new
descriptions, we introduce the idea of{\em representation space}.
Representation space is a lattice of representations that specifies
the order in which they should be considered for describing an object.
Each of the representations in the lattice is associated with an
object only after the object has been described multiple times in the
representation and the parameters of the representation have been
judged to be ``stable''.  We define stability in a statistical sense,
enhanced by a set of explanations describing valid reasons for
deviations from expected measurements.  These explanations may draw
on many types of knowledge, including the physics of the sensor, the
performance of the segmentation procedure, and the reliability of the
matching technique.  To illustrate the power of these ideas, we have
implemented a system, which we call TraX, that constructs and refines
models of outdoor objects detected in sequences of range data.}
}

@techreport{aic-tn-1989:458,
number=458,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Vision Problem: Exploiting Parallel Computation",
author={Martin A. Fischler and Oscar Firschein and Stephen T. Barnard and Pascal V. Fua and Yvan G. Leclerc},
month="February",
year=1989,
keywords={Vision!Parallel processing, Parallel processing|see{Vision}},
abstract={
  This technical report consists of an introductory paper and three
technical papers presented at the session, ``AI Application of Supercomputers:
The Vision Problem,'' (wherein the major problems in computer vision
are outlined, and the ``signals to symbols'' (SS) and the ``monolithic
computing (MC) approaches to these problems are described.  We note
that the availability of parallel computation makes the MC approach
feasible) at the Fourth International Conference on Supercomputing, 
Santa Clara, California, April 30 to May 5, 1989.}
}

@techreport{aic-tn-1989:457,
number=457,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Application Of Default Logic To Speech Act Theory",
author={C. Raymond Perrault},
month="February",
year=1989,
keywords={Natural Language!Speech act theory, Logic!Default},
abstract={
  One of the central issues to be addressed in basing a theory of speech
acts on independently motivated accounts of propositional attitudes (belief,
knowledge, intentions, etc.) and action is the specification of the effects of
communicative acts.  The very fact that speech acts are conventional means that
specifying the effects of the utterance of, say, a declarative sentence, or the
performance of an assertion, requires taking into consideration many possible
deviations from the conventional use of sentences --- specifically uses that are
insincere, not serious, or indirect.  Previous approaches to the problem of
specifying speech act consequences have paid insufficient attention to the
dependence of the participants' mental state after an utterance on their mental
state preceding it.  We present a limited solution to the problem of belief
revision within Reiter's nonmonotonic Default Logic and show how to formulate
the consequences of many uses of declarative sentences.  Default rules are used
to embody a simple theory of belief adoption, action observation, and the
relation between the form of a sentence and the attitudes it is used to convey.}
}

@techreport{aic-tn-1989:456,
number=456,
price="$16.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Cl-Patr Reference Manual",
author={Stuart M. Shieber},
month="February",
year=1989,
keywords={PATR!CL-PATR, CL-PATR|see{PATR}},
}

@techreport{aic-tn-1989:455,
number=455,
price="$3.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Integrated Framework For Semantic and Pragmatic Interpretation",
author={Martha E. Pollack and Fernando C.N. Pereira},
month="January",
year=1989,
keywords={Natural Language!Semantic and pragmatic interpretation},
abstract={
  We report on a mechanism for semantic and pragmatic interpretation
that has been designed to take advantage of the generally compositional nature
of semantic analysis, without unduly constraining the order in which pragmatic
decisions are made.  To achieve this goal, we introduce the idea of a
{\it conditional interpretation}: one that depends upon a set of assumptions
about subsequent pragmatic processing.  Conditional interpretations are
constructed compositionally according to a set of declaratively specified
interpretation rules.  The mechanism can handle a wide range of pragmatic
phenomena and their interactions.}
}

@techreport{aic-tn-1988:454,
number=454,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Structure and Performance Efficiency In Noninteractive Spoken, Modalities",
author={Sharon L. Oviatt and Philip R. Cohen},
month="December",
year=1988,
keywords={Natural Language!Speech modalities},
abstract={
  Speaker interaction is a central feature of human dialogue, one with
a powerful influence on its discourse structure and performance efficiency. The
present study examined two speech modalities that represent opposites on the
spectrum of speaker interaction --- the telephone dialogue and audiotape
monologue.  Experts provided instructions by either telephone or audiotape as
their novice partner completed an assembly task. Within this task framework, a
comprehensive analysis is provided of the basic differences in discourse
organization, referential characteristics, and performance efficiency for these
two spoken modalities. The outlined distinctions are interpreted with special
reference to the role of confirmation feedback in promoting dialogue
efficiency.  Implications are discussed for the development of prospective
speech systems designed to be habitable, high quality, and relatively enduring.
Finally, a theoretical model of collaborative dialogue is proposed from which
several features of interactive and noninteractive speech can be derived.}
}

@techreport{aic-tn-1988:453,
number=453,
price="$7.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Tense, Aspect, and The Interpretation Of Tenseless Elements In English",
author={Mary Dalrymple},
month="November",
year=1988,
keywords={Natural Language Understanding!Tense and Aspect},
abstract={
An analysis of English tense and aspect is presented that specifies
temporal precedence relations with a sentence.  The relevant reference
points for interpretation are taken to be the initial and terminal
points of events in the world, as well as two ``hypothetical'' times:
the {\em perfect} time (when a sentence contains perfect aspect) and
the {\em progressive} or {\em during} time.  We also describe a method
for providing temporal interpretation for nontensed elements such as
nouns and adjectives, whose interpretation may be temporally dependent.}
}

@techreport{aic-tn-1988:452,
number=452,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Contributing Influence Of Speech and Interaction On Human, Discourse Patterns",
author={Sharon L. Oviatt and Philip R. Cohen},
month="November",
year=1988,
keywords={Natural Language!Speech and Interaction, Speech|see{Natural Language}},
abstract={
Communication
channels physically constrain the flow and shape of human language
just as irresistably as a river bed directs the river's current.
Escarpments speed the current, sculpting jetties and whirlpools.
Meadows encourage evenness, a certain recumbency.  The flow becomes a
deafening cascade as it passes over grainte bolders, and is arrested
abruptly behind man-made dams.  In short, the river is molded,
rendered navigable or not, through the physical medium of its own bed.
Although the communication modalities may be less visually compelling
than the terrain surrounding a river, it is a mistake to assume that
they are any less influential in shaping the language transmitted
within them.  Understanding the influence of communication modalities
begins with an identification of their landmark features, and of the
observable impact of these features on language.  One goal of this
chapter is to provide a comparison of the discourse and performance
characteristics of instructions presented in three different
modalities, each of which was classified according to the presence or
absence of:  1) speech and 2) interaction.  A second goal is to begin
constructing an analytical framework from which predictions can be
made about the separate impact of speech and interaction on specific
aspects of discourse and performance.}
}

@techreport{aic-tn-1988:451,
number=451,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Prolog-Like Inference System For Computing Minimum-Cost Abductive Explanations In Natural-Language Interpretation",
author={Mark E. Stickel},
month="September",
year=1988,
keywords={Prolog, Reasoning!Abduction, Abduction|see{Reasoning}},
abstract={
  By determining what added assumptions would suffice to make the
logical form of a sentence in natural language provable, abductive inference
can be used in the interpretation of sentences to determine what information
should be added to the listener's knowledge, i.e., what he should learn from
the sentence.  This is a comparatively new application of mechanized abduction.
A new form of abduction---least specific abduction---is proposed as being more
appropriate to the task of interpreting natural language than the forms that
have been used in the traditional diagnostic and design-synthesis applications
of abduction.  The assignment of numerical costs to axioms and assumable
literals permits specification of preferences on different abductive
explanations.  A new Prolog-like inference system that computes abductive
explanations and their costs is given.  To facilitate the computation of
minimum-cost explanations, the inference system, unlike others such as Prolog,
is designed to avoid the repeated use of the same instance of an axiom or
assumption.}
}

@techreport{aic-tn-1988:450,
number=450,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Brief Overview Of The Candide Project",
author={Fernando C.N. Pereira and Martha E. Pollack},
month="September",
year=1988,
keywords={CANDIDE, Natural Language!CANDIDE},
}

@techreport{aic-tn-1988:449,
number=449,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="P-Patr: A Compiler For Unification-Based Grammars",
author={Susan Beth Hirsh},
month="September",
year=1988,
keywords={PATR!P-PATR, P-PATR|see{PATR}, Unification grammar},
}

@techreport{aic-tn-1988:446,
number=446,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Hierarchic Autoepistemic Theories For Nonmonotonic Reasoning: Preliminary, Report",
author={Kurt Konolige},
month="August",
year=1988,
keywords={Reasoning!Nonmonotonic, Nonmonotonic Reasoning|see{Reasoning}, Logic!Autoepistemic},
abstract={
  Nonmonotonic logics are meant to be a formalization of nonmonotonic
reasoning.  However, for the most part they fail to embody two of the most
important aspects of such reasoning:  the explicit computational nature of
nonmonotonic inference, and the assignment of preferences among competing
inferences.  We propose a method of nonmonotonic reasoning in which the notion
of inference from specific bodies of evidence plays a fundamental role.  The
formalization is based on autoepistemic logic, but introduces additional
structure, a hierarchy of evidential spaces.  The method offers a natural
formalization of many different applications of nonmonotonic reasoning,
including reasoning about action, speech acts, belief revision, and various
situations involving competing defaults.}
}

@techreport{aic-tn-1988:443,
number=443,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Classification-Based Tracking Of Objects and Material",
author={Kenneth I. Laws},
month="July",
year=1988,
keywords={KNIFE, Vision!Tracking},
abstract={
  SRI's KNIFE image analysis system can be used for tracking objects
and material classes from one image to another.  Variations on this theme are
the initial acquisition of target instances from database signatures and the
subsequent acquisition of additional instances in an image once a few objects
have been labeled.  Classification-based tracking is facilitated by improved
color and texture-energy transforms.  KNIFE's labeling and partitioning methods
can be used with complex targets, and are relatively unaffected by occlusions
and changes in object appearance during tracking.}
}

@techreport{aic-tn-1988:442,
number=442,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Coarse Coding For Material and Object Identification",
author={Kenneth I. Laws},
month="July",
year=1988,
keywords={Vision!Identification},
abstract={
  A new coarse-coding technique is presented for labeling image pixels
and regions to match exemplars or multivariate material signatures.  This
multinomial classification method can be used for object cuing and tracking, as
well as for material identification and image segmentation.  Pixels are
classified---and classification reliability can be estimated---with only
single-band histograms and one pass through each image band.  An example of
four-class labeling illustrates the power of this two-level classification
algorithm.}
}

@techreport{aic-tn-1988:441,
number=441,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Integrated Split/Merge Image Segmentation",
author={Kenneth I. Laws},
month="July",
year=1988,
keywords={KNIFE, Vision!Segmentation},
abstract={
  The KNIFE segmentation algorithm interleaves splitting and merging of
regions during mono\-chrome or multiband image partitioning.  KNIFE splits
regions along object boundaries, thus avoiding rectangular quadtree artifacts
and establishing a context for good statistical decisions.  Its iterative
subregion extraction is based on multiband cluster analysis, with
histogram-based threshold analysis used as a heuristic shortcut in simple
cases.  Splitting and merging decisions are based on sloped (rather than
constant) surface fits, with successively more powerful thresholds and
techniques employed until each region is split or found homogeneous.  The user
specifies only a desired level of segmentation, which is converted to
procedural form by the KNIFE control process.  The KNIFE package also offers a
region-growing algorithm based on recursive splitting of neighboring regions.
Examples of the two techniques are given for the domains of aerial cartography
and reconnaissance, target cuing, and navigational vision.}
}

@techreport{aic-tn-1988:440,
number=440,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Descriptive Model Of Reference Using Defaults",
author={Douglas Appelt and Amichai Kronfeld},
month="May",
year=1988,
keywords={Natural Language!Reference model},
abstract={
  In this article we try to answer the following question:  how do we
let our audience know what we are talking about?  How, in other words,
do a speaker and hearer form an agreemtn as to which entities are the
subject of the conversation?}
}

@techreport{aic-tn-1988:439,
number=439,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Literal Goal and Discourse Purpose Of Referring",
author={Amichai Kronfeld},
month="May",
year=1988,
keywords={Natural Language!Referring},
}

@techreport{aic-tn-1988:438,
number=438,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Management Of Miscommunications: Toward A System For Automatic\ Telephone, Interpretation Of Japanese-English Dialogues",
author={Sharon L. Oviatt},
month="May",
year=1988,
keywords={Natural Language!Interpretation, Natural Language!Miscommunications, Natural Language!Dialogue},
abstract={
  This report presents exploratory research on miscommunications
and their resolution during Japanese--English interpretation, based on
interviews with experienced professional interpreters. ``Brokering'' is
identified as a naturally occurring and central dialogue management strategy
that supports interpreted communications. A comprehensive, ecologically
oriented description of brokering is provided that focuses on its structure,
functions, and the communicative factors associated with its use. In addition,
analyses are provided of three types of disruptive miscommunication that
predominate during Japanese--English interpretation, along with the brokering
techniques that interpreters use to resolve them effectively. Implications of
these research findings for the design of an automatic Japanese--English
telephone interpretation system are discussed. It is argued that such a system
would benefit from incorporation of a brokered approach to interpretation that
is based on adequate recognition of a speaker's intentions.}
}

@techreport{aic-tn-1988:437,
number=437,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Uniform Architecture For Parsing and Generation",
author={Stuart M. Shieber},
month="May",
year=1988,
keywords={PATR, Natural Language!Parsing, Natural Language!Generation},
abstract={
 The use of a single grammar for both parsing and
generation is an idea with a certain elegance, the desirability of
which several researchers have noted.  In this paper, we discuss a
more radical possibility: not only can a single grammar be used by
different processes engaged in various ``directions'' of processing, but
one and the same language-processing architecture can be used for
processing the grammar in the various modes.  In particular, parsing
and generation can be viewed as two processes engaged in by a single
parameterized theorem prover for the logical interpretation of the
formalism.  We discuss our current implementation of such an
architecture, which is parameterized in such a way that it can be used
for either purpose with grammars written in the PATR formalism.
Furthermore, the architecture allows fine tuning to reflect different
processing strategies, including parsing models intended to mimic
psycholinguistic phenomena.  This tuning allows the parsing system to
operate within the same realm of efficiency as previous architectures
for parsing alone, but with much greater flexibility for engaging in
other processing regimes.}
}

@techreport{aic-tn-1988:436,
number=436,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Donnellan's Distinction As An Adequacy Test For A Referring Model",
author={Amichai Kronfeld},
month="April",
year=1988,
keywords={Natural Language!Referring},
}

@techreport{aic-tn-1988:435,
number=435,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Descriptive Approach To Reference: Why It Is Difficult To Work With, And, Why We Have To",
author={Amichai Kronfeld},
month="April",
year=1988,
keywords={Natural Language!Referring},
}

@techreport{aic-tn-1988:434,
number=434,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Methodological Notes On A Computational Model Of Referring",
author={Amichai Kronfeld},
month="April",
year=1988,
keywords={Natural Language!Referring},
}

@techreport{aic-tn-1988:433,
number=433,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Rational Interaction As The Basis For Communication",
author={Phil R. Cohen and Hector J. Levesque},
month="April",
year=1988,
keywords={Natural Language!Interaction},
abstract={
  This paper derives the basis of a theory of communication from a
formal theory of rational interaction.  The major result is a demonstration
that illocutionary acts need neither be primitive, nor explicitly recognized.
As a test case, we derive Searle's conditions on requesting from principles of
rationality coupled with a theory of imperatives.  The theory rests on a formal
account of intention and distinguishes insincere or nonserious imperatives from
true requests.  A theory of purposeful communication thus emerges as a
consequence of principles of action and interaction.}
}

@techreport{aic-tn-1988:432,
number=432,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Practical Nonmonotonic Theory For Reasoning About Speech Acts",
author={Douglas Appelt and Kurt Konolige},
month="April",
year=1988,
keywords={Reasoning!Speech acts, Natural Language!Speech acts, Reasoning!Nonmonotonic, Logic!Autoepistemic},
abstract={
  A prerequisite to a theory of the way agents understand
speech acts is a theory of how their beliefs and intentions are revised
as a consequence of events.  This process of attitude revision is an
interesting domain for the application of nonmonotonic reasoning because
speech acts have a conventional aspect that is readily represented by
defaults, but that interacts with an agent's beliefs and intentions in
many complex ways that may override the defaults.  Perrault has
developed a theory of speech acts, based on Reiter's default logic, that
captures the conventional aspect; it does not, however, adequately
account for certain easily observed facts about attitude revision
resulting from speech acts.  A natural theory of attitude revision seems
to require a method of stating preferences among competing defaults.
We present here a speech act theory, formalized in hierarchic autoepistemic
logic (a refinement of Moore's autoepistemic logic), in which revision of
both the speaker's and hearer's attitudes can be adequately described.
As a collateral benefit, efficient automatic reasoning methods for the
formalism exist.  The theory has been implemented and is now being employed
by an utterance-planning system.}
}

@techreport{aic-tn-1988:430,
number=430,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Explaining Evidential Analyses",
author={Thomas M. Strat and John D. Lowrance},
month="January",
year=1988,
keywords={Evidential Reasoning!Explaining, Reasoning!Evidential|see{Evidential Reasoning}},
abstract={
   One of the most highly touted virtues of knowledge-based
expert systems is their ability to construct explanations for their
lines of reasoning.  However, there is a basic difficulty in
generating explanations in expert systems that reason under
uncertainty using numeric measures.  In particular, systems based upon
evidential reasoning using the theory of belief functions have lacked
all but the most rudimentary facilities for explaining their
conclusions.  In this paper we review the process whereby other expert
system technologies produce explanations, and present a methodology
for augmenting an evidential-reasoning system with a versatile
explanation facility.  The method, which is based on sensitivity
analysis, has been implemented, and several examples of its use are described.}
}

@techreport{aic-tn-1987:429,
number=429,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Local Pragmatics",
author={Jerry R. Hobbs and Paul Martin},
month="December",
year=1987,
keywords={TACITUS, Natural Language!Pragmatics, Commonsense Knowledge},
abstract={
   The outline of a unified theory of local pragmatics
phenomena is presented, including an approach to the problems of
reference resolution, metonymy, and interpreting nominal compounds.
The TACITUS computer system embodying this theory is also described.
The theory and system are based on the use of a theorem prover to draw
the appropriate inferences from a large knowledge base of commonsense
and technical knowledge.  Issues of control are discussed. Two
important kinds of implicatures are defined, and it is shown how they
can be used to determine what in a text is given and what is new.}
}

@techreport{aic-tn-1987:428,
number=428,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Localizing Expression Of Ambiguity",
author={John Bear and Jerry R. Hobbs},
month="November",
year=1987,
keywords={Natural Language!Ambiguity},
abstract={
   In this paper we describe an implemented program for
localizing the expression of many types of syntactic ambiguity, in the
logical forms of sentences, in a manner convenient for subsequent
inferential processing.  Among the types of ambiguities handled are
prepositional phrases, very compound nominals, adverbials, relative
clauses, and preposed prepositional phrases.  The algorithm we use is
presented, and several possible shortcomings and extensions of our
method are discussed.}
}

@techreport{aic-tn-1987:426,
number=426,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Core Knowledge System",
author={Thomas M. Strat and Grahame B. Smith},
month="October",
year=1987,
keywords={Core Knowledge System, CKS},
abstract={
  This document contains an in-depth description of the Core Knowledge
System (CKS)--an integrative environment for the many functions that
must be performed by sensor-based autonomous and semi-autonomous
systems.  The CKS itself has been designed to support a wide variety
of potential applications.  However, special attention has been given
to assuring its relevance to a particular application--that of an
autonomous land vehicle operating in an unconstrained outdoor
environment.  The functionality provided by the system is described,
along with discussions of the various design decisions and their
associated trade-offs where applicable.  This paper is not intended to
serve as a user's manual, rather its purpose is to describe the CKS in
sufficient detail to allow the reader to ascertain its relevance to a
particular application and to provide a technical critique of its
strengths and weaknesses.}
}

@techreport{aic-tn-1988:425R,
number=425R,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Plans and Resource-Bounded Practical Reasoning",
author={Michael E. Bratman and David J. Israel and Martha E. Pollack},
month="September",
year=1988,
keywords={Planning!Resource-bounded, Rational Agents!Planning},
abstract={
 An architecture for a rational agent must allow for
means-end reasoning, for the weighing of competing alternatives, and
for interactions between these two forms of reasoning.  Such an
architecture must also address the problem of resource boundedness.
We sketch a solution of the first problem that points the way to a
solution of the second.  In particular, we present a high-level
specification of the practical-reasoning component of an architecture
for a resource-bounded rational agent.  In this architecture, a major
role of the agent's plans is to constrain the amount of further
practical reasoning she must perform.}
}

@techreport{aic-tn-1988:424,
number=424,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Survey Of Architectures For Distributed Artificial Intelligence",
author={Todd R. Davies},
month="June",
year=1988,
keywords={Distributed Artificial Intelligence},
abstract={
 This report surveys literature and research in the field of
distributed artificial intelligence (DAI) and provides an overview of
computer architectures particularly suited to such research.  It
concentrates on work to date that has involved the construction of
testbeds and development tools for DAI. It tries to draw some lessons
from these efforts and suggests ways in which testbeds, which
heretofore have been used primarily for experimentation, might be used
in the full course of system development.}
}

@techreport{aic-tn-1988:423,
number=423,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Localized Event-Based Reasoning For Multiagent Domains",
author={Amy L. Lansky},
month="January",
year=1988,
keywords={GEMPLAN, GEM, Multiple Agents!Planning},
abstract={
  This paper presents the {\bf GEM} concurrency model and {\bf
GEMPLAN}, a multiagent planner based on this model.  Unlike standard
state-based AI representations, GEM is unique in its explicit emphasis on
events and domain structure. In particular, a world domain is modeled as a {\it
set of regions composed of interrelated events.} Event-based temporal logic
constraints are then associated with each region to delimit legal domain
behavior.  The GEMPLAN planner directly reflects this emphasis on domain
structure and constraints.  It can be viewed as a general-purpose constraint
satisfaction facility which constructs a network of interrelated events (a
``plan'') that is subdivided into regions (``subplans''), satisfies all
applicable regional constraints, and also achieves some stated goal.  Because
GEMPLAN is specifically geared towards parallel, multiagent domains, we believe
that its natural application areas will include scheduling and other forms of
organizational coordination.

GEMPLAN extends and generalizes previous planning architectures in
several ways.  First of all, it can handle a much broader range of
constraint forms than most planners.  Second, GEMPLAN's constraint
satisfaction search strategy can be flexibly tuned.  A third and
critical aspect of our work has been an emphasis on {\it localized}
reasoning --- techniques that make explicit use of domain structure.
For example, GEM localizes the applicability of domain constraints and
imposes additional ``locality constraints'' on the basis of domain
structure.  Together, constraint localization and locality constraints
provide semantic information that can be used to alleviate several
aspects of the frame problem for multiagent domains.  The GEMPLAN
planner reflects this use of locality by subdividing its constraint
satisfaction search space into {\it regional planning search spaces}.
Utilizing constraint and property localization, GEMPLAN can pinpoint
and rectify interactions among these regional search spaces, thus
reducing the burden of ``interaction analysis'' ubiquitous to most
planning systems.}
}

@techreport{aic-tn-1987:422,
number=422,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Separating Linguistic Analyses From Linguistic Theories",
author={Stuart M. Shieber},
month="May",
year=1987,
keywords={Natural Language!Theories, PATR},
abstract={
  This paper  explores the question   of what level  of  linguistic
practice  is   best  suited for  use  in natural-language  processing (NLP)
efforts.  In particular, I  argue that because  the goals and strategies of
linguistic theory and NLP differ, linguistic theories and formalisms may be
inappropriate for   importation into NLP  applications.   The  question  of
whether insights from linguistic analyses can  still be taken advantage  of
without  such importation  is  elucidated    by making use  of   notational
reductions from  one formalism to another,  thereby helping demonstrate the
independence  of analyses  from  formalisms.  Several  such reductions  are
informally discussed, and one, a reduction from the  formalism used in work
on    generalized phrase-structure grammar to   the   PATR-II  formalism is
presented in more detail.}
}

@techreport{aic-tn-1987:421,
number=421,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Learning and Recognition In Natural Environments",
author={Alex P. Pentland},
month="June",
year=1987,
keywords={Learning, Vision!Object recognition},
abstract={
  We present  a system  for learning descriptions of objects,  and for
subsequently recognizing learned  objects, that functions  in outdoor, natural
environments.  We  describe  in detail  two  modes of functioning  within this
system: (1) unguided, bottom-up  learning of object  descriptions directly from
image data; (2) top-down recognition of objects whose approximate position and
structure  are known by use of  image-level matching.  We  then argue that the
system's performance  in    these two  modes  indicates  that  robust  outdoor
performance can be achieved within the structure of our vision system.}
}

@techreport{aic-tn-1987:420,
number=420,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Grammars and Logics Of Partial Information",
author={Fernando C.N. Pereira},
month="April",
year=1987,
keywords={Natural Language!Unification grammar, Logic Programming},
abstract={
  This paper is an informal survey of models of grammatical categories
in  unification-based formalisms  from  computational linguistics  and   their
relationship  to  current logic programming  concepts.    The  basic notion of
partiality in   the   informational content of   grammatical   categories   is
introduced, and specific automata-theoretic,  denotational and logical  models
of partial information are outlined.  The expression of disjunction, negation,
inheritance and indeterminate constraints in categories is  also  discussed.}
}

@techreport{aic-tn-1987:419,
number=419,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Implicature and Definite Reference",
author={Jerry R. Hobbs},
month="March",
year=1987,
keywords={Natural Language!Implicature, Natural Language!Definite Reference},
abstract={
  An account is given of the  appropriateness conditions  for definite
reference, in terms  of the operations  of inference and   implicature.  It is
shown how a number of problematic cases noticed by Hawkins can be explained in
this framework.  In addition, the use of unresolvable definite noun phrases as
a literary device  and definite noun phrases with  nonrestrictive material can
be explained within the same framework.}
}

@techreport{aic-tn-1987:418,
number=418,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Planning",
author={Michael P. Georgeff},
month="March",
year=1987,
keywords={Planning},
}

@techreport{aic-tn-1987:417,
number=417,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Many Agents Are Better Than One",
author={Michael P. Georgeff},
month="March",
year=1987,
keywords={Planning!Multiagent|see{Multiple agents}, Frame Problem, Multiple Agents},
abstract={
   This paper  aims to show   how much  of the frame   problem can  be
alleviated by using  domain models that allow for  the simultaneous occurrence
of actions and events.  First, a generalized situation calculus is constructed
for describing and reasoning about events in multiagent settings.  Notions  of
{\it independence} and {\it causality} are then introduced and it is shown how
they can be used to determine the persistence of facts over time.  Finally, it
is   shown    how  these  notions,   together   with   traditional   predicate
circumscription, make it possible  to  retain a simple  model  of action while
avoiding most of the difficulties associated with the frame problem.}
}

@techreport{aic-tn-1986:416,
number=416,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Automating Argument Construction",
author={John D. Lowrance},
month="November",
year=1986,
keywords={Evidential Reasoning!Formal basis, GISTER, Evidential Reasoning!GISTER},
abstract={
  Over the past five years the  Artificial Intelligence Center at  SRI
has been  developing a  new technology  to   address  the problem of automated
information management within real-world contexts.  The result of this work is
a body of techniques for automated reasoning  from evidence that  we call {\em
evidential reasoning.} The techniques are based upon the mathematics of belief
functions developed by Dempster and Shafer and have been  successfully applied
to a variety  of problems  including computer vision, multisensor integration,
and intelligence analysis.

We  have  developed both a  formal   basis and   a  framework for implementing
automated reasoning systems based upon these techniques.  Both the formal  and
practical approach  can be  divided into four  parts:  (1)~specifying a set of
distinct  propositional  spaces,  (2)~specifying  the interrelationships among
these spaces, (3)~representing bodies of evidence as belief distributions, and
(4)~establishing paths for the bodies of evidence to move through these spaces
by means of evidential operations, eventually  converging on spaces  where the
target questions can  be answered.   These steps specify  a  means for arguing
from   multiple  bodies  of   evidence toward   a  particular  (probabilistic)
conclusion.  Argument  construction is the process by   which such  evidential
analyses are constructed and is the analogue of constructing proof  trees in a
logical context.

This technology features the ability to reason from uncertain, incomplete, and
occasionally inaccurate  information  based upon seven  evidential operations:
fusion, discounting, translating,  projection, summarization,  interpretation,
and gisting.  These   operations are theoretically  sound but  have  intuitive
appeal as well.

In implementing this formal approach, we have found  that evidential arguments
can be represented as graphs.  To support the construction,  modification, and
interrogation    of evidential arguments,  we   have developed Gister.  Gister
provides an  interactive, menu-driven, graphical  interface that  allows these
graphical structures to be easily manipulated.

Our goal is to provide effective automated aids to domain experts for argument
construction.  Gister represents our first attempt at such an aid.}
}

@techreport{aic-tn-1987:415,
number=415,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Persistence, Intention, and Commitment",
author={Philip R. Cohen and Hector J. Levesque},
month="February",
year=1987,
keywords={Beliefs, Goals, Actions, Intentions},
abstract={
  This paper explores principles governing the rational  balance among
an agent's beliefs, goals,  actions, and intentions.   Such principles provide
specifications for artificial agents, and approximate a theory of human action
(as philosophers use the term).  By making explicit the conditions under which
an agent can drop his goals, i.e., by specifying how the agent is committed to
his goals, the  formalism  captures  a  number   of  important properties   of
intention.  Specifically, the formalism  provides analyses for Bratman's three
characteristic functional roles  played by intentions   [7,8],  and shows  how
agents can avoid intending all the foreseen side-effects of what they actually
intend.  Finally, the analysis shows how intentions can be adopted relative to
a  background   of   relevant beliefs and  other   intentions   or goals.   By
relativizing one agent's intentions in terms of beliefs  about another agent's
intentions  (or   beliefs), we derive a  preliminary  account of interpersonal
commitments.}
}

@techreport{aic-tn-1987:414,
number=414,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Stereo Matching By Hierarchical, Microcanonical Annealing",
author={Stephen T. Barnard},
month="February",
year=1987,
keywords={Vision!Stereo, Vision!Annealing},
abstract={
  An improved stochastic stereo-matching   algorithm is presented.  It
incorporates two substantial  modifications to  an  earlier  version:  a   new
variation   of  simulated  annealing  that is   faster,   simpler,  and   more
controllable than the conventional ``heat-bath'' version,  and a hierarchical,
coarse-to-fine-resolution  control structure.   The Hamiltonian   used  in the
original model   is minimized,  but  far more   efficiently.   The   basis  of
microcanonical annealing is the Creutz  algorithm  .  Unlike its  counterpart,
the familiar Metropolis algorithm, the  Creutz algorithm simulates a thermally
isolated system at equilibrium.  The hierarchical  control structure, together
with a Brownian state-transition function, tracks ground states across  scale,
beginning with small, coarsely coded levels.   Results  are shown for  a 512 x
512 pair with 50 pixels of disparity.}
}

@techreport{aic-tn-1987:413,
number=413,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Sri Mobile Robot Testbed --- A Preliminary Report",
author={Stanley Reifel},
month="February",
year=1987,
keywords={Flakey, Robot!Mobile},
abstract={
  This paper describes a mobile robot designed  for experimentation in
artificial  intelligence (AI).    Presented  here  are details  of the robot's
hardware and software architecture.  The  robot is  driven by two electrically
powered wheels.   On-board  computers  control  the   motors  and   visual and
ultrasonic sensors.   A  library  of low-level  software  provides   primitive
functions for high-level  programs to interface with the   robot's sensors and
effectors.}
}

@techreport{aic-tn-1987:412,
number=412,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Synthesis Of Digital Machines With Probable Epistemic Properties",
author={Stanley Rosenschein and Leslie Pack Kaelbling},
month="April",
year=1987,
keywords={Logic!Epistemic, Rex, Multiple Agents},
abstract={
  Researchers using epistemic logic as a formal framework for studying
knowledge properties of artificial-intelligence (AI)  systems  often interpret
the knowledge   formula $K$($x,\varphi$) to   mean that  machine  $x$  encodes
$\varphi$ in its state as a syntactic formula or can  derive it inferentially.
If  $K$($x$,$\varphi$) is defined instead in  terms of the correlation between
the state of the machine and that of its environment, the formal properties of
modal system  S5 can be satisfied without  having to store  representations of
formulas as data    structures.  In this  paper,  we  apply the  correlational
definition of knowledge to machines with composite structure and describe  the
semantics  of  knowledge   representations  in   terms  of   correlation-based
denotation functions.  In particular, we describe how  epistemic properties of
synchronous digital  machines can be analyzed, starting  at the level of gates
and delays, by  modeling the machine's components  as  agents in a  multiagent
system  and  reasoning about  the  flow of   information among them.   We also
introduce Rex, a language for computing machine  descriptions recursively, and
explain how it can be used  to  construct machines with provable informational
properties.}
}

@techreport{aic-tn-1987:411,
number=411,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Procedural Knowledge",
author={Michael P. Georgeff and Amy L. Lansky},
month="January",
year=1987,
keywords={PRS, Reasoning!Procedural, Procedural Knowledge, Commonsense Knowledge},
abstract={
  Much of commonsense knowledge about the real world is in the form of
{\it procedures} or {\it sequences} of actions for achieving particular goals.
In this paper, a formalism is presented  for representing such knowledge using
the notion of {\it process}.  A declarative semantics  for  the representation
is given, which allows a user to state {\it facts} about the effects  of doing
things in the  problem domain of interest.  An  operational semantics is  also
provided, which  shows   {\it  how}  this knowledge  can  be  used to  achieve
particular   goals or to form intentions   regarding their achievement.  Given
both semantics,  our   formalism   additionally  serves   as an     executable
specification  language suitable for constructing  complex systems.   A system
based  on this formalism  is described, and  examples involving  control of an
autonomous robot and fault diagnosis for NASA's space shuttle are provided.}
}

@techreport{aic-tn-1987:410,
number=410,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Using Causal Rules In Planning",
author={David E. Wilkins},
month="March",
year=1987,
keywords={Planning!Using causal rules, SIPE},
abstract={
  Reasoning about actions necessarily  involves tracking the truth  of
assertions about the  world over time.  The  SIPE  planning system retains the
efficiency of the STRIPS assumption for this while  enhancing expressive power
by  allowing the specification  of  a causal theory.  Separation  of knowledge
about causality from knowledge about  actions relieves   operators of much  of
their representational burden and allows them to be applicable in a wide range
of contexts.  The implementation of causal  rules is described,  together with
examples and evaluations of the  system's expressive power  and  efficiency.}
}

@techreport{aic-tn-1987:409,
number=409,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Computational Model Of Referring",
author={Douglas E. Appelt and Amichai Kronfeld},
month="January",
year=1987,
keywords={Natural Language!Referring},
abstract={
  In  this paper we  present a  theory of referring.   This  theory is
presented within  the framework   of  a  general theory of  speech   acts  and
rationality advanced by  Cohen   and Levesque.  Understanding   a  speech  act
involves two tasks on the part of the hearer: recognition of  the {\it literal
goal}, and recognition and satisfaction  of {\it identification  constraints}.
We present a  theory   in  which it is   possible to   demonstrate that,  if a
referring expression is  uttered under appropriate  circumstances, the literal
goal is achieved.  Furthermore, the same theory can account  for the fact that
referring expressions can be used under other circumstances to inform and make
requests.  This theory  has application to  the design   of a natural-language
utterance planning system that formulates utterances to  achieve its  goals.}
}

@techreport{aic-tn-1986:408,
number=408,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Logical Foundations Of Evidential Reasoning",
author={Enrique H. Ruspini},
month="December",
year=1986,
keywords={Evidential Reasoning!Logical foundations, Reasoning!Evidential},
abstract={
 The approach proposed by Carnap for the development  of logical bases
for  probability theory is  investigated by using formal  structures that  are
based on epistemic logics.   Epistemic logics are modal  logics introduced  to
deal  with  issues that  are relevant to the state  of knowledge that rational
agents have about the real world.  The use of epistemic logics  in problems of
analysis of  evidence is  justified  by  the need to   distinguish  among such
notions as  the state of  a real system,  the state of knowledge possessed  by
rational agents, and the impact of information on that knowledge.

Carnap's method for generating a universe of possible worlds is followed using
an   enhanced notion   of possible world  that    encompasses descriptions  of
knowledge   states.   Within such   generalized or {\em  epistemic} universes,
several  classes  of  sets are identified  in  terms   of  the truth-values of
propositions that describe either  the state of   the world  or  the  state of
knowledge about it.   These classes of  subsets have  the structure of a sigma
algebra.

Probabilities defined  over  one of these   sigma algebras,  called  the  {\em
epistemic algebra,} are  then shown to  have the properties of  the belief and
basic probability   assignment functions  of  the Dempster-Shafer  calculus of
evidence.

It is also shown that any extensions of a probability function defined  on the
epistemic algebra (representing different states  of   knowledge) to the  {\em
truth algebra} (representing true states of the real world)  must satisfy  the
interval probability  bounds derived from  the  Dempster-Shafer theory.  These
bounds are  also shown to  correspond to  the  classical notions  of lower and
upper probability.  Furthermore, these constraints  are shown  to  be the best
possible bounds, given a specific state of knowledge.

Finally, the problem of combining the knowledge that several agents have about
a real-world system is addressed.  Structures representing possible results of
the integration of that knowledge are introduced and a general formula for the
combination    of evidence is    derived.   From   this   formula  and certain
probabilistic independence  assumptions,  a  generalization of  the   rule  of
combination of Dempster is readily proved.   The meaning of these independence
assumptions  is made explicit  through   the  insight provided by the   formal
structures that are used to represent knowledge and truth.

Finally, simple cases of combination of dependent evidence are discussed as an
introduction to more general problems of general combination that are examined
in a related paper.}
}

@techreport{aic-tn-1987:407,
number=407,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="On The Relation Between Default and Autoepistemic Logic",
author={Kurt G. Konolige},
month="August",
year=1987,
keywords={Logic!Autoepistemic, Logic!Default},
abstract={
  Default logic  is a formal  means of reasoning  about defaults:
what normally is  the  case,  in  the  absence  of contradicting  information.
Autoepistemic logic, on the other hand, is meant to  describe the consequences
of reasoning  about  ignorance: what must  be true if  a certain  fact  is not
known.  Although the motivation and formal character of these  two systems are
different, a closer  analysis  shows that they share  a common trait, which is
the indexical  nature of  certain elements in  the theory.   In this  paper we
compare the expressive power of the two systems.   First, we give an effective
translation of default logic into  autoepistemic  logic; default theories  can
thus be embedded into  autoepistemic logic.  We also  present a more surprising
result: the  reverse   translation is also  possible, so   that every   set of
sentences in autoepistemic  logic can be effectively   rewritten as  a default
theory.   The  formal equivalence  of  these two differing  systems  is   thus
established.  This analysis gives an interpretive semantics  to default logic,
and yields insight into the nature of defaults in autoepistemic reasoning.}
}

@techreport{aic-tn-1986:406,
number=406,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Recognition By Parts",
author={Alex P. Pentland},
month="December",
year=1986,
keywords={Vision!Part recognition},
abstract={
  To have a general-purpose machine vision capability, we must be able
to recognize  things;  we argue that most  natural objects have  a  {\em  part
structure} that we can recover from image data and thus  use as the  basis for
``general-purpose'' recognition.  We describe  a ``parts''  representation that
is fairly general purpose,  despite having only  a small number of parameters.
Having this expressive power  captured by a  small number of parameters allows
us to approach the problem of recovering an object's part  structure by use of
the model-based  vision  technique  of global   search-and-match.   We present
several examples of recovering  part structure  using various types of   range
imagery to show that the recovery procedure is robust.}
}

@techreport{aic-tn-1986:405,
number=405,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evidential Reasoning For Geographic Evaluation For Helicopter, Route Planning",
author={Thomas D. Garvey},
month="December",
year=1986,
keywords={Evidential Reasoning, Shafer-Dempster|see{Evidential Reasoning}},
abstract={
 In order to plan operations where knowledge of significant elements
is imprecise and uncertain, a means of characterizing the situation in terms
of the various factors that may influence those operations must be provided.
In this paper we discuss an approach to that characterization that uses
evidential reasoning to handle the uncertainty, imprecision, and
incompleteness typical of sources of real-world information and knowledge, to
support planning routes for military helicopters.

Evidential reasoning  is a maturing  collection  of  inference techniques  for
reasoning with uncertain information.  Based on the  Shafer-Dempster theory of
evidence, evidential reasoning uses a  non-Bayesian updating scheme to combine
evidence provided by multiple, diverse knowledge sources.  Knowledge sources in
an evidential reasoning system are not required to attribute their belief to a
universe of  discourse  comprised  solely of mutually  exclusive,  exhaustive,
singleton events, as required  by a  classical probability  approach.  Rather,
they  may express   levels of  ignorance explicitly by  allocating  belief  to
disjunctions of propositions, thereby leading directly  to an interval measure
of belief; ignorance is expressed by the width of this interval.

Evidential reasoning  evolved  from   consideration of  appropriate models for
reasoning about information acquired from sensors, and therefore seems natural
for  drawing conclusions from sensor  data and prestored  maps regarding   the
degree to which a  selected geographic  area will support  certain activities.
Here, we   discuss evidential reasoning and    illustrate  the utility  of the
technology for classifying   geographic  areas by   describing   our   current
map-and-sensor-based research in which we estimate the utility of  land  areas
for concealing helicopter operations.}
}

@techreport{aic-tn-1987:404,
number=404,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Actions, Processes, and Causality",
author={Michael P. Georgeff},
month="February",
year=1987,
keywords={Events!Models of, Actions!Models of, Multiple Agents},
abstract={
  The purpose of this paper  is to construct  a  model of actions  and
events that  facilitates reasoning   about dynamic domains  involving multiple
agents.   Unlike  traditional approaches,  the proposed  model allows for  the
simultaneous  performance   of   actions, rather  than    use  an interleaving
approximation. A generalized situation calculus is constructed  for describing
and   reasoning about  actions   in  multiagent  settings.    Notions of  {\it
independence} and {\it correctness} are introduced, and  it is shown  how they
can be used to determine the persistence of facts over time and whether or not
actions can  be performed concurrently.   Unlike most previous   formalisms in
both single- and multiagent domains, the proposed law  of persistence  is {\it
monotonic} and thus has a well-defined model-theoretic semantics.  It is shown
how the concept of {\it causality} can be employed to simplify the description
of actions and  to model arbitrarily  complex machines and  physical  devices.
Furthermore, it is  shown how  sets  of causally interrelated  actions can  be
grouped together  in  {\it  processes} and  how  this  structuring of  problem
domains can substantially reduce  combinatorial  complexity.  Finally,  it  is
indicated how the law of persistence,  together with the notion  of causality,
makes it possible to retain a  simple model of  action while  avoiding most of
the difficulties associated with the frame problem.}
}

@techreport{aic-tn-1986:403,
number=403,
price="$20.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Inferring Domain Plans In Question-Answering",
author={Martha E. Pollack},
month="December",
year=1986,
keywords={Natural Language!Plan inference, Natural Language!Question-answering, SPIRIT},
abstract={
  The importance of plan inference in models of  conversation has been
widely     noted in  the   com\-putational-linguistics  literature,     and  its
incorporation in question-answering systems has enabled a range of cooperative
behaviors.  The plan inference process in each  of these systems, however, has
assumed that the  questioner  (Q),  whose plan   is being  inferred,  and  the
respondent (R), who is drawing the inference, have identical beliefs about the
actions in the domain.  I  demonstrate that this assumption is  too strong and
that it often results in failure not only  of  the plan-inference process, but
also of the communicative process that plan inference is meant to support.  In
particular, it precludes the principled generation of appropriate responses to
queries that arise from invalid plans.  I present a model of plan inference in
conversation that distinguishes between the beliefs  of the questioner and the
beliefs of the respondent.  This model rests on an account of  plans as mental
phenomena: ``having a plan'' is analyzed as  having a particular configuration
of beliefs and intentions.  Judgments  that a plan  is invalid are associated
with particular discrepancies between the beliefs that R ascribes to Q, when R
believes that  Q  has  some particular  plan, and  the beliefs  that R herself
holds.  I define several types  of invalidities from which  a plan may suffer,
relating each to a particular type of belief discrepancy,  and show  that  the
types of any invalidities judged to be present in  the plan underlying a query
can  affect the context of a  cooperative response.  The plan  inference model
has been implemented in SPIRIT, a System for Plan Inference that Reasons about
Invalidities Too, which reasons about plans underlying  queries  in the domain
of                               computer                              mail.}
}

@techreport{aic-tn-1986:402,
number=402,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Hierarchical Warp Stereo",
author={Lynn H. Quam},
month="December",
year=1986,
keywords={Vision!Stereo, Vision!Digital Terrain Models},
abstract={
  This  paper  describes a   new technique for   use  in  the automatic
production of digital terrain models from stereo pairs  of aerial images.  This
technique employs  a  coarse-to-fine hierarchical  control structure both  for
global  constraint  propagation and for  efficiency.   By the  use of disparity
estimates  from  coarser   levels  of the   hierarchy,   one  of the  images is
geometrically warped to improve the performance  of the cross-correlation-based
matching operator.  A newly  developed surface interpolation  algorithm is used
to fill holes  wherever the matching  operator fails.  Experimental results for
the Phoenix Mountain  Park  data  set  are presented   and compared with  those
obtained by ETL.}
}

@techreport{aic-tn-1986:401,
number=401,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Representation Of Parallel Activity Based On Events, Structure,, and Causality",
author={Amy L. Lansky},
month="December",
year=1986,
keywords={Logic!Temporal, Multiple Agents, Knowledge Representation},
abstract={
   Most AI  domain representations  have been  based on state-oriented
world models.  In this paper we present  an event-based model  that focuses on
domain events  (both atomic  and nonatomic) and   on  the causal and  temporal
relationships among  them.  Emphasis  is  also placed   on representing   {\it
locations} of activity and using them to structure  the domain representation.
Our model is based on first-order temporal logic, which has  a well-understood
semantics and  has  been employed extensively  in concurrency theory.  We show
how temporal-logic  constraints on  {\it  event   histories} (records of  past
activity)   can   facilitate  the  description   of  many    of   the  complex
synchronization    properties   of    parallel,      multiagent     domains.}
}

@techreport{aic-tn-1986:400,
number=400,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Architecture For Intelligent Reactive Systems",
author={Leslie P. Kaelbling},
month="October",
year=1986,
keywords={Robot!Autonomous Mobile, Reactive systems},
abstract={
  Any intelligent  system that operates in a  moderately complex or
unpredictable  environment must  be reactive ---  that is, it  must respond
dynamically to changes in its environment.  A robot  that blindly follows a
program   or plan without  verifying that  its operations  are having their
intended effects is not reactive.  For simple tasks in carefully engineered
domains, non-reactive behavior is acceptable;  for more intelligent  agents
in unconstrained domains, it is not.

     This paper  presents the outline  of  an  architecture for intelligent
reactive  systems.  Much  of the discussion  will relate to the problem  of
designing an autonomous mobile robot, but the ideas are independent of  the
particular system.    The architecture is   motivated by  the  desires  for
modularity, awareness, and robustness.}
}

@techreport{aic-tn-1986:399,
number=399,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Knowledge-Based Architecture For Organizing Sensory Data",
author={Grahame B. Smith and Thomas M. Strat},
month="October",
year=1986,
keywords={Autonomous System!Information manager},
abstract={
  This paper describes  an architecture  for an information manager
that is at the core of a  sensor-based autonomous system.  The architecture
provides the means by which sensor-based data can be integrated with stored
knowledge to  provide the information needed  for autonomous behavior.  The
overall architecture can be viewed as a community of independent processes,
each of which interact with an active database whose structure mirrors that
of the three-dimensional world.}
}

@techreport{aic-tn-1986:398,
number=398,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Shading Into Texture",
author={Alex P. Pentland},
month="October",
year=1986,
keywords={Shading, Texture, Fractals},
abstract={
   Current shape-from-shading  and shape-from-texture  methods
are applicable only to smooth surfaces, while real surfaces  are often
rough and crumpled.  To extend such methods  to real  surfaces we must
have a model that also applies to rough surfaces.  The fractal surface
model provides a formalism that is competent to describe  such natural
3-D surfaces and,  in addition, is  able to  predict human  perceptual
judgments of smoothness versus roughness.  We have used  this model of
natural surface shapes to derive a technique for 3-D  shape estimation
that treats   shaded  and textured   surfaces  in  a  unified  manner.}
}

@techreport{aic-tn-1986:397,
number=397,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="How To Clear A Block:  A Theory Of Plans",
author={Zohar Manna and Richard Waldinger},
month="December",
year=1986,
keywords={Program Synthesis, Planning!Situational logic, Logic!Situational, Deduction!Deductive Tableau},
abstract={
  Problems in commonsense and robot planning are approached by
methods adapted from program synthesis research; planning  is regarded
as an  application of automated  deduction.  To support this approach,
we introduce  a variant of situational  logic, called plan  theory, in
which plans are explicit objects.

A machine-oriented deductive-tableau  inference system is  adapted  to
plan theory.  Equations and equivalences of  the theory are built into
a unification algorithm for the system.   Frame axioms  are built into
the resolution rule.

Special attention   is  paid to the    derivation  of conditional  and
recursive plans.  Inductive  proofs of  theorems for even the simplest
planning   problems, such as  clearing  a    block, have been found to
require  challenging  generalizations.}
}

@techreport{aic-tn-1986:396,
number=396,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Morphological Recognizer With Syntactic and Phonological Rules",
author={John Bear},
month="September",
year=1986,
keywords={Natural Language!Morphology},
abstract={
  This paper describes a morphological analyzer which, when parsing
a word, uses  two sets of rules: describing  the syntax of  words, and rules
describing facts about orthography.}
}

@techreport{aic-tn-1986:393,
number=393,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Natural-Language Interfaces",
author={C. Raymond Perrault and Barbara J. Grosz},
month="August",
year=1986,
keywords={Natural Language!Interfaces},
}

@techreport{aic-tn-1986:392,
number=392,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Commonsense Metaphysics and Lexical  Semantics",
author="Jerry R. Hobbs and William Croft and Todd Davies and Douglas Edwards and Kenneth  Laws",
month="August",
year=1986,
keywords={Commonsense Reasoning, Reasoning!Commonsense|see{Commonsense Reasoning}},
}

@techreport{aic-tn-1986:390,
number=390,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="On The Imaging Of Fractal Surfaces",
author={Alex Pentland and Paul Kube},
month="December",
year=1986,
keywords={Fractals!Imaging},
abstract={
  We  examine the imaging of standard  Brownian Fractal surfaces,
and  find that,  given  certain  assumptions,  a  Fractal   surface with power
spectrum proportional  to  $f^{-\beta}$  has   an  image  with power  spectrum
proportional to $f^{2-\beta}$.}
}

@techreport{aic-tn-1986:388,
number=388,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="High-Level Planning In A Mobile Robot Domain",
author={David E. Wilkins},
month="July",
year=1986,
keywords={Planning!Mobile robot domain, SIPE, Robot!Planning},
abstract={
  An application of the  SIPE planning system  to  high-level
task planning for an autonomous indoor mobile robot is presented.  The
primary purpose was to evaluate the adequacy  of SIPE for this domain,
extending  and improving the system in  the process.  The mobile robot
domain as encoded in SIPE and the approach to  interfacing the planner
and the lower-level   routines are described.  The  bulk  of the paper
presents both problems encountered during the process  of encoding this
domain, and extensions of the planning system that  were made to solve
them.

The most   significant addition   was   a  redesign of   the  deductive
capability  of the planner,    which  is described   in   some detail.
Efficiency considerations and the ability  to intermingle planning and
execution are discussed.     The most  important problem   encountered
involved hierarchical  planning,  an  ambiguous term.  We    present a
definition   of  it,  and examine several  of   the  reasons  for this
ambiguity.   An  explication  of hierarchical-planning implementations
entails two distinct notions: abstraction level and planning level.  A
problem  in currently implemented  planners that is caused  by  mixing
these two levels is  presented and various remedies suggested.   Three
solutions  that have been  implemented  in the   current SIPE planning
system are described.}
}

@techreport{aic-tn-1986:387,
number=387,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Model Of Plan Inference That Distinguishes Between The Beliefs  of Actors and Observers",
author="Mrtha E. Pollack",
month="August",
year=1986,
keywords={Planning!Inference, SPIRIT, Multiple Agents!Plan inference},
abstract={
  Existing models of  plan inference (PI)  in  conversation  have
assumed that the agent whose plan is being inferred (the actor)  and the agent
drawing the inference (the  observer) have identical beliefs  about actions in
the domain.  I argue that this assumption often results in failure of both the
PI process  and the communicative process  that PI is  meant  to support.   In
particular, it precludes the principled generation of appropriate responses to
queries that arise from invalid plans.  I describe a model of PI that abandons
this  assumption.    It rests on  an analysis  of plans as   mental phenomena.
Judgments that a plan is invalid are associated with particular discrepancies
between the beliefs that the observer herself holds.  I show that  the content
of an appropriate  response to  a query  is affected by the types  of any such
discrepancies of belief judged to be present in the plan  inferred to underlie
that query.  The  PI model  described here has been  implemented  in SPIRIT, a
small demonstration system that answers questions about the domain of computer
mail.}
}

@techreport{aic-tn-1987:385,
number=385,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Logical Approach To Reasoning By Analogy",
author={Todd R. Davies},
month="July",
year=1987,
keywords={Reasoning!By analogy},
abstract={
  We analyze    the logical form  of  the  domain knowledge  that
grounds analogical inferences and generalizations from a single instance.  The
form of the assumptions which justify analogies is given schematically as  the
``determination rule'', so called because it expresses the relation of one set
of variables   determining  the values  of  another   set.   The determination
relation is a   logical generalization of  the  different  types of dependency
relations defined in database  theory.   Specifically, we define determination
as a  relation between schemata of first  order logic that  have two  kinds of
free variables: (1) object variables and (2) what we call ``polar'' variables,
which hold the place of truth  values.  Determination  rules  facilitate sound
rule inference    and valid conclusions   projected  by   analogy  from single
instances, without  implying  what  the  conclusion should   be  prior to   an
inspection  of  the  instance.  They  also  provide  a  way  to   specify what
information is sufficiently relevant to decide a question, prior  to knowledge
of the answer to the question.}
}

@techreport{aic-tn-1986:384,
number=384,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Simple Reconstruction Of Gpsg",
author={Stuart M. Shieber},
month="May",
year=1986,
keywords={Natural Language!GPSG},
abstract={
    Like most linguistic  theories,  the theory of generalized
phrase structure grammar (GPSG)  has described language axiomatically,
that is,  as a set  of  universal and language-specific constraints on
the well-formedness of linguistic elements of some sort.  The coverage
and detailed analysis  of  English  grammar in the   ambitious  recent
volume "" """""", """"",  """""", """  """ entitle" Generalized Phrase
Structure  Grammar   [2] are  impressive,  in part  because    of  the
complexity of the axiomatic system developed by the authors.   In this
paper, we examine the  possibility  that simpler descriptions  of  the
same theory can be achieved through a slightly different, albeit still
axiomatic, method.   Rather  than characterize   the well-formed trees
directly, we progress in two stages by procedurally characterizing the
well-formedness   axioms themselves, which  in  turn  characterize the
trees.}
}

@techreport{aic-tn-1987:382R,
number=382R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Prolog Technology Theorem Prover:  Implementation By An Extended, Prolog Compiler",
author={Mark E. Stickel},
month="November",
year=1987,
keywords={Deduction!PTTP, PTTP, Prolog},
abstract={
  A Prolog technology theorem prover (PTTP) is an extension of
Prolog that  is complete for the full  first-order predicate calculus.
It differs from Prolog in its use of unification with the occurs check
for soundness, the  model-elimination reduction rule  that is added to
Prolog  inferences   to  make the inference    system   complete,  and
depth-first iterative-deepening   search   instead  of   unbounded
depth-first search to make  the search   strategy  complete. A  Prolog
technology theorem  prover has been implemented  by an extended 
Prolog-to-LISP  compiler  that supports  these  additional  features.   It is
capable of proving theorems in the full first-order predicate calculus
at a rate of thousands of inferences per second.}
}

@techreport{aic-tn-1988:381R,
number=381R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Rex Programmer's Manual",
author={Leslie Pack Kaelbling and Nathan J. Wilson},
month="July",
year=1988,
keywords={Rex},
}

@techreport{aic-tn-1987:380,
number=380,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reasoning and Planning In Dynamic Domains:  An Experiment With A Mobile Robot",
author={Michael P. Georgeff, Amy L. Lansky and Marcel J. Schoppers},
month="April",
year=1987,
keywords={Reasoning!Dynamic domains, Planning!Dynamic domains, Flakey, Robot!Planning},
abstract={
  In this  paper,  the   reasoning  and planning   capabilities  of an
autonomous mobile robot are described.  The reasoning system that controls the
robot is designed  to exhibit the  kind of  behavior expected   of  a rational
agent, and is endowed with the psychological  attitudes of belief, desire, and
intention.  Because these  attitudes are explicitly  represented,  they can be
manipulated  and reasoned  about,  resulting  in  complex   goal-directed  and
reflective behaviors.  Unlike most planning  systems,  the plans or intentions
formed by the robot need only be partly  elaborated before  it decides  to act.
This   allows  the robot to   avoid  overly    strong  expectations about  the
environment,  overly constrained  plans    of  action, and   other  forms   of
overcommitment  common    to previous planners.    In  addition, the  robot is
continuously reactive and has  the ability to  change its goals and intentions
as   situations   warrant.  Thus,  while  the  system  architecture allows for
reasoning about means and ends in  much the same  way as traditional planners,
it also possesses the reactivity required for survival in highly  dynamic  and
uncertain  worlds.  The system  has been  tested   with SRI's autonomous robot
(Flakey) in a space station scenario involving  navigation and the performance
of emergency tasks.}
}

@techreport{aic-tn-1986:379,
number=379,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Stereo Integral Equation",
author={Grahame B. Smith},
month="September",
year=1986,
keywords={Vision!Stereo},
abstract={
  A new approach to the formulation and solution  of  the problem
of recovering  scene topography from a  stereo image pair  is presented.   The
approach circumvents the need to solve the correspondence problem, returning a
solution that   makes surface   interpolation unnecessary.  The    methodology
demonstrates a way of handling  image analysis problems that  differs from the
usual linear-system approach.  We  exploit  the  use of nonlinear functions of
local image measurements to constrain and infer global  solutions that must be
consistent with such measurements.  Because the solution techniques we present
entail certain computational  difficulties,  significant work still lies ahead
before they can be routinely applied to image analysis tasks.}
}

@techreport{aic-tn-1986:378,
number=378,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Using Generic Geometric Knowledge To Delineate Cultural Objects In Aerial Imagery",
author={Pascal Fua and Andrew J. Hanson},
month="March",
year=1986,
keywords={Vision!Object recognition},
abstract={
    We present a   paradigm for discovering   the  outlines of
arbitrarily complex cultural objects  in aerial imagery.  The approach
starts with  a low-level image partition  and generic  (as  opposed to
specific or template-like) object descriptions.  We then use geometric
reasoning and context knowledge to suggest corrections to  the discrepancies
between the  segmentation boundaries and  the object  models.
Finally,   when the corrections   appear consistent   with the generic
cultural object  model, we  resegment  the  partition  to  produce new
labeled regions  with   clear  semantic interpretations.   The general
features of our approach appear to be applicable to  a number of other
domains.}
}

@techreport{aic-tn-1986:377,
number=377,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Epipolar-Plane Image Analysis: A Technique For Analyzing Motion Sequences",
author={Robert C. Bolles and H. Harlyn Baker},
month="February",
year=1986,
keywords={Vision!Epipolar-plane image analysis},
abstract={
  A technique for unifying spatial and temporal analysis of an
image sequence taken by a camera moving in a straight line is
presented.  The technique is based on a ``dense'' sequence of
images--images taken close enough together to form a solid block of
data.  Slices of this solid directly encode changes due to motion of
the camera.  These slices, which have one spatial dimension and one
temporal dimension, are more structured than conventional images.
This additional structure makes them easier to analyze.  We present
the theory behind this technique, describe an initial implementations,
and discuss our preliminary results.}
}

@techreport{aic-tn-1986:376R,
number=376R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Algorithm For Generating Quantifier Scopings",
author={Jerry R. Hobbs and Stuart M. Shieber},
month="October",
year=1986,
keywords={Natural Language!Quantifier scopings},
abstract={
   The syntactic structure of a sentence often manifests quite
clearly the predicate-argument structure and relations of grammatical
subordination.  But scope dependencies are not so transparent.  As a
result, many systems for representing the semantics of sentences have
ignored scoping or generated scopings with mechanisms that have often
been inexplicit as to the range of scopings they choose among or
profligate in the scopings they allow.  

In this paper, we present an algorithm, along with proofs of some of
its important properties, that generates scoped semantic forms from
unscoped expressions encoding predicate-argument structure.  The
algorithm is not profligate as are those based on permutation of
quantifiers, and it can provide a solid foundation for computational
solutions where completeness is sacrificed for efficiency and
heuristic efficacy.}
}

@techreport{aic-tn-1986:375,
number=375,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A System For Reasoning In Dynamic Domains: Fault Diagnosis On The Space Shuttle",
author={Michael P. Georgeff and Amy L. Lansky},
month="January",
year=1986,
keywords={Reasoning!Dynamic domains},
abstract={
  This report describes a reactive  system for  reasoning
about    and performing complex  tasks  in  dynamic   environments.  A
powerful and theoretically sound scheme for representing and reasoning
about actions and processes  has been devised.  The representation  is
sufficiently rich to  describe the effects  of arbitrary  sequences of
tests and  actions, and the  reasoning mechanism provides a  means for
directly using this knowledge to attain  desired goals  or to react to
critical situations.  A declarative  semantics for  the representation
has been constructed  that   allows a  user to   specify   facts about
behaviors independently of context.  An operational semantics has also
been defined that shows  how these facts  can  be  used by a system to
achieve   its goals.  Possession   of   both a    declarative  and  an
operational semantics provides the  system  with the ability to reason
about  complex  actions, to explain its   reasoning to others, to cope
with modifications to   its  environment,  and  to  be   amenable   to
verification.   The system also includes  powerful metalevel reasoning
capabilities, using the same formalism for representing this knowledge
as for object-level  knowledge.  A practical  implementation of such a
system  has been constructed  and applied to  some crucial problems in
the  automation of space operations.}
}

@techreport{aic-tn-1986:374,
number=374,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evidential Knowledge-Based Computer Vision",
author={Leonard P. Wesley},
month="January",
year=1986,
keywords={Evidential Reasoning!Vision, Vision!Evidential Reasoning},
abstract={
  It has  been argued that knowledge-based  systems (KBS)
must reason from evidential information--i.e.,  information that is to
some degree uncertain,  imprecise, and occasionally inaccurate.   This
is  no less true of KBS  that operate in the  domain of computer-based
image interpretation.  Recent research has  suggested that the work of
Dempster    and Shafer (DS)     provides  a  viable    alternative  to
Bayesian-based  techniques for reasoning  from evidential information.
In this  paper, we discuss some   of the differences   between  the DS
theory  and some popular Bayesian-based   approaches to effecting  the
reasoning task.   We then discuss  some work on   integrating   the DS
theory into  a knowledge-based high-level  computer  vision system  in
order to examine various aspects of this new technology that have  not
been explored  to   date.  Results  from  a   large number  of   image
interpretation experiments will  be presented.  These  results suggest
that   a  KBS's performance   improves substantially when  it exploits
various features  of the DS theory that  are not  readily available in
pure Bayesian-based approaches.}
}

@techreport{aic-tn-1986:373,
number=373,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Stochastic Approach To Stereo Vision",
author={Stephen T. Barnard},
month="April",
year=1986,
keywords={Vision!Stereo, Vision!Stochastic approach, Vision!Simulated Annealing},
abstract={
  A  stochastic optimization  approach  to stereo matching  is
presented.   Unlike conventional   correlation matching   and  feature
matching, the  approach   provides a  dense array   of  disparities,
eliminating the  need for interpolation.   First, the  stereo  matching
problem is defined in terms of finding a disparity  map that satisfies
two competing  constraints: (1) matched points  should  have similar
image intensity, and (2) the disparity  map  should be smooth.   These
constraints  are  expressed in  an  ``energy'' function that can   be
evaluated locally.  A simulated annealing algorithm is used to  find a
disparity  map  that has  very low  energy  (i.e., in  which both constraints 
have simultaneously been approximately satisfied).  Annealing
allows the large-scale  structure of  the disparity map   to emerge at
higher temperatures, and avoids the problem of  converging too  quickly
on a local  minimum.    Results are  shown  for   a  sparse  random-dot
stereogram, a  vertical  aerial stereogram (shown   in comparison   to
ground truth),  and an   oblique  ground-level  scene  with  occlusion
boundaries.}
}

@techreport{aic-tn-1985:372,
number=372,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A General Selection Criterion For Inductive Inference",
author={Michael P. Georgeff and Christopher S. Wallace},
month="December",
year=1985,
keywords={Reasoning!Induction, Reasoning!Abduction},
abstract={
  This paper presents a   general criterion for measuring  the
degree to which any given theory can be  considered a good explanation
of a particular body of data.  A formal definition of what constitutes
an acceptable explanation of a body of data  is given,  and the length
of explanation used as  a measure for selecting  the best of a set  of
competing  theories.    Unlike most previous  approaches to  inductive
inference, the length  of explanation includes a measure  of the  complexity 
or likelihood of a  theory as well as a  measure of the degree
of fit between theory and data.  In this way, prior expectations about
the environment can be represented, thus providing a hypothesis  space
in which  search for  good or optimal theories  is made more tractable.
Furthermore, it is shown how theories can be represented as structures
that reflect the conceptual entities used to describe and  reason about
the given problem domain.}
}

@techreport{aic-tn-1985:371,
number=371,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Linear Precedence In Discontinuous Constituents:  Complex Fronting In German",
author={Hans Uszkoreit},
month="October",
year=1985,
keywords={Natural Language!Discontinuous constituents},
abstract={
  Syntactic processes that have been identified as  sources of
discontinuous constituents  exhibit radically    different properties.
They seem to   fall  into  several  classes:   leftward  ``extraction,''
right-ward  ``movements,'' ``scrambling'' phenomena,  and  parenthetical
insertions.  Current linguistic theories differ as to the formal tools
they employ both for   describing the participating  syntactic phenomena
and for encoding the resulting representations.
	
In this paper, the general problem of  determining the linear order in
the discontinuous parts of a constituent is discussed.  The focus lies
on  frameworks that  use their feature mechanisms  for connecting  the
noncontiguous elements.  It is  then shown that  the current framework
of Generalized Phrase Structure   Grammar  (GPSG)  is not  suited  for
describing the  interaction of leftward  extractions,  scrambling, and
constraints  on linear  order.  The  relevant  data come  from  German
fronting.  Previous analyses (Johnson 1983;  Nerbonne  1984; Uszkoreit
1982; 1984)  have   neglected certain types of fronting   or failed to
integrate their account  of  fronting properly  with   an analysis  of
linear precedence.}
}

@techreport{aic-tn-1985:369,
number=369,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Structures Of Discourse Structure",
author={Barbara J. Grosz},
month="November",
year=1985,
keywords={Natural Language!Discourse structure},
}

@techreport{aic-tn-1985:368,
number=368,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reference and Denotation:  The Descriptive Model",
author={Amichai Kronfeld},
month="October",
year=1985,
keywords={Natural Language!Referring, Natural Language!Denotation},
}

@techreport{aic-tn-1985:367,
number=367,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="One-Eyed Stereo: A Unified Strategy To Recover Shape From A Single Image",
author={Thomas M. Strat},
month="November",
year=1985,
keywords={Vision!Stereo},
abstract={
  A single two-dimensional image  is   an  ambiguous
representation of  the three-dimensional world--many  different scenes
could have produced the  same image--yet the  human  visual  system  is
extremely successful at recovering a qualitatively  correct depth model
from   this   type of  representation.  Workers    in    the field  of
computational  vision  have devised a  number of distinct schemes that
attempt to     emulate this human  capability;    these   schemes  are
collectively known as   ``shape  from....'' methods  (e.g., shape from
shading, shape from texture, or shape from contour).  In this paper we
contend that the distinct assumptions  made in each  of  these schemes
must be tantamount to   providing a  second (virtual)  image    of the
original scene, and that any one of these approaches can be translated
into a conventional stereo formalism.  In particular,  we  show that it
is frequently  possible to structure the  problem as one of recovering
depth from a stereo pair consisting of the  supplied perspective image
(the original image)   and  an hypothesized  orthographic  image   (the
virtual image).  We present a new algorithm of  the  form required  to
accomplish this type of stereo reconstruction task.}
}

@techreport{aic-tn-1985:366,
number=366,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Stereo Challenge Data Base",
author={Marsha Jo Hannah},
month="October",
year=1985,
keywords={Vision!Stereo},
}

@techreport{aic-tn-1985:365,
number=365,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evaluation Of Stereosys Vs. Other Stereo Systems",
author={Marsha Jo Hannah},
month="October",
year=1985,
keywords={Vision!Stereo},
}

@techreport{aic-tn-1985:364,
number=364,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Constraints On Order",
author={Hans Uszkoreit},
month="October",
year=1985,
keywords={Natural Language!GPSG},
abstract={
   Partially free  word  order  as   it occurs  in German  and
probably to  some extent in  all natural languages  arises through the
interaction  of potentially   conflicting ordering   principles.     A
modified  linear precedence  (LP)  component  of   Generalized  Phrase
Structure Grammar (GPSG) is proposed that accommodates  partially free
word  order.  In  the  revised framework,  LP  rules  are  sets  of LP
clauses.  In a  case in which  these clauses make conflicting ordering
predictions, more than one order is grammatical.  LP clauses may refer
to   different  types  of  categorial  information  such  as  category
features, morphological  case,   thematic roles, discourse  roles, and
phonological  information.   The  modified   framework is  applied  to
examples from  German.   It is demonstrated how  the new  LP component
constrains the linear ordering of major nominal categories.}
}

@techreport{aic-tn-1985:363,
number=363,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Experimental Robot Psychology",
author={Kurt G. Konolige},
month="November",
year=1985,
keywords={Reasoning!Robot psychology},
abstract={
   In  this paper I argue  that  an intentional methodology is
appropriate  in  the design of  robot  agents in  cooperative planning
domains--at least in those domains that are sufficiently open-ended to
require extensive  reasoning  about the  environment  (including other
agents).  That is, we should take seriously the notion that an agent's
cognitive state expresses beliefs about the world, desires or goals to
change the world, and intentions  or plans that  are likely to achieve
these goals.  In cooperative situations, reasoning about  these cognitive
structures is  important for  communication  and problem-solving.
How can  we construct such models of  agent cognition?  Here I propose
an approach that I  call it  experimental  robot psychology because it
involves formalizing and reasoning about the  design of existing robot
agents.  It shows promise of yielding an efficient  and  general means
of reasoning about cognitive states.}
}

@techreport{aic-tn-1985:362,
number=362,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Formal Theories Of Knowledge In Ai and Robotics",
author={Stanley J. Rosenschein},
month="September",
year=1985,
keywords={Knowledge Representation!Theories, Situated Automata},
abstract={
   Although the concept  of knowledge plays a  central role in
artificial  intelligence, the   theoretical foundations of   knowledge
representation currently rest on a very limited conception  of what it
means for a machine to know a proposition.  In the  current view,  the
machine is regarded as knowing a  fact  if its state either explicitly
encodes the fact as a sentence of an interpreted formal language or if
such a sentence can be derived  from other encoded sentences according
to  the rules  of an appropriate  logical system.   We  contract  this
conception, the interpreted-symbolic-structure approach, with another,
the situated-automata approach,  which seeks  to analyze  knowledge  in
terms of relations between the state of a machine and the state of its
environment over   time using logic  as a  metalanguage  in  which the
analysis is carried out.}
}

@techreport{aic-tn-1985:361,
number=361,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="More Notes From The Unification Underground:  A Second Compilation Of Papers On Unification-Based Grammar Formalisms",
author={Stuart M. Shieber, Lauri Karttunen, Fernando Pereira and Martin Kay},
month="August",
year=1985,
keywords={Natural Language!Unification},
}

@techreport{aic-tn-1985:359,
number=359,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Weak Logic Of Knowledge and Belief:  Epistemic and Doxastic Logic For The Yuppie Generation",
author={David Israel},
month="",
year=1985,
keywords={Logic!Epistemic, Logic!Doxastic},
}

@techreport{aic-tn-1985:358,
number=358,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Preliminary Report On A Theory Of Plan Synthesis",
author={Edwin P.D. Pednault},
month="August",
year=1985,
keywords={Planning!Synthesis},
}

@techreport{aic-tn-1986:357R,
number=357R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Perceptual Organization and The Representation Of Natural Form",
author={Alex P. Pentland},
month="July",
year=1986,
keywords={Vision!Perceptual organization},
abstract={
  To support  our  reasoning abilities perception must recover
environment  regularities--e.g.,  rigidity,  ``objectness,'' axes   of
symmetry--for later use of cognition.  To create a  theory of  how our
perceptual apparatus can produce meaningful  cognitive primitives from
an   array of  image  intensities  we   require a representation whose
elements may  be lawfully  related to  important physical regularities,
and that correctly describes the perceptual organization people impose
on   the   stimulus.   Unfortunately,  the  representations   that are
currently  available  were  originally developed    for other purposes
(e.g., physics, engineering) and have so far proven unsuitable for the
problems of perception  or  commonsense  reasoning.  In answer to this
problem   we  present a representation that   has proven competent  to
accurately  describe  an  extensive  variety of  natural forms  (e.g.,
people, mountains,  clouds, trees), as well   as man-made forms,  in a
succinct   and  natural   manner.   The   approach    taken    in this
representational system is to describe scene structure at a scale that
is similar  to  our naive perceptual notion  of ``a part,''  by use of
descriptions that reflect a possible formative history of  the object,
e.g., how the object  might have been  constructed from lumps of clay.
For this representation to  be useful it must be  possible to  recover
such descriptions from image data; we show that the primitive elements
of  such  descriptions  may  be recovered   in an  overconstrained and
therefore reliable manner.  We  believe  that this  descriptive system
makes an important contribution  towards solving current problems   in
perceiving  and reasoning  about  natural   forms by allowing us    to
construct  accurate descriptions that  are  extremely compact and that
capture people's   intuitive  notions  about the    part  structure of
three-dimensional forms.}
}

@techreport{aic-tn-1986:356R,
number=356R,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title=" Team: An Experiment In The Design Of Transportable Natural-Language Interfaces",
author={Barbara Grosz and Douglas E. Appelt and Paul Martin and Fernando Pereira},
month="September",
year=1986,
keywords={TEAM, Natural Language!Interfaces},
abstract={
  This paper describes TEAM, a transportable  natural-language
interface system.   TEAM was constructed to test   the  feasibility of
building a natural-language system that could  be adapted to interface
with new databases by users who  were not experts  in natural-language
processing.  The  paper presents an   overview  of the  system design,
emphasizing those    choices that  were imposed   by  the demands   of
transportability.    It    discusses   several  general problems    of
natural-language   processing that  were   faced  in constructing  the
system, including quantifier  scoping, various  pragmatic issues,  and
verb acquisition.  The paper also provides  a comparison  of TEAM with
several   other   transportable  systems;    the  comparison  includes
discussion of the range of natural language handled by each as well as
a description of the approach taken to transportability in each.}
}

@techreport{aic-tn-1985:355,
number=355,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Special Relations In Automated Deduction",
author={Zohar Manna and Richard Waldinger},
month="June",
year=1985,
keywords={Deduction!Special relations},
abstract={
  Two deduction  rules  are introduced   to  give  streamlined
treatment   to relations of   special  importance    in  an  automated
theorem-proving  system.     These  rules,    the  \underline{relation
replacement} and \underline{relation matching} rules, generalize to an
arbitrary binary relation the paramodulation  and  E-resolution rules,
respectively,  for  equality, and may  operate within  a nonclausal  or
clausal system.  The new rules depend on an extension of the notion of
\underline{polarity} to apply to subterms as  well as to subsentences,
with respect to  a given  binary  relation.  The   rules  allow us  to
eliminate troublesome  axioms, such as  transitivity and monotonicity,
from the system; proofs are shorter and  more comprehensible, and  the
search space is correspondingly deflated.}
}

@techreport{aic-tn-1985:354,
number=354,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Criteria For Designing Computer Facilities For Linguistic Analysis",
author={Stuart Shieber},
month="April",
year=1985,
keywords={Natural Language!Tools, PATR},
abstract={
  In  the natural-language-processing  research community, the
usefulness of computer tools for testing linquistic  analyses is often
taken for granted.  Linguists, on the  other hand, have generally been
unaware of  or  ambivalent  about such devices.  We   discuss  several
aspects of   computer use that  are   preeminent  in  establishing the
utility for linguistic research of computer tools and describe several
factors that must be considered in  designing  such computer tools  to
aid in testing linguistic analyses of grammatical phenomena.  A series
of  design alternatives, some    theoretically  and some   practically
motivated, is  then  based on the  resultant criteria.  We present one
way of pinning down these choices which culminates in a description of
a particular grammar formalism  for use  in computer linguistic tools.
The   PATR-II  formalism thus   serves  to  exemplify   our    general
perspective.}
}

@techreport{aic-tn-1985:353,
number=353,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Configurational Variation In English: A Study Of Extraposition and Related Matters",
author={Susan U. Stucky},
month="March",
year=1985,
keywords={Natural Language!Extraposition},
abstract={
   Natural  languages typically permit more than  one order of
words or phrases, though they differ  with respect to  both the amount
of  order variation allowed  and the  kind  of  information carried by
these differences in order.  In  some  languages, linear order conveys
information about  the  argument relations.  In others,   this role is
performed by  morphology  alone.    Linear  order may  otherwise   bear
information about the status  of the  content of an  utterance  in the
discourse--whether it is new or expected, for instance.  Even within a
particular   language,  different   orders    may carry  fundamentally
disparate kinds of information.}
}

@techreport{aic-tn-1986:351R,
number=351R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Origin Of The Binary-Search Paradigm",
author={Richard Waldinger and Zohar Manna},
month="October",
year=1986,
keywords={Program Synthesis},
abstract={
  In  a binary-search  algorithm   for  the computation  of  a
numerical function, the interval in which the desired output is sought
is divided in  half at  each iteration.  The  paper considers how such
algorithms might be derived from their specifications by an  automatic
program-synthesis system.  The derivation of the binary-search concept
has  been found to   be surprisingly  straightforward.   The  programs
obtained, though reasonably simple  and efficient, are quite different
from  those  that  would have been  constructed by   informal  means.}
}

@techreport{aic-tn-1985:350,
number=350,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Synchronization Of Multiagent Plans Using A Temporal Logic Theorem Prover",
author={Christopher Stuart},
month="December",
year=1985,
keywords={Multiple Agents!Planning, Logic!Temporal, Deduction!Temporal},
}

@techreport{aic-tn-1985:348,
number=348,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Cognitivist Reply To Behaviorism",
author={Robert C. Moore},
month="March",
year=1985,
keywords={Cognitivism},
abstract={
  The objections  to mentalistic psychology  raised by Skinner
in ``Behaviorism  at Fifty'' [Skinner,  1984]   are reviewed,  and it is
argued that a ``cognitivist''  perspective offers a  way of constructing
mentalistic theories that overcome these objections.}
}

@techreport{aic-tn-1985:347,
number=347,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Triangle Tables: A Proposal For A Robot Programming Language",
author={Nils J. Nilsson},
month="February",
year=1985,
keywords={Robot!Programming language},
abstract={
   Structures called   \underline{triangle} \underline{tables}
were used  in connection  with the  SRI  robot \underline{Shakey}  for
storing  sequences  of robot  actions.    Because  the  rationale  for
triangle tables still seems  relevant, I have recently elaborated  the
original concept and have begun to consider how the expanded formalism
could be used as a  general robot-programming  language.  The present
article describes this new view of triangle tables and  how they might
be used in a language that  supports asynchronous ad concurrent action
computations.}
}

@techreport{aic-tn-1985:346,
number=346,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Recovering From Execution Errors In Sipe",
author={David E. Wilkins},
month="January",
year=1985,
keywords={SIPE, Planning!Error recovery},
abstract={
  In real-world  domains (e.g.,  a mobile robot  environment),
things do not always proceed as planned, so it is important to develop
better  execution-monitoring techniques  and replanning  capabilities.
This paper describes these capabilities in  the SIPE  planning system.
The   motivation behind SIPE  is to  place  enough  limitations on the
representation so that planning can be done efficiently, while retaining 
sufficient power to still be useful.  This  work  assumes that new
information given to  the   execution  monitor   is  in  the  form  of
predicates,  thus avoiding the difficult  problem   of how to generate
these predicates from information provided by sensors.

The replanning  module  presented here  takes  advantage  of the  rich
structure of SIPE plans and is  intimately connected with the planner,
which can be called  as a subroutine.   This allows the  use of SIPE's
capabilities to determine efficiently how unexpected events affect the
plan being executed and, in many cases, to retain most of the original
plan  by making changes  in it  to avoid  problems   caused  by  these
unexpected  events.   SIPE is also capable of  shortening the original
plan   when serendipitous events occur.  A   general set of replanning
actions is presented along  with a general  replanning capability that
has been implemented by using these actions.}
}

@techreport{aic-tn-1984:345,
number=345,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="On The Mathematical Properties Of Linguistic Theories",
author={C. Raymond Perrault},
month="November",
year=1984,
keywords={Natural Language!Linguistic theories},
abstract={
   Metatheoretical   findings    regarding  the  decidability,
generative capacity, and  recognition  complexity of several  syntactic
theories are surveyed.  These  include context-free, transformational,
lexical-functional,  generalized  phrase structure, tree  adjunct, and
stratificational grammars.  The paper  concludes  with a discussion of
the implications of these results with respect to linguistic theory.}
}

@techreport{aic-tn-1984:344,
number=344,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Representation Of Adverbs, Adjectives and Events In Logical Form ",
author={William Croft},
month="December",
year=1984,
keywords={Logic!Representation},
abstract={
  The  representation  of  adjectives   and   their  adverbial
counterparts in logical form rises a number of issues in the  relation
of  (morpho)syntax to semantics, as well  as more specific problems of
lexical and grammatical analysis.   This paper addresses those  issues
which  have bearing on the relation  of properties to  events.   It is
argued that attributes and context play only  an indirect  role in the
relation between  properties  and  events.     The body of the   paper
addresses  the criteria for  relating  surface forms  to  logical form
representations, and offers a unified analysis of adjectives and their
adverbial  counterparts  in logical  form while   maintaining  a  clear
distinction  between  operators  and predicates;  this   requires  the
postulation of a factive sentential operator and the relaxation of the
one-to-one  syntax-semantics  correspondence  hypothesis.  Criteria for
determining the number  of  arguments for a  predicate are established
and  are  used for    the  analyses  of  phenomena such   as  passive-sensitivity, 
lexical derivational patterns, and gradability.}
}

@techreport{aic-tn-1984:343,
number=343,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Team User's Guide ",
author={Lorna Shinkle},
month="October",
year=1984,
keywords={TEAM, Natural Language!TEAM|see{TEAM}},
abstract={
   TEAM (\underline{T}ransportable   \underline{E}nglish  Data
\underline{A}ccess      \underline{M}edium)      is   a  transportable
natural-language (NL) interface  to  a   database.  It  is a   tool of
considerable power that enables the user to retrieve data  and  elicit
answers to queries by asking questions and giving commands  in English
instead of a formal query language.  Moreover,  TEAM is not limited to
any particular   database,    but  can  be   adapted   to  demonstrate
natural-language retrieval in a  broad variety of application domains.
The prototype TEAM  software  described herein  was  developed by  the
Artificial Intelligence Center of SRI International to demonstrate the
system's capabilities and adaptive potential.

This user's guide is designed to assist new TEAM users to learn about the
concepts and tasks involved in retrieving data and in preparing a demonstration
for a new application area.  An effort has been made to illustrate some of the
problems TEAM must solve in translating an English question into a database
query.  However, the necessarily limited scope of this guide cannot include a
discussion of all the natural-language-processing issues addressed by the
system; our emphasis is on a practical, rather than theoretical,
understanding of the concepts.  Similarly, while this guide cannot
cover every detail of creating a new demonstration for TEAM, it does
provide a thorough introduction to the procedure to be followed and
explains how to use the on-line ``help'' provided by the system.

This introductory manual is  designed  to be read in  conjunction with
actual use of the system.  While a casual perusal of this document may
acquaint the reader with  some of TEAM's features,  using the examples
and suggestions as  a practical  guide to actually  experimenting with
the system will prove a much more effective method  of learning how to
use  TEAM and becoming  familiar  with   both its capabilities and its
limitations.  Much of this guide  consists of comments,  descriptions,
and  other background  information,  but   the  user   is   frequently
instructed  to perform  some action as  a learning experience.  In the
examples shown in the text, the portions printed in boldface are typed
as selected by the  user; these portions   may  or may  not appear  in
boldface on the screen when TEAM is used.  In the examples illustrated
by  figures, however,  the type faces   do correspond exactly  to  the
screen display.}
}

@techreport{aic-tn-1984:342,
number=342,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Description Of Sri's Baseline Stereo System",
author={Marsha Jo Hannah},
month="October",
year=1984,
keywords={Vision!Stereo},
abstract={
  We are  implementing  a baseline system  for automated area-based 
stereo compilation.  This  system, STSYS, operates   in  several
passes over the data, during which  it iteratively builds, checks, and
refines its model of the 3-dimensional world, as represented by a pair
of  images.  In this papers,  we describe the  components of STSYS and
give examples of the results it produces.  We find  that these results
agree  reasonably well with  those  produced on the interactive   DIMP
system at ETL, the best available benchmark.}
}

@techreport{aic-tn-1984:340,
number=340,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Automated Deduction By Theory Resolution",
author={Mark E. Stickel},
month="October",
year=1984,
keywords={Deduction!Theory Resolution},
abstract={
  Theory resolution  constitutes a set of  complete procedures
for incorporating theories into  a resolution theorem-proving program,
thereby making it unnecessary to resolve  directly upon axioms  of the
theory.  This can greatly reduce  the  length of proofs and  the size o
the search space.  Theory resolution effects a  beneficial division of
labor, improving the performance of the theorem  prover and increasing
the applicability of the   specialized  reasoning procedures.    Total
theory resolution utilizes a  decision procedure that   is  capable of
determining unsatisfiability of any set of clauses using predicates in
the theory.  Partial  theory  resolution  employs a   weaker decision
procedure that  can  determine potential unsatisfiability   of sets of
literals.  Applications include the  building in of  both mathematical
and  special decision procedures, e.g., for  the taxonomic information
furnished by a knowledge representation system.   Theory resolution is
a generalization of numerous previously  known resolution refinements.
Its   power  is  demonstrated  by  comparing  solutions of ``Schubert's
Steamroller'' challenge problem  with or  without building  in   axioms
through theory resolution.}
}

@techreport{aic-tn-1984:339,
number=339,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Simple and Efficient Implementation Of Higher-Order Functions In Lisp",
author={Michael P. Georgeff and Stephen F. Bodnar},
month="December",
year=1984,
keywords={LISP!Higher-order functions},
abstract={
   A  relatively   simple  method  for  handling  higher-order
functions (funargs) in  LISP is described.  It  is also shown how this
scheme  allows extension  of   the LISP  language  to  include partial
application of functions.

The basis  of the approach  is  to defer evaluation of function-valued
expressions until sufficient arguments have been accumulated to reduce
the expression to  a nonfunctional value.   This results in  stacklike
environment structures rather than the treelike structures produced by
standard evaluation  schemes.   Consequently,  the  evaluator  can  be
implemented on a standard runtime stack without requiring  the complex
storage management schemes usually employed for  handling higher-order
functions.

A full version of  LISP has been  implemented  by  modifying the FRANZ
LISP interpreter  to incorporate the new  scheme.   These modifications
prove to be both simple and efficient.}
}

@techreport{aic-tn-1984:338,
number=338,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Parallel Guessing:  A Strategy For High-Speed Computation",
author={Martin A. Fischler and Oscar Firschein},
month="September",
year=1984,
keywords={Vision!Parallel processing},
abstract={
  Attempts   have been  made  to speed  up image-understanding
computation  involving conventional serial algorithms  by  decomposing
these  algorithms  into portions  that   can be computed  in parallel.
Because many classes of algorithms do not readily decompose, one seeks
some other  basis for parallelism (i.e, for  using additional hardware
to  obtain higher processing  speed).    In this paper   we argue that
``parallel guessing'' for image analysis is a useful approach,  and that
several  recent IU algorithms  are based on   this  concept.  Problems
suitable for   this  approach have   the   characteristic  that either
``distance'' from a true solution, or the correctness of a guess, can be
readily  checked.   We  review  image-analysis   algorithms  having  a
parallel guessing or randomness flavor.

We envision a parallel set of computers,  each  of which carries out a
computation on a data  set using some random or  guessing process, and
communicate the ``goodness'' of its results to its co-workers through  a
``blackboard'' mechanisms.}
}

@techreport{aic-tn-1984:337,
number=337,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Possible-World Semantics For Autoepistemic Logic",
author={Robert C. Moore},
month="August",
year=1984,
keywords={Logic!Autoepistemic, Possible Worlds},
abstract={
   In a  previous paper [Moore, 1983a, 1983b],  we presented a
nonmonotonic logic for modeling the beliefs of ideally rational agents
who reflect on their own beliefs, which we call ``autoepistemic
logic.'' 
We define a simple and intuitive semantics for autoepistemic logic  and
proved  the logic  sound and complete  with respect to that semantics.
However, the  nonconstructive character  of  both the  logic  and  its
semantics made it difficult to prove the existence of sets  of beliefs
satisfying all  the constraints  of  autoepistemic logic.    This note
presents an alternative,  possible-world  semantics  for autoepistemic
logic that  enables us to  construct finite models  for  autoepistemic
theories,   as well as   to  demonstrate the  existence   of sound and
complete autoepistemic theories based on given sets of premises.}
}

@techreport{aic-tn-1984:335,
number=335,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Role Of Logic In Artificial Intelligence",
author={Robert C. Moore},
month="July",
year=1984,
keywords={Logic!Role in AI},
abstract={
   Formal logic  has played  an  important part in  artificial
intelligence (AI) research  for almost thirty years, but  its role has
always   been    controversial.   This paper   surveys  three possible
applications  of logic in  AI: (1)  as  an analytical  tool,  (2) as a
knowledge representation formalism and method of reasoning, and (3) as
a programming language.   The paper examines each  of these  in  turn,
exploring   both the problems  and the  prospects  for  the successful
application of logic.}
}

@techreport{aic-tn-1984:334,
number=334,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Goal-Directed Textured-Image Segmentation",
author={Kenneth I. Laws},
month="September",
year=1984,
keywords={SLICE, Vision!Segmentation},
abstract={
   The   SLICE   textured-image segmentation system identifies
image regions that differ  in gray-level distribution, color,  spatial
texture,  or other local  property.    It  has been  developed for the
analysis of aerial imagery, although it can be used  for any domain in
which homogeneous image regions must  be found prior to interpretation
or enhancement.  This report  concentrates on textured-image  segmentation
using local  texture-energy  measures and user-delimited training
regions.

The   SLICE  algorithm   combines   knowledge of  target  textures  or
signatures  with    knowledge  of    background    textures  by   using
histogram-similarity transforms.    Regions of  high   similarity to a
target  texture and  of low  similarity  to any negative examples  are
identified and then  mapped back  to the original  image.  This use of
texture-similarity transforms during the segmentation process improves
segmenter performance  and focuses  segmentation activity  on material
types of greatest  interest.    The   system can  also  be   used  for
goal-independent      texture  segmentation    by       omitting   the
similarity-transform computations,  and   its hierarchical,  recursive
segmentation strategy integrates very well with  other image-analysis
techniques.}
}

@techreport{aic-tn-1984:333,
number=333,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Fast Surface Interpolation Technique",
author={Grahame B. Smith},
month="August",
year=1984,
keywords={Vision!Surface interpolation},
abstract={
  A  method for interpolating a  surface  through 3-D data  is
presented.  The method is computationally efficient and general enough
to allow  the construction of  surfaces with  either  smooth  or rough
texture.}
}

@techreport{aic-tn-1984:332,
number=332,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evaluation Of Scene-Analysis Algorithms",
author={Kenneth I. Laws},
month="August",
year=1984,
keywords={Vision!Scene analysis},
abstract={
  A software evaluation methodology has been  developed at SRI
International   for evaluating  contributions to    the ARPA/DMA Image
Understanding Testbed.  This paper  describes the  criteria  that have
shaped  the evaluation methodology.  Diverse   examples of  evaluation
results are presented for the GHOUGH object detection system from  the
University of   Rochester,   the  PHOENIX  segmentation system     from
Carnegie-Mellon University  (CMU), and  the RELAX  relaxation  package
from the University of Maryland.}
}

@techreport{aic-tn-1984:331,
number=331,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Image-To-Image Correspondence:  Linear-Structure Matching",
author={Grahame B. Smith and Helen C. Wolf},
month="July",
year=1984,
keywords={Vision!Image correspondence},
abstract={
  We examine the task of matching images of a  scene when they
are  taken   from  very  different vantage   points,    when  there is
considerable scale   change, and   when the  image   orientations  are
unknown.  We use the  linear structures in  the scene as the  basis of
our correspondence  procedure.   This paper  considers the  problem of
describing   the  linear  structures in  a   manner that  is invariant
relative to the variations that can occur among images, and  discusses
a method of  finding the best  description of the linear structures.}
}

@techreport{aic-tn-1984:330,
number=330,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evidence Against The Context-Freeness Of Natural Language",
author={Stuart M. Shieber},
month="June",
year=1984,
keywords={Natural Language},
abstract={
    In searching for  universal  constraints  on  the class of
natural languages, linquists   have investigated a   number of  formal
properties, including that of  context-freeness.  Soon after Chomsky's
categorization  of languages into his  well-known hierarchy  [Chomsky,
1963], the common conception of the context-free class of languages as
a    tool for describing    natural languages  was   that it  was  too
restrictive a class--interpreted \underline{strongly}  (as a   way  of
characterizing structure sets) and even  \underline{weakly} (as  a way
of characterizing string sets).}
}

@techreport{aic-tn-1984:329,
number=329,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Sublanguage and Knowledge",
author={Jerry R. Hobbs},
month="June",
year=1984,
keywords={Natural Language!Sublanguage},
}

@techreport{aic-tn-1984:328,
number=328,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Tablog:  The Deductive-Tableau Programming Language",
author={Richard Waldinger and Yonathan Malachi and Zohar Manna},
month="September",
year=1984,
keywords={Deduction!Deductive Tableau, TABLOG},
abstract={
  TABLOG  (Tableau Logic  Programming Language) is  a language
based  on  first-order predicate   logic with equality    that combines
functional  and logic programming.  TABLOG  incorporate  advantages of
LISP and PROLOG.

A program  in TABLOG is a  list of formulas   in  a first-order  logic
(including  equality, negation, and  equivalence) that is more general
and  more expressive  than  PROLOG's  Horn  Clauses.   Whereas  PROLOG
programs  must  be  relational,  TABLOG  programs  may  define  either
relations or functions.    While LISP programs  yield   results  of  a
computation by returning a single output value, TABLOG programs can be
relations and can produce several results simultaneously through  their
arguments.

TABLOG employs the  Manna-Waldinger \underline{deductive-tableau} proof  system as
an interpreter  in the same  way that  PROLOG  uses a resolution-based
proof system.  Unification is used by  TABLOG to match  a call with  a
line   in the  program and   to  bind arguments.  The  basic  rules of
deduction used  for computing are nonclausal  resolution and rewriting
by means of equality and equivalence.

A pilot interpreter for the language has been implemented.}
}

@techreport{aic-tn-1984:327,
number=327,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Notes From The Unification Underground:  A Compilation  Of Papers On Unification-Based Grammar Formalisms",
author={Stuart M. Shieber and Lauri Karttunen and Fernando C.N. Pereira},
month="June",
year=1984,
keywords={Natural Language!Unification, PATR},
abstract={
  This report is a compilation of papers by the PATR group in the
Artificial Intelligence Center at SRI International reporting on ongoing
research on both practical and theoretical issues concerning grammar formalisms.
The current formalism being simultaneously designed, implemented and used by
the group, PATR-II, is based on unification of directed-graph structures.
Unification is thus a theme both of our research, and of the papers reproduced
in this volume.  These papers provide an overview of the design of PATR-II
(Chapter 1), a discussion of the use of disjunction and negation in
unification-based feature systems (Chapter 2), and a theoretical framework for
unification-based grammar formalisms which is founded on the domain-theoretic
techniques of Data Scott (Chapter 3).  All three chapters are versions of
papers presented at the Tenth International Conference on Computational
Linguistics at Stanford University, Stanford, California during July 2 through
7, 1984.

The research was initially part  of the KLAUS (Knowledge  Learning and
Using  System)   project  at  SRI,  set  up  with    the  intention of
experimenting with  mathematically well-founded   alternatives  to the
DIALOGIC  natural-language  processing system.   The  more theoretical
research  was made possible in  part  by a    gift from the    Systems
Development  Foundation and was conducted  as  part of  a  coordinated
research  effort with  the Computer Language subprogram  at the Center
for the Study of Language and Information, Stanford University.

The PATR group at SRI is a rather liquid  group  of  researchers which
has included, at  various times, John  Bear, Lauri Karttunen, Fernando
Pereira, Jane Robinson, Stan Rosenschein, Stuart Shieber, Susan Stucky,
Mabry Tyson, and  Hans Uszkoreit.  The research  reported here  is   a
direct result of their aid and interaction.  However,  they should not
be blamed for any errors in the present  work, nor should the opinions
expressed  herein  be  construed   as  indicative   of  their personal
predilections.}
}

@techreport{aic-tn-1984:326,
number=326,
price="$18.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Deduction Model Of Belief and Its Logics",
author={Kurt Konolige},
month="August",
year=1984,
keywords={Reasoning!About beliefs} ,
abstract={
  Reasoning  about the knowledge and beliefs   of computer and
human agents  is   assuming    increasing importance   in   artificial
intelligence systems  for  natural  language, understanding, planning,
and knowledge representation.  A  natural  model  of  belief for  robot
agents is the deduction model:  an agent is represented  as having  an
initial set of beliefs about the world in some internal language and a
deduction process for deriving some (but not  necessarily all) logical
consequences of  these   beliefs.   Because the  deduction  model   is
explicitly computational model,  it is possible to   take into account
limitations of an agent's resources when reasoning.

This thesis is an investigation of a Gentzen-type formalization of the
deductive model of belief.  Several original results are proved.  Among
these  are soundness and completeness theorems  for a  deductive belief
logic; a correspondence  result  that  relates our deduction  model  to
competing  possible-world models;  and a model analog  to   Herbrand's
Theorem for the  belief logic.  Specialized  techniques for  automatic
deduction based on resolution are developed using this theorem.

Several other topics of knowledge and belief are explored in the
thesis from the viewpoint of the deduction model, including:
\begin{itemize}
\item A theory of introspection about self-beliefs

\item A theory of circumscriptive ignorance, in which facts that an agent
       doesn't know are formalized by limiting or circumscribing the
       information available to him.
\end{itemize}

This report is a slightly revised version of a thesis submitted to the
Department of Computer Science at Stanford University in June 1984, in
partial fulfillment of the requirements for the degree of Doctor of
Philosophy.}
}

@techreport{aic-tn-1984:325,
number=325,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Inductive Approach To Figural Perception",
author={Stephen T. Barnard},
month="September",
year=1984,
keywords={Vision!Figural perception},
abstract={
  The  problem  of  interpreting single  images   of  abstract
figures is addressed.  It is argued that neither rule-based  deductive
inference nor   model-based matching are   satisfactory  computational
paradigms for this problem.  As an alternative, an inductive  approach
consisting  of two parts is  presented.   The  first  part  involves a
scheme, based on  differential geometry, for describing  the shapes of
curves  and surfaces,  and  for generating    these descriptions  from
images.  The  second  part of the approach relies  on  a criterion for
deciding  which   description, among the candidates  allowed   by  the
constraints in the image, is to be preferred.  This criterion--minimum
entropy--is   related    to  concepts   from     Gestalt   psychology,
thermodynamics, and information theory.  Several examples are given to
illustrate the inductive approach.}
}

@techreport{aic-tn-1984:324,
number=324,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reasoning About Control:  An Evidential Approach",
author={Leonard P. Wesley and John D. Lowrance and Thomas D. Garvey},
month="July",
year=1984,
keywords={Evidential Reasoning!About control},
abstract={
   Expert  systems  that  operate  in   complex   domains  are
continually confronted with  the problem of deciding  what to do next.
Being able to reach a decision requires, in part,  having the capacity
to  ``reason'' about a set of  alternative actions.  It  has been argued
that  expert systems  must  reason  from evidential information--i.e.,
uncertain, incomplete,   and    occasionally  inaccurate   information
[Low82a].  As a consequence, a model for reasoning about control  must
be capable  of performing several   tasks:  to combine the  evidential
information that  is generically distinct  and from disparate sources;
to overcome minor  inaccuracies in the evidential  information that is
needed to reach a decision; to reason about what additional evidential
information is required; to explain the actions  taken (based on  such
information) by the system.  These are a few of the formidable control
problems that remain largely unsolved [Bar82].  If expert systems  are to
improve   their   performance  significantly,    they   must   utilize
increasingly  sophisticated and general models  for  dealing with  the
evidential information required for reasoning about their behavior.  To
this  end we present   an  alternative evidentially-based  approach to
reasoning   about control that  has  several  advantages over  existing
techniques.  It  enables us  to  reason   from  limited and  imperfect
information; to partition bodies of meta-  and domain-knowledge into
modular   components;  and  to order   potential actions  flexibly  by
allowing any  number of constraints (i.e., control  strategies) to  be
imposed over a set  of alternative actions.  Furthermore,   because it
can   be  used  for reasoning  about  the   expenditure  of additional
resources to  obtain the evidential information  needed as a  basis for
choosing  among alternatives,    this  approach    can be     employed
recursively.}
}

@techreport{aic-tn-1984:323,
number=323,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Shakey The Robot",
author={Nils J. Nilsson},
month="April",
year=1984,
keywords={Shakey, Robot!Shakey},
abstract={
  From 1966 through 1972,  the Artificial  Intelligence Center
at SRI conducted research on  a mobile robot  system nicknamed ``Shakey.''
Endowed with a limited ability to  perceive  and model its environment,
Shakey could perform tasks that required  planning, route-finding, and
the rearranging of simple objects.  Although the Shakey project led to
numerous advances in AI techniques, many of which were reported in the
literature, much specific  information  that might be useful   in current
robotics research appears only  in  a series of relatively inaccessible
SRI technical reports.  Our purpose here, consequently, is to make this
material more  readily  available by extracting  and  reprinting those
sections of the reports that seem particularly interesting, relevant and
important.}
}

@techreport{aic-tn-1984:322,
number=322,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Artificial Intelligence, Employment, and Income",
author={Nils J. Nilsson},
month="February",
year=1984,
keywords={Artificial Intelligence},
abstract={
   Artificial  intelligence  (AI) and   other developments  in
computer science are giving birth to a dramatically different class of
machines--machines that   can   perform  tasks  requiring   reasoning,
judgment, and perception that previously could be done only by humans.
Will  these machines reduce  the  need for human  toil and  thus cause
unemployment?  there  are   two  opposing views in   response  to this
question.  Some claim that AI is not really very different from  other
technologies    that    have  supported   automation   and   increased
productivity--technologies such   as       mechanical     engineering,
electronics, control engineering, and operations research.  Like them,
AI may also lead ultimately to an expanding economy with a concomitant
expansion of  employment opportunities.  At worst,  according  to this
view, there will be some, perhaps even substantial shifts in the \underline{types}
of jobs, but certainly no  overall reduction  in  the total \underline{number}  of
jobs.  In my opinion, however, such an outcome is  based  on an overly
conservative   appraisal    of  the  real  potential     of artificial
intelligence.}
}

@techreport{aic-tn-1984:321,
number=321,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Probabilistic Logic",
author={Nils J. Nilsson},
month="February",
year=1984,
keywords={Logic!Probabilistic},
abstract={
   Because many  artificial intelligence applications  require
the ability to deal with uncertain knowledge, it is  important to seek
appropriate generalizations of logic for that case.  We present here a
semantical generalization  of logic  in which  the   truth-values   of
sentences are probability values (between 0  and 1).  Our
generalization applies to any logical system for which the consistency
of a finite set of sentences can be established.  (Although we cannot
always establish the consistency of a finite set of sentences of
first-order logic, our method is usable in those cases in which we
can.)  The method described in the present paper combines logic with
probability theory in such a way that probabilistic logical entailment
reduces to ordinary logical entailment when the probabilities of all sentences
are either 0 or 1.}
}

@techreport{aic-tn-1984:320,
number=320,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Formal Theory Of Knowledge and Action",
author={Robert C. Moore},
month="February",
year=1984,
keywords={Knowledge and Action},
abstract={
  Most work on planning and problem solving  within  the field
of  artificial  intelligence  assumes  that  the  agent  has  complete
knowledge of all  relevant aspects of  the problem domain  and problem
situation.   In  the  real  world,  however, planning and acting  must
frequently be performed without complete knowledge.   This imposes two
additional burdens on an intelligent agent trying  to act effectively.
First, when the agent  entertains a plan for  achieving some  goal, he
must consider not only whether the physical prerequisites of the  plan
have  been  satisfied,  but  also whether  he has  all the information
necessary to carry  out the plan.  Second, he  must be  able to reason
about what  he can do  to obtain  necessary information that he lacks.
In this paper, we present a  theory of  action in which these problems
are taken  into account, showing how  to formalize both the  knowledge
prerequisites of action and the effects of action on knowledge.}
}

@techreport{aic-tn-1984:319,
number=319,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Belief and Incompleteness",
author={Kurt Konolige},
month="January",
year=1984,
keywords={Belief and Incompleteness},
abstract={
   Two artificially intelligent (AI) computer agents begin to
play a game of chess, and the following conversation ensues:

\begin{itemize}
\item S1:  Do you know the rules of chess?
\item S2:  Yes.
\item S1:  Then you know whether White has a forced initial win
	     or not.
\item S2:  Upon reflection, I realize that I must.
\item S1:  Then there is no reason to play.
\item S2:  No.

\end{itemize}

Both agents  are state-of-the-art  constructions,   incorporating  the
latest AI research in chess  playing, natural-language  understanding,
planning,  etc.  But because   of  the  overwhelming  combinatorics of
chess, neither they  nor the fastest  foreseeable  computers would  be
able to search the  entire game  tree to find  out whether White has a
forced  win.  Why then  do they come  to such an  odd conclusion about
their own knowledge of the game?

The chess scenario  is  an anecdotal  example  of the   way inaccurate
cognitive models can lead to behavior that is less than intelligent in
artificial agents.  In this case,  the agents' model  of belief is not
correct.  They make  the  assumption that  an agent actually knows all
the consequences of  his beliefs.  S1 knows   that chess   is a finite
game, and thus reasons that, in principle, knowing the  rules of chess
is  all that is required  to  figure  out whether White  has  a forced
initial  win.  After learning that S2  does  indeed  know the rules of
chess, he comes to  the erroneous conclusion  that S2 also  knows this
particular consequence of the  rules.   and S2 himself,  reflecting on
his own knowledge in the same manner, arrives  at the same conclusion,
even though in actual fact  he could never carry out  the computations
necessary to demonstrate it.}
}

@techreport{aic-tn-1983:318,
number=318,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Ai Approach To Information Fusion",
author={Thomas D. Garvey and John D. Lowrance},
month="December",
year=1983,
keywords={Evidential Reasoning!Information Fusion},
abstract={
   This  paper discusses   the  use   of  selected  artificial
intelligence  (AI) techniques for  integrating multisource information
in  order to develop an  understanding  of an  ongoing situation.  The
approach  takes an active,  ``top-down'' view of the  task, projecting a
situation description forward in time, determining gaps in the current
model,   and tasking sensors   to acquire   data   to fill  the  gaps.
Information derived from tasked sensors and other  sources is combined
using new, non-Bayesian inference techniques.

This active approach seems critical to solve the problems posed by the
low emission signatures anticipated for  near-future threats.  Simulation 
experiments lead to the conclusion that the utility of ESM system
operation in future conflicts will depend on how  effectively  onboard
sensing resources are managed by the system.

The view of  AI that will  underly the discussion is that  of  a technology 
attempting to  extend automation  capabilities from the current
``replace  the   human's hands'' approach   to  that  of    replacing or
augmenting   the human's  cognitive  and   perceptual    capabilities.
Technology transfer  issues discussed   in the  presentation  are  the
primary  motivation   for highlighting this    view.    The paper will
conclude with a discussion of unresolved problems associated with  the
introduction of AI technology into real world military systems.}
}

@techreport{aic-tn-1984:317,
number=317,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Sri Artificial Intelligence Center--A Brief History",
author={Nils J. Nilsson},
month="January",
year=1984,
keywords={SRI AIC History},
abstract={
  Charles A. Rosen came  to SRI in 1957.  I  arrived  in 1961.
Between these dates, Charlie organized an  Applied Physics  Laboratory
and  became  interested in ``learning  machines''  and  ``self-organizing
systems.''  That interest launched a group that ultimately  grew into a
major world center of artificial  intelligence research--a center that
has endured    twenty-five years of  boom and   bust  in  fashion, has
``graduated''  over   a    hundred  AI research  professionals, and  has
generated ideas and programs resulting in  new products  and companies
as well as scientific articles, books, and  this particular collection
itself.

The SRI Artificial Intelligence Center  has  always been an  extremely
cohesive  group, even though it  is  associated with many  contrasting
themes.   Perhaps  these  very  contrasts  are   responsible  for  its
vitality.  It  is  a group of  professional  researchers, but visiting
Ph.D.  candidates  (mainly from  Stanford  University)   have  figured
prominently in  its intellectual achievements.  It  is not  part  of a
university, yet its approach to  AI has often  been more academic  and
basic   than those  used   in   some   of  the prominent    university
laboratories.  For many years a  vocal  group  among its professionals
has strongly   emphasized the role  of  logic and  the  centrality  of
reasoning and declarative representation in AI, but it is also home to
many researchers who pursue other aspects of the discipline.  Far more
people have left it (to pursue careers in industry) than  are now part
of it, yet it is still about as large as it has ever been  and retains
a  more  or   less consistent character.  It is   an American research
group, supported  largely by  the  Defense Department, but, from   the
beginning, it has been a melting pot of nationalities.}
}

@techreport{aic-tn-1984:315,
number=315,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Choosing A Basis For Perceptual Space",
author={Stephen T. Barnard},
month="January",
year=1984,
keywords={Vision!Perceptual space},
abstract={
  If it is possible to interpret an  image as  a projection of
rectangular forms, there is a strong tendency for people to do so.  In
effect, a  mathematical basis for a  vector space  appropriate  to the
world, rather  than  to  the image,   is  selected.  A   computational
solution  to  this problem is  presented.  It works by  backprojecting
image features into  three-dimensional    space, thereby    generating
(potentially) all  possible  interpretations, and  by selecting  those
which  are  maximally orthogonal.    In general,  two  solutions  that
correspond to perceptual reversals are found.  The problem of choosing
one of these is related to the knowledge of verticality.  A measure of
consistency of image features with a hypothetical solution is defined.
In    conclusion,   the  model     supports  an  information-theoretic
interpretation of the Gestalt view of perception.}
}

@techreport{aic-tn-1983:314,
number=314,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Procedural Expert Systems",
author={Michael Georgeff and Umberto Bonollo (Monash U., Australia)},
month="December",
year=1983,
keywords={PRS, Reasoning!Procedural},
abstract={
    A  scheme for explicitly    representing and  using expert
knowledge of  a procedural kind is  described.  The scheme allows  the
\underline{explicit} representation of both declarative and procedural knowledge
within a unified  framework, yet retains  all the desirable properties
of expert  systems  such  as  modularity, explanatory  capability, and
extendability.  It  thus bridges the  gap between  the  procedural and
declarative languages, and allows formal algorithmic knowledge  to  be
uniformly integrated with heuristic declarative knowledge.   A version
of the scheme has been fully implemented and applied to the  domain of
automobile engine fault diagnosis.}
}

@techreport{aic-tn-1983:313,
number=313,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Communication and Interaction In Multi-Agent Planning",
author={Michael Georgeff},
month="December",
year=1983,
keywords={Multiple Agents!Planning},
abstract={
   A method for  synthesizing multi-agent  plans  from simpler
single-agent plans is described.  The idea is  to insert communication
acts  into   the single-agent  plans  so  that  agents can synchronize
activities  and avoid  harmful  interactions.   Unlike   most previous
planning systems, actions are represented   by \underline{sequences} of  states,
rather  than as simple  state  change  operators.  This   allows   the
expression of more complex kinds of interaction than  would  otherwise
be possible.  An efficient  method of interaction  and safety analysis
is then developed and used to identify critical regions  in the plans.
An essential feature of the method is  that the analysis is  performed
without   generating  all possible  interleavings  of the  plans, thus
avoiding a combinatorial explosion.  Finally, communication primitives
are inserted into the plans and a supervisor process created to handle
synchronization.}
}

@techreport{aic-tn-1983:312,
number=312,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Planning English Referring Expressions",
author={Douglas Appelt},
month="October",
year=1983,
keywords={Natural Language!Referring, Natural Language!Planning},
abstract={
   This paper describes a theory  of language generation based
on planning.   To illustrate   the theory, the   problem  of  planning
referring   expressions  is  examined in detail.   A theory  based  on
planning makes  it possible for one  to account for  noun phrases that
refer, that inform the hearer of additional information, and that  are
coordinated   with the  speaker's   physical actions  to  clarify  his
communicative  intent.  The  theory  is embodied in  a computer system
called KAMP, which plans both physical and linguistic actions, given
a high-level description of the speaker's goals.}
}

@techreport{aic-tn-1983:310,
number=310,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Overview Of The Image Understanding Testbed",
author={Andrew J. Hanson},
month="October",
year=1983,
keywords={Vision!IU Testbed|see{Image Understanding Testbed}, Image Understanding Testbed},
abstract={
  The Image Understanding Testbed is  a system of hardware and
software that is designed to facilitate  the integration, testing, and
evaluation of implemented  research concepts in machine   vision.  The
system  was developed  by the  Artificial Intelligence  Center  of SRI
International under  the joint  sponsorship  of the  Defense  Advanced
Research Projects Agency (DARPA) and the Defense Mapping Agency (DMA).
The  primary purpose  of  the Image  Understanding  (IU) Testbed is to
provide a means for transferring technology  from  the DARPA-sponsored
IU research  program to DMA  and other organizations   in  the defense
community.

The approach taken to achieve this purpose has two components:

\begin{itemize}
\item The establishment of a uniform environment that will be as
      compatible as possible with the environments of research centers at
      universities participating in the IU program.  Thus, organizations
      obtaining copies of the testbed can receive new results of ongoing
      research as they become available.

\item The acquisition, integration, testing, and evaluation of
      selected scene analysis techniques that represent mature examples of
      generic areas of research activity.  These contributions from IU
      program participants will allow organizations with testbed copies to
      immediately begin investigating potential applications of IU
      technology to problems in automated cartography and other areas of
      scene analysis.
\end{itemize}

An important  component of the   DARPA IU research   program  is   the
development of image-understanding techniques that could be applied to
automated  cartography and military  image interpretation  tasks; this
work forms the principal focus  of the  testbed project.   A number of
computer modules developed by participants in the IU program have been
transported to the uniform testbed environment as a first step in  the
technology transfer process.  These include systems written in UNIX C,
MAINSAIL, and FRANZ  LISP.    Capabilities  of the  computer  programs
include  segmentation, linear  feature  delineation, shape  detection,
stereo   reconstruction,  and  rule-based recognition of   classes  of
three-dimensional objects.}
}

@techreport{aic-tn-1983:309,
number=309,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Abstract Prolog Instruction Set",
author={David H.D. Warren},
month="October",
year=1983,
keywords={Prolog},
abstract={
   This  report describes  an  abstract Prolog instruction set
suitable  for software,  firmware,   or  hardware implementation.  The
instruction set is abstract in  that  certain details of  its encoding
and implementation are  left  open, so  that it may  be realized in  a
number of different forms.  The forms that are contemplated are:

\begin{itemize}
\item Translation into a compact bytecode, with emulators written in
        C (for maximum portability), Progol (a macrolanguage generating
        machine code, for efficient software implementations as an
	alternative to direct compilation on machines such as the
	VAX), and VAX-730 microcode.

\item Compilation into the standard instructions of machines such as
        the VAX or DECsystem-10/20.

\item Hardware (or firmware) emulation of the instruction set on a
        specially designed Prolog processor.

\end{itemize}

The abstract machine described herein (``new Prolog Engine'') is a major
revision of the ``old Prolog Engine'' described in a  previous document.
The new model overcomes certain  difficulties in the  old model, which
are discussed in a later section.  The new model can  be considered to
be a modification of the old model, where the stack contains compiler-defined 
goals called environments instead  of user-defined goals.  The
environments correspond to some number of goals forming  the tail of a
clause.  The  old model was  developed having  primarily  in  mind   a
VAX-730 microcode  implementation.   The new model   has, in addition,
been influenced by hardware  implementation considerations, but should
remain  equally amenable  to software  or  firmware implementation  on
machines such as the VAX.}
}

@techreport{aic-tn-1983:308,
number=308,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Overlapped Prolog Processor",
author={Evan Tick},
month="October",
year=1983,
keywords={Prolog},
abstract={
   This  report  describes  the design of  a   Prolog  machine
organization implementing D. Warren's architecture.  The objective was
to determine the maximum performance attainable by a sequential Prolog
machine for ``reasonable'' cost. The  report compares the organization
to both general-purpose, microcoded  machines and reduced-instruction-set 
machines.  Hand timings indicate that a  peak  performance rate of
450 K  LIPS (logical  inferences  per  second) is  well within current
technology limitations and 1 M LIPS is potentially feasible.}
}

@techreport{aic-tn-1983:307,
number=307,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Evidential Reasoning:  An Implementation For Multisensor Integration",
author={John D. Lowrance and Thomas D. Garvey},
month="December",
year=1983,
keywords={Evidential Reasoning!Multisensor integration},
abstract={
    Reasoning   from  uncertain,   incomplete,   and sometimes
inaccurate information is necessary whenever any system is to interact
in  an intelligent  way with its  environment.  This  follows directly
from   the  fact that  understanding  the  world  is  possible only by
perceiving it  through a   set of  knowledge   sources   that  provide
partially processed   sensory  information.  Because  of  the  limited
capabilities   of any   sensor,    the    information is    inherently
``evidential.'' That is, perceptual information is not readily captured
in terms of simple truths  and falsities or in terms  of probabilistic
estimates,  when   the appropriate  statistical    data are   lacking.
Therefore,  neither  logical  nor  standard   probabilistic  reasoning
techniques are uniformly applicable in this context.}
}

@techreport{aic-tn-1983:306,
number=306,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Relationship Between Image Irradiance and Surface Orientation",
author={Grahame B. Smith},
month="September",
year=1983,
keywords={Vision!Image irradiance},
abstract={
  A formulation of shape from  shading  is  presented in which
surface orientation is  related to image  irradiance without requiring
detailed knowledge of  either the scene  illumination or the albedo of
the surface material.  The case for uniformly diffuse  reflection  and
perspective projection is discussed in  detail.  Experiments aimed  at
using the formulation to recover surface orientation are presented and
the  difficulty  of nonlocal   computation  discussed.  We present  an
algorithm   for  reconstructing  the   3-D  surface shape once surface
orientations are known.}
}

@techreport{aic-tn-1983:305,
number=305,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="On Some Formal Properties Of Metarules",
author={Hans Uszkoreit},
month="September",
year=1983,
keywords={Natural Language!Grammar metarules},
abstract={
  Grammars contain rules for  generating sentences.  Metarules
are statements about these  rules.   They are metagrammatical  devices
that can be used to generate rules of the grammar or to encode certain
relations among them such as redundancies in their form.}
}

@techreport{aic-tn-1983:304,
number=304,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Metarules As Meta-Node-Admissibility Conditions",
author={Susan U. Stucky},
month="September",
year=1983,
keywords={Natural Language!Grammar metarules},
abstract={
  Metarule phrase-structure grammars (MPS grammars)  have been
shown to be  an extremely  powerful formalism in  need  of constraints
from both  the  computational and the  linguistic points of view.  One
problem with the  standard generative interpretation   of metarules is
the generation of infinite  rule sets.  Furthermore,  even if grammars
having this property are disallowed,  the possibility of  a combinatorial
explosion  of rules   still remains.    In the present  paper we
explore a view  of  metarules  as  meta-node-admissibility  conditions
(MNACs) which  allows  a non-generative  interpretation  of metarules.
Under such an interpretation, an MPS grammar will not have  either  of
the two problems mentioned  above.  We find that,  under one suggested
implementation, the  above   mentioned problem   appears under  another
guise, so  that   additional  constraints are   needed  to ensure   an
effective  procedure  for  checking  admissibility  conditions in  the
computational setting.  The important observation is that  one can, by
parsing with  MNACs on  the fly, recognize  languages for  which   the
generative interpretation is not available.}
}

@techreport{aic-tn-1983:301,
number=301,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Relax Image Relaxation System: Description and Evaluation",
author={Kenneth I. Laws and Grahame B. Smith},
month="August",
year=1983,
keywords={Vision!Relaxation, Image Understanding Testbed},
abstract={
  The primary purpose of the Image Understanding (IU)  Testbed
is to  provide  a  means for transferring  technology  from the DARPA-sponsored
IU research program  to  DMA and other organizations in  the
defense community.

The approach taken to achieve this purpose has two components:

\begin{enumerate}
\item The establishment  of a uniform  environment that
will be as compatible  as possible with  the environments  of research
centers  at   universities participating  in the  IU   program.  Thus,
organizations obtaining copies of  the Testbed  can receive  a flow of
new results derived from ongoing research.

\item The acquisition,   integration, testing, and  evaluation of
selected scene analysis techniques that represent mature  examples  of
generic   areas  of  research  activity.      These contributions from
participants in the IU program will  allow organizations with  Testbed
copies to immediately begin investigating potential applications of IU
technology to  problems in automated cartography  and other   areas of
scene analysis.
\end{enumerate}

The IU Testbed project was carried out under  DARPA Contract
No.   MDA903-79-C-0588.  The views  and conclusions contained in  this
document  are those of  the author  and should  not  be interpreted as
necessarily representing the   official policies, either expressed  or
implied, of  the Defense Advanced   Research   Projects Agency  or the
United States government.

This report describes the RELAX relaxation package contributed by the
University of Maryland and presents an evaluation of its
characteristics and features.}
}

@techreport{aic-tn-1984:299,
number=299,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Darpa/Dma Image Understanding Testbed System Manager's Manual",
author={Andrew J. Hanson},
month="January",
year=1984,
keywords={Image Understanding Testbed},
abstract={
   The primary purpose of the Image Understanding (IU) Testbed
is to provide a means for transferring technology from the
DARPA-sponsored IU research program to DMA and other organizations in
the defense community.

The approach taken to achieve this purpose has two components:

\begin{enumerate}
\item The establishment of a uniform environment that will be as compatible
    as possible with the environments of research centers at universities
    participating in the IU program.  Thus, organizations obtaining copies
    of the Testbed can receive a flow of new results derived from ongoing
    research.

\item  The  acquisition,  integration,  testing,  and evaluation of selected
    scene analysis techniques that represent  mature  examples of generic
    areas of research activity.  These contributions from participants in
    the  IU  program  will  allow  organizations  with  Testbed copies to
    immediately   begin   investigating   potential   applications  of IU
    technology to problems in  automated  cartography  and other areas of
    scene analysis.
\end{enumerate}

     The IU Testbed project was carried out under DARPA  Contract
No.  MDA903-79-C-0588.  The views  and  conclusions contained in  this
document  are those of the author  and  should  not  be interpreted as
necessarily representing the   official policies, either expressed  or
implied, of the   Defense Advanced  Research  Projects  Agency or  the
 United States government.

This manual contains a selection of  information and procedures needed
by system managers responsible for the  maintenance of the  IU Testbed
software system.}
}

@techreport{aic-tn-1984:298,
number=298,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Darpa/Dma Image Understanding Testbed Programmer's Manual",
author={Kenneth I. Laws},
month="January",
year=1984,
keywords={Image Understanding Testbed},
abstract={
   The primary purpose of the Image Understanding (IU) Testbed
is to provide a means for transferring technology from the
DARPA-sponsored IU research program to DMA and other organizations in
the defense community.

The approach taken to achieve this purpose has two components:

\begin{enumerate}

\item The establishment of a uniform environment that will be as compatible
    as possible with the environments of research centers at universities
    participating in the IU program.  Thus, organizations obtaining copies
    of the Testbed can receive a flow of new results derived from ongoing
    research.

\item The acquisition,  integration,  testing,  and evaluation  of selected
    scene analysis  techniques that represent mature  examples of generic
    areas of research activity.  These contributions from participants in
    the IU  program  will  allow  organizations  with  Testbed  copies to
    immediately   begin   investigating   potential  applications  of  IU
    technology to  problems  in  automated cartography and other areas of
    scene analysis.

\end{enumerate}

     The  IU Testbed project was  carried  out  under DARPA  Contract   No.
MDA903-79-C-0588.    The   views  and conclusions    contained in this
document are those of  the author and  should  not be  interpreted  as
necessarily  representing the  official policies,  either expressed or
implied, of the  Defense  Advanced  Research Projects  Agency or   the
United States government.

This report  provides UNIX-style  programmer's reference documentation
for IU Testbed  software modules that are  based  on the   UNIX system
environment.}
}

@techreport{aic-tn-1983:297,
number=297,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Telegram:  A Grammar Formalism For Language Planning",
author={Douglas E. Appelt},
month="June",
year=1983,
keywords={TELEGRAM, Natural Language!Planning, Planning!Natural Language|see{Natural Language}},
abstract={
   Planning provides    the basis for   a  theory  of language
generation that considers the communicative goals of the speaker  when
producing utterances.  One central problem in designing a system based
on such a theory is specifying the requisite linguistic knowledge in a
form  that interfaces well  with a planning system  and allows for the
encoding  of   discourse  information.    The  TELEGRAM  (TELEological
GRAMmar) system described  in  this   paper   solves  this problem  by
annotating a unification grammar with assertions about how grammatical
choices are used to achieve various goals, and by enabling the planner
to augment the functional description  of an utterance  as it is being
unified.   The control   structures  of  the planner   and the grammar
unifier are then merged in a manner that makes it possible for general
planning to be  guided  by  unification of  a  particular   functional
description.}
}

@techreport{aic-tn-1983:296,
number=296,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A New Characterization Of Attachment Preferences",
author={Fernando C.N. Pereira},
month="March",
year=1983,
keywords={Natural Language!Attachment},
abstract={
  Several authors have tried  to model attachment  preferences
for structurally  ambiguous sentences, which cannot  be  disambiguated
from  semantic information.  These  models  lack rigor and  have  been
widely criticized.  By starting from   a precise  choice  of   parsing
model,  it is  possible to give  a  simple and rigorous description of
Minimal Attachment  and Right Association   that avoids some  of   the
problems of other models.}
}

@techreport{aic-tn-1983:295,
number=295,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Parsing As Deduction",
author={Fernando C.N. Pereira and David H.D. Warren},
month="June",
year=1983,
keywords={Deduction!Parsing as, Natural Language!Parsing},
abstract={
  By exploring the relationship between parsing and deduction,
a new and more  general view  of  chart  parsing   is obtained,  which
encompasses parsing for  grammar formalisms based on  unification, and
is  the basis of  the Earley Deduction proof  procedure  for  definite
clauses.  The efficiency of this approach for an interesting class  of
grammars is discussed.}
}

@techreport{aic-tn-1983:293,
number=293,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Transportability and Generality In A Natural-Language Interface System",
author={Paul Martin and Douglas Appelt and Fernando Pereira},
month="November",
year=1983,
keywords={TEAM, Natural Language!Interface},
abstract={
  This paper describes the design of  a transportable  natural
language (NL)    interface  to databases   and  the   constraints that
transportability places  on each components  of such a  system.   By a
\underline{transportable} NL system,  we mean an  NL processing system that  is
constructed so that a domain expert (rather than an AI or  linguistics
expert)  can  move the  system  to   a new  application domain.  After
discussing the general  problems  presented by transportability,  this
paper describes \underline{TEAM} (an acronym for \underline{T}ransportable \underline{E}nglish database
\underline{A}ccess \underline{M}edium),  a demonstratable  prototype of  such  a system.   The
discussion  of TEAM shows how domain-independent  and domain-dependent
information  can be  separated  in the  different components  of an NL
interface system, and presents one method of obtaining domain-specific
information from a domain expert.}
}

@techreport{aic-tn-1983:292,
number=292,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Providing A Unified Account Of Definite Noun Phrases In Discourse",
author={Barbara J. Grosz, Et Al},
month="June",
year=1983,
keywords={Natural Language!Discourse},
abstract={
  Linguistic   theories typically assign    various linguistic
phenomena  to  one of  the  categories,  \underline{syntactic},  \underline{semantic},  or
\underline{pragmatic},  as if the  phenomena  in each  category were  relatively
independent of those in  the  others.  However, various  phenomena  in
discourse do  not seem to yield  comfortably  to  any account that  is
strictly a syntactic or semantic or pragmatic one.  This paper focuses
on particular  phenomena of this  sort--the use  of  various referring
expressions  such as definite noun phrases  and pronouns--and examines
their interaction   with mechanisms   used  to    maintain   discourse
coherence.

Even a casual survey  of the literature on  definite descriptions  and
referring expressions  reveals  not  only  defects  in  the individual
accounts provided by  theorists (from  several different disciplines),
but also deep confusions about the roles that syntactic, semantic, and
pragmatic   factors play  in   accounting  for  these phenomena.   The
research  we have undertaken  is an attempt to sort  out some of these
confusions and to create  the basis for  a theoretical framework  that
can account for a variety of  discourse  phenomena in  which all three
factors of  language  use interact.  The   major premise on  which our
research  depends  is that  the   concepts  necessary for an  adequate
understanding of the phenomena in question  are not exclusively either
syntactic or semantic or pragmatic.

The  next  section of this  paper   defines  two levels of   discourse
coherence  and describes their  roles in  accounting  for  the  use of
singular definite noun   phrases.  To illustrate   the integration  of
factors in explaining the uses of referring expressions, their use  on
one of these levels, i.e., the  local one, is discussed in  Sections 3
and 4.  This account requires introducing the notion of the centers of
a sentence in a discourse, a notion that cannot be defined in terms of
factors that are exclusively syntactic or  semantic or pragmatic.   In
Section 5, the  interactions of the two levels  with these factors and
their effects on the  uses of referring expressions in  discourse  are
discussed.}
}

@techreport{aic-tn-1983:291,
number=291,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Direct Parsing Of Id/Lp Grammars",
author={Stuart M. Shieber},
month="August",
year=1983,
keywords={Natural Language!ID/LP grammars},
abstract={
  The Immediate Dominance/Linear  Precedence (ID/LP) formalism
is a recent extension of  Generalized  Phrase Structure Grammar (GPSG)
designed to  perform   some  of  the tasks  previously    assigned  to
metarules--for example, modeling  the  word-order  characteristics  of
so-called free-word-order languages.  It allows a simple specification
of  classes of rules  that differ only in   constituent order.   ID/LP
grammars (as well as metarule grammars) have been  proposed for use in
parsing by expanding them into an equivalent context-free grammar.  We
develop a parsing algorithm,  based  on the algorithm  of Earley,  for
parsing ID/LP grammars  directly, circumventing  the initial expansion
phase.  A proof of correctness of the algorithm is supplied.   We also
discuss some aspects of the time complexity of the  algorithm and some
formal   properties  associated   with   ID/LP  grammars and     their
relationship to context-free grammars.}
}

@techreport{aic-tn-1983:290,
number=290,
price="$20.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Applied Logic--Its Use and Implementation As A Programming Tool",
author={David H.D. Warren},
month="June",
year=1983,
keywords={Prolog, Logic Programming|see{Prolog}},
abstract={
  The first part of the thesis  explains from first principles
the concept of ``logic  programming'' and its  practical  application in
the programming  language Prolog.  Prolog   is a simple but   powerful
language which encourages  rapid,  error-free  programming  and clear,
readable, concise  programs.  The  basic computational mechanism  is a
pattern matching process (``unification'') operating  on general  record
structures (``terms'' of logic).

The ideas are illustrated by describing  in detail  one sizable Prolog
program  which  implements a  simple   compiler.   The advantages  and
practicability of using Prolog  for ``real'' compiler implementation are
discussed.

The  second part of the  thesis describes techniques  for implementing
Prolog  efficiently.  In particular, it  is  shown how  to compile the
patterns   involved in  the matching process   into  instructions of a
low-level language.  This  ideas  has actually been implemented  in  a
compiler (written in  Prolog)  from Prolog  to  DECsystem-10  assembly
language.   However, the principles  involved    are   explained  more
abstractly in terms  of a  ``Prolog Machine.''  The  code generated is
comparable in speed with  that   produced  by   existing DEC10    Lisp
compilers.  Comparison is possible since pure Lisp can be viewed as  a
(rather restricted) subset of Prolog.

It is argued  that structured data objects,  such  as lists and trees,
can be  manipulated  by pattern matching using a ``structure sharing''
representation as  efficiently   as   by conventional selector     and
constructor functions operating on linked records in ``heap'' storage.
Moreover,   the  pattern  matching  formulation  actually  helps   the
implementor to produce a better implementation.}
}

@techreport{aic-tn-1982:289,
number=289,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Phoenix Image Segmentation System: Description and Evaluation",
author={Kenneth I. Laws},
month="December",
year=1982,
keywords={PHOENIX, Image Understanding Testbed, Vision!Segmentation},
abstract={
  PHOENIX is  a computer  program  for segmenting  images into
homogeneous closed regions.  It uses histogram analysis, thresholding,
and connected-components analysis  to produce a  partial segmentation,
then resegments  each  region until   various  stopping  criteria  are
satisfied.   Its major  contributions over  other recursive segmenters
are a sophisticated  control interface, optional use  of more than one
histogram-dependent intensity threshold during  tentative segmentation
of each region, and spatial analysis of resulting subregions as a form
of ``look-ahead'' for  choosing between promising  spectral features  at
each step.

PHOENIX was contributed  to the DARPA Image Understanding  Testbed  at
SRI by   Carnegie-Mellon    University.    This    report   summarizes
applications for which PHOENIX  is  suited, the history and nature  of
the  algorithm, details of the Testbed  implementation, the manner  in
which PHOENIX is invoked and controlled, the type of results that  can
be expected,    and  suggestions for  further development.    Baseline
parameter sets are  given   for producing reasonable segmentations  of
typical imagery.}
}

@techreport{aic-tn-1982:288,
number=288,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Ghough Generalized Hough Transform Package: Description, and Evaluation",
author={Kenneth I. Laws},
month="December",
year=1982,
keywords={GHOUGH, Image Understanding Testbed, Vision!Detection},
abstract={
   GHOUGH is  a computer program for  detecting instances of a
given shape within an image.  It may be used for  cueing, counting, or
mensuration.  GHOUGH can find  instances that are displaced, rescaled,
rotated, or  incomplete  relative to  the  shape template.    They are
detected by  computing a ``generalized Hough transform'' of the  image
edge elements.  Each edge element votes for all those instances of the
shape that  could  contain it; the  votes   are tallied and  the  best
supported instances are reported as likely matches.

GHOUGH was contributed to the DARPA Image Understanding Testbed at SRI
by the University of  Rochester.  This report summarizes  applications
for which GHOUGH is suited, the history and  nature of  the algorithm,
details of the Testbed  implementation, the manner  in which GHOUGH is
invoked and controlled, the types of results that can be expected, and
suggestions for further development.  The  scientific contributions of
this technical note  are  the analysis of  GHOUGH's parameter settings
and performance characteristics.}
}

@techreport{aic-tn-1983:287,
number=287,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Shape From Shading:  An Assessment",
author={Grahame B. Smith},
month="May",
year=1983,
keywords={Vision!Shape detection, Vision!Image irradiance},
abstract={
   We review previous efforts  to  recover surface shape  from
image irradiance   in  order to  assess   what  can and    cannot   be
accomplished.   We   consider   the  informational    requirements and
restrictions of these  approaches.  In dealing with the  question   of
what surface parameters can be recovered  locally from image  shading,
we show that, at most,  shading determines relative surface curvature,
i.e., the ratio  of  surface  curvature measured   in orthogonal image
directions.   The relationship between relative surface  curvature and
the  second  derivatives of image  irradiance is independent of  other
scene parameters, but  insufficient to determine  surface shape.  This
result  places in perspective the  difficulty  encountered in previous
attempts to recover surface orientation from image shading.}
}

@techreport{aic-tn-1983:285,
number=285,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Framework For Processing Partially Free Word Order",
author={Hans Uszkoreit},
month="May",
year=1983,
keywords={Natural Language!Free word order},
abstract={
   The partially free  word order in  German  belongs to the  class  of
phenomena in natural language that  require a close interaction between  syntax
and pragmatics.  Several competing principles, which are based on syntactic and
on discourse information, determine   the linear order   of noun  phrases.    A
solution to problems of this sort is  a prerequisite  for high-quality language
generation.  The linguistic  framework of Generalized Phrase  Structure Grammar
offers tools for dealing with word order variation.   Some slight modifications
to  the framework allow for an  analysis of the German data   that incorporates
just the right degree of interaction between syntactic and pragmatic components
and     that  can    account   for    conflicting      ordering   statements.}
}

@techreport{aic-tn-1983:284,
number=284,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Semantical Considerations On Nonmonotonic Logic",
author={Robert C. Moore},
month="June",
year=1983,
keywords={Logic!Nonmonotonic, Commonsense Reasoning},
abstract={
   Commonsense reasoning is ``nonmonotonic'' in the sense that
we often draw, on the basis of partial information, conclusions that
we later retract when we are given more complete information.  Some of
the most interesting products of recent attempts to formalize
nonmonotonic reasoning are the nonmonotonic logics of McDermott and
Doyle [McDermott and Doyle, 1980; McDermott, 1982].  These logics,
however, all have peculiarities that suggest they do not quite succeed
in capturing the intuitions that prompted their development.  In this
paper we reconstruct nonmonotonic logic as a model of an ideally
rational agent's reasoning about his own beliefs.  For the resulting
system, called \underline{autoepistemic logic}, we define an intuitively based
semantics for which we can show autoepistemic logic to be both sound
and complete.  We then compare autoepistemic logic with the approach
of McDermott and Doyle, showing how it avoids the peculiarities of
their nonmonotonic logic.}
}

@techreport{aic-tn-1983:283,
number=283,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Formal Constraints On Metarules",
author={Stuart M. Shieber, Susan U. Stucky, Hans Uszkoreit, and Jane J. Robinson},
month="April",
year=1983,
keywords={Natural Language!Grammar metarules},
abstract={
  Metagrammatical formalisms  that combine context-free phrase
structure rules and metarules  (MPS grammars) allow concise  statement
of generalizations     about   the   syntax  of    natural  languages.
Unconstrained  MPS  grammars,  unfortunately,  are not computationally
``safe.''  We evaluate  several proposals for constraining them,  basing
our assessment on computational tractability and explanatory adequacy.
We  show that none  of them satisfies both criteria,  and suggest  new
directions for research on alternative metagrammatical formalisms.}
}

@techreport{aic-tn-1983:282,
number=282,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Can Drawing Be Liberated From The Von Neumann Style?",
author={Fernando C.N. Pereira},
month="June",
year=1983,
keywords={Prolog, Graphics database tools},
abstract={
  Current graphics database tools  give the   user a view   of
drawing  that is too  constrained by the low-level machine  operations
used to implement the tools.  A new approach  to graphics databases is
proposed, based on the description of objects and their  relationships
in the restricted form  of logic embodied in  the programming language
Prolog.}
}

@techreport{aic-tn-1983:281,
number=281,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Sentence Disambiguation By A Shift-Reduce Parsing Technique",
author={Stuart M. Shieber},
month="March",
year=1983,
keywords={Natural Language!Shift-Reduce parsing},
abstract={
  Native  speakers  of English   show definite  and consistent
preferences for certain readings of syntactically ambiguous sentences.
A user of a natural-language-processing  system would naturally expect
it to reflect the same preferences.  Thus,  such systems must model in
some  way the  \underline{linguistic performance}  as well  as the \underline{linguistic
competence} of the  native   speaker.  We have   developed   a parsing
algorithm--a   variant  of  the   LALR(1) shift-reduce algorithm--that
models the preference  behavior of native  speakers  for  a range   of
syntactic  preference   phenomena reported  in  the   psycholinguistic
literature,  including the  recent  data on  lexical preferences.  The
algorithm  yields   the  preferred   parse  deterministically, without
building  multiple parse trees  and choosing   among them.  As  a side
effect,  it displays   appropriate behavior  in   processing  the much
discussed  garden-path sentences.   The parsing  algorithm   has  been
implemented and has confirmed the feasibility  of our approach to  the
modeling of these phenomena.}
}

@techreport{aic-tn-1984:280,
number=280,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Fractal-Based Description Of Natural Scenes",
author={Alex P. Pentland},
month="February",
year=1984,
keywords={Fractals!Natural scenes},
abstract={
  This  paper  addresses the  problems   of: (1)  representing
natural shapes such as mountains, trees, and clouds, and (2) computing
their description from image data.   To solve these  problems, we must
be able to relate  natural surfaces to  their images;  this requires a
good  model of natural surface shapes.    Fractal functions are a good
choice  for  modeling 3-D natural surfaces because  (1)  many physical
processes produce a  fractal surface  shape, (2) fractals  are  widely
used as a graphics tool for generating natural-looking shapes, and (3)
a survey  of natural  imagery has shown  that the  3-D fractal surface
model,  transformed  by the   image formation process,  furnishes   an
accurate description of both textured and shaded image regions.

The 3-D fractal model provides a characterization of 3-D surfaces and
their images for which the appropriateness of the model is verifiable.
Furthermore, this characterization is stable over transformations of
scale and linear transforms of intensity.

The 3-D fractal model has been successfully applied to the problems of
(1)  texture segmentation and  classification,  (2) estimation of  3-D
shape information,  and   (3)   distinguishing    between perceptually
``smooth'' and perceptually ``textured'' surfaces in the scene.}
}

@techreport{aic-tn-1983:278,
number=278,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Representation Of Time For Planning",
author={Peter Cheeseman},
month="February",
year=1983,
keywords={Planning!Time},
abstract={
   A  new   time  representation  is  described  that allows a
continuously changing  world to be  represented, so that queries about
the truth of a  proposition at an  instant or  over an interval can be
answered.  The deduction mechanism used  to answer the common  queries
necessary  in planning is  the same as  that employed for deducing all
other information, thereby  avoiding the  need for a specialized  time
expert.     The representation allows   any   time   information to be
represented without forcing an over specification.  The implementation
of  this representation requires  mechanisms to detect  the effects of
world changes on previous deductions (truth maintenance).}
}

@techreport{aic-tn-1984:277,
number=277,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Darpa/Dma Image Understanding Testbed User's Manual",
author={Andrew J. Hanson},
month="January",
year=1984,
keywords={Image Understanding Testbed},
abstract={
  The primary purpose of the Image Understanding  (IU) Testbed
is  to  provide  a   means  for   transferring   technology  from  the
DARPA-sponsored IU research program to DMA and  other organizations in
the defense community.

The approach taken to achieve this purpose has two components:

\begin{enumerate}
\item The establishment of a uniform environment that will be as compatible
    as possible with the environments of research centers at universities
    participating in the IU program.  Thus, organizations obtaining copies
    of the Testbed can receive a flow of new results derived from ongoing
    research.

\item The acquisition, integration, testing, and evaluation of selected
    scene analysis techniques that represent mature examples of generic
    areas of research activity.  These contributions from participants in
    the IU program will allow organizations with Testbed copies to
    immediately begin investigating potential applications of IU
    technology to problems in automated cartography and other areas of
    scene analysis.

\end{enumerate}

The IU Testbed project  was   carried out  under DARPA Contract  
No. MDA903-79-C-0588. The views and conclusions  contained in this
document are those  of  the  author and  should not be interpreted  as
necessarily representing the official   policies, either expressed  or
implied,  of the Defense  Advanced  Research Projects   Agency or  the
United States government.

This document   presents a   user's  view of  the IU   Testbed and the
facilities it provides.  Many talented people, both at SRI and at each
of the contributing institutions, must be  acknowledged for their part
in bringing the Testbed into existence.  Special recognition is due to
David Kashtan and Kenneth Laws  for  their  essential contributions to
the environment described here.}
}

@techreport{aic-tn-1983:276,
number=276,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A General Approach To Machine Perception Of  Linear, Structure In Imaged Data",
author={Martin A. Fischler and Helen C. Wolf},
month="February",
year=1983,
keywords={Vision!Linear structure},
abstract={
   In  this paper we   address  a  basic   problem in  machine
perception:  the tracing  of  ``line-like'' structures  appearing  in an
image.  It is shown that this problem can profitably  be viewed as the
process of  finding skeletons in a  gray   scale image after observing
(1) that  line detection does   not  necessarily   depend on  gradient
information, but   rather  is  approachable  from  the   standpoint of
measuring  total   intensity  variation, and   (2) that  smoothing the
original image  produces   an approximate    distance  transform.   An
effective technique for  extracting the  delineating skeletons from an
image   is  presented, and examples  of this   approach using  aerial,
industrial, and radiographic imagery are shown.}
}

@techreport{aic-tn-1983:275,
number=275,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Logic For Natural Language Analysis",
author={Fernando Pereira},
month="January",
year=1983,
keywords={Natural Language!Logic, Logic!For Natural Language, Prolog},
abstract={
  This  work investigates   the   use of  formal  logic as   a
practical tool for describing the syntax and semantics  of a subset of
English, and building a computer program to  answer data  base queries
expressed in that subset.

To achieve  an intimate  connection  between logical  descriptions and
computer programs,  all the descriptions  given  are   in the definite
clause subset of the  predicate  calculus, which  is the basis  of the
programming language Prolog.  The logical descriptions run directly as
efficient Prolog programs.

Three aspects of  the  use of logic in natural  language  analysis are
covered:  formal   representation of  syntactic  rules by  means  of a
grammar   formalism based  on  logic,  extraposition  grammars; formal
semantics for the chosen  English subset, appropriate for  data   base
queries; informal semantic  and  pragmatic rules to translate analyzed
sentences into their formal semantics.

On these three aspects, the work improves and extends earlier  work by
Colmerauer   and others, where    the use of   computational logic  in
language analysis was first introduced.}
}

@techreport{aic-tn-1982:273,
number=273,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="From Image Irradiance To Surface Orientation",
author={Grahame B. Smith},
month="December",
year=1982,
keywords={Vision!Image irradiance},
abstract={
  The image  irradiance  equation constrains the  relationship
between surface orientation in  a  scene and  the   irradiance  of its
image.   This  equation requires detailed  knowledge of both the scene
illumination and the  reflectance of the  surface material.   For this
equation  to   be  used to recover  surface   orientation from   image
irradiance, additional   constraints are  necessary.   The constraints
usually  employed require that  the  recovered surface  be smooth.  We
demonstrate that smoothness is not sufficient for this task.

A new formulation of shape from shading  is presented in which surface
orientation is related to image irradiance  without requiring detailed
knowledge of the scene illumination, or  of the  albedo of the surface
material.    This formulation,  which assumes isotropic    scattering,
provides some interesting performance parallels  to those exhibited by
the human visual system.}
}

@techreport{aic-tn-1982:272,
number=272,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Local Shading Analysis",
author={Alex P. Pentland},
month="November",
year=1982,
keywords={Vision!Shading analysis},
abstract={
   Local analysis of image shading, in the absence of prior
knowledge about the viewed scene, may be used to provide information
about the scene.  The following has been proved.

     \underline{Every} image point has the  same  image intensity and  first and second
derivatives  as  the image of  an umbilical point (a  point with equal
principal curvatures) on a Lambertian surface; there  is \underline{exactly one}
combination of surface orientation, curvature, (overhead) illumination
direction and albedo times illumination intensity  that will produce a
particular set of image intensity and first and second derivatives.  A
solution for the unique combination of surface  orientation,  etc., at
umbilical points is presented.

     This solution has been extended by using general position and regional
constraints to obtain estimates of the following:

\begin{itemize}
\item 	Surface orientation at each image point

\item	Whether the surface is planar, singly or doubly curved at each
		point

\item 	The mean illuminant direction within a region

\item	Whether a region is convex, concave, or is a saddle surface.

\end{itemize}

     Algorithms to recover illuminant  direction, identify discontinuities,
and estimate surface orientation  have been evaluated on both  natural
and synthesized  images,  and  have  been found to    produce   useful
information about the scene.}
}

@techreport{aic-tn-1982:271,
number=271,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Interpreting Perspective Images",
author={Stephen T. Barnard},
month="November",
year=1982,
keywords={Vision!Perspectives},
abstract={
  A fundamental problem in computer vision is how to determine
the 3-D spatial  orientation  of curves  and  surfaces appearing in an
image.  The problem is generally under-constrained, and is complicated
by the fact  that metric properties, such as  orientation and  length,
are not invariant under projection.  Under perspective projection (the
correct model for most  real images)  the transform is nonlinear,  and
therefore hard to invert.  Two   constructive  methods are  presented.
The  first finds the  orientation   of  parallel lines  and planes  by
locating vanishing points and  vanishing lines.  The second determines
the   orientation  of  plans by   ``backprojection''  of  two  intrinsic
properties of contours: angle magnitude and curvature.}
}

@techreport{aic-tn-1982:270,
number=270,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Dialogic:  A Core Natural-Language Processing System",
author={Barbara J. Grosz and Norman Haas and Gary Hendrix and Jerry Hobbs and Paul Martin and Robert Moore and Jane Robinson and Stanley Rosenschein},
month="November",
year=1982,
keywords={DIALOGIC, Natural Language!Processing},
abstract={
    The  DIALOGIC system   translates  English sentences  into
representations  of their  literal  meaning  in the  context    of  an
utterance.  These representations, or ``logical forms,''  are intended to
be a purely  formal language   that  is as close  as possible  to  the
structure   of   natural    language,   while providing  the  semantic
compositionality   necessary   for meaning-dependent     computational
processing.  The  design of DIALOGIC  (and of its constituent modules)
was influenced by the goal of using it as the core language-processing
component in a variety of systems, some  of which are transportable to
new domains of application.}
}

@techreport{aic-tn-1982:268,
number=268,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Nonclausal Connection-Graph Resolution Theorem-Proving Program",
author={Mark E. Stickel},
month="October",
year=1982,
keywords={Deduction!Nonclausal Connection-Graph},
abstract={
   A new   theorem-proving  program,   combining  the use   of
nonclausal resolution and connection graphs, is described.  The use of
nonclausal resolution as the inference  system eliminates some of  the
redundancy  and  unreadability of clause-based systems.   The use of a
connection graph restricts   the search  space  and facilitates  graph
searching for efficient deduction.}
}

@techreport{aic-tn-1982:267,
number=267,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Modeling and Using Physical Constraints In Scene Analysis",
author={Martin A. Fischler, Stephen T. Barnard, Robert C. Bolles, Michael Lowry, Lynn Quam, Grahame Smith, and Andrew Witkin},
month="September",
year=1982,
keywords={Vision!Modeling},
abstract={
   This  paper describes the  results obtained  in a  research
program  ultimately concerned with deriving a   physical sketch  of  a
scene  from one  or   more images.  Our   approach   involves modeling
physically meaningful information that can  be used  to constrain  the
interpretation process, as well as modeling  the actual scene content.
In particular, we address the problems of modeling the imaging process
(camera and illumination), the scene geometry (edge classification and
surface   reconstruction), and  elements of    scene content (material
composition and skyline delineation).}
}

@techreport{aic-tn-1983:266R,
number=266R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Domain Independent Planning:  Representation and Plan Generation",
author={David Wilkins},
month="May",
year=1983,
keywords={Planning!Domain Independent},
abstract={
   A   domain  independent  planning program   that  supports both
automatic and interactive generation of hierarchical,  partially ordered  plans
is   described.   An      improved    formalism  makes     extensive   use   of
\underline{constraints}   and  \underline{resources}  to represent  domains and
actions  more powerfully.  The formalism   also  offers  efficient methods  for
representing  properties  of  objects that  do not  change over  time,   allows
specification of the  plan rationale (which  includes scoping of conditions and
appropriately  relating different levels  in the hierarchy),  and  provides the
ability to  express deductive rules for deducing  the effects of  actions.  The
implications of  allowing parallel actions  in a  plan  or problem solution are
discussed, and  new techniques for efficiently  detecting and remedying harmful
parallel interactions are presented.  The  most important of these  techniques,
reasoning about resources,  is emphasized and explained.   The system  supports
concurrent exploration of different branches in  the search, making  best-first
search                easy                     to                  implement.}
}

@techreport{aic-tn-1982:265,
number=265,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A View Of The Fifth Generation and Its Impact",
author={David H. D. Warren},
month="July",
year=1982,
keywords={Fifth Generation},
abstract={
  In  October 1981,  Japan   announced a national   project to
develop highly innovative  computer systems for the   1990s,  with the
title ``Fift" Generation Computer  Systems.''  This paper  is a personal
view of that project, its significance, and reactions to it.}
}

@techreport{aic-tn-1982:264,
number=264,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Role Of Logic In Knowledge Representation and Commonsense Reasoning",
author={Robert C. Moore},
month="June",
year=1982,
keywords={Logic!Role, Knowledge Representation, Commonsense Reasoning},
abstract={
  This paper examines the role that formal logic ought to play
in  representing and reasoning with commonsense  knowledge.  We   take
267issue with the commonly held view (as expressed by Newell [1980]) that
the use of  representations based on  formal logic is inappropriate in
most   applications  of  artificial  intelligence.   We argue  to  the
contrary  that  there   is  an important   set   of issues,  involving
incomplete knowledge  of a problem  situation, that so  far have  been
addressed only    by systems  based   on formal  logic  and  deductive
inference, and that, in some sense, probably can be dealt with only by
systems based   on logic and deduction.   We  further   argue that the
experiments of  the late 1960s on  problem-solving by  theorem-proving
did not show  that the use  of logic and deduction in  AI  systems was
necessarily  inefficient, but  rather that what was needed  was better
control of the deduction process, combined with  more attention to the
computational properties of axioms.}
}

@techreport{aic-tn-1982:263R,
number=263R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Team:  A Transportable Natural-Language Interface System",
author={Barbara J. Grosz},
month="November",
year=1982,
keywords={TEAM, Natural Language!Interface},
abstract={
   A  major benefit of  using  natural language  to access the
information in a database is that it shifts onto the system  to burden
of mediating between two views of the data: the way in which the  data
is stored (the ``database  view''),  and the  way in  which  an end-user
thinks about it (the ``user's view'').  Database information is recorded
in  terms of   files, records, and    fields, while   natural-language
expressions  refer to the  same information in terms  of entities  and
relationships  in  the   world.  A major    problem in constructing  a
natural-language interface  is determining how  to encode and use  the
information    needed  to     bridge   these  two    views.    Current
natural-language  interface  systems  require  extensive  efforts   by
specialists in  natural-language processing to  provide them with  the
information they need to do the bridging.  The systems are, in effect,
handtailored to provide access to particular databases.

     This paper focuses on the problem of constructing \underline{transportable}
natural-language interfaces, i.e.,   systems that can  be adapted   to
provide access  to  databases  for which they were  not   specifically
handtailored.   It describes an  initial  version   of a transportable
system, called TEAM (for \underline{T}ransportable \underline{E}nglish \underline{A}ccess Data
\underline{M}anager).  The hypothesis underlying the research  described in  this
paper is  that  the information  required  for the  adaptation  can be
obtained  through  an  interactive dialogue  with  database management
personnel  who  are   not   familiar with natural-language  processing
techniques.}
}

@techreport{aic-tn-1982:261,
number=261,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Computational Stereo",
author={Stephen T. Barnard and Martin A. Fischler},
month="March",
year=1982,
keywords={Vision!Stereo},
abstract={
  Perception of  depth is  a  central  problem in machine
vision.   Stereo  is an  attractive  technique  for  depth  perception
because, compared to monocular  techniques, it  leads to more  direct,
unambiguous, and  quantitative  depth measurements,  and  unlike  such
``active'' approaches  as radar  and laser  ranging,  it is suitable  in
almost all application domains.

We  broadly  define computational   stereo    as  the recovery  of the
three-dimensional characteristics  of   a scene  from multiple  images
taken from  different points of view.  The  first part  of   the paper
identifies and discusses each  of  the  functional components  of  the
computational stereo  paradigm:  image acquisition,   camera modeling,
feature acquisition, matching, depth determination, and interpolation.
The second  part   discusses the    criteria that are   important  for
evaluating    the    effectiveness  of  various  computational  stereo
techniques.   The third part  surveys  a  representative  sampling  of
computational stereo research.}
}

@techreport{aic-tn-1982:259,
number=259,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Planning Natural-Language Utterances To Satisfy Multiple Goals",
author={Douglas E. Appelt},
month="March",
year=1982,
keywords={Natural Language!Planning},
abstract={
  This dissertation presents the results of research on a
planning formalism for  a theory of   natural-language generation that
will support the generation of utterances that satisfy multiple goals.
Previous   research    in    the   area  of  computer   generation  of
natural-language utterances  has concentrated two  aspects of language
production: (1) the process of producing surface syntactic forms  from
an underlying representation,  and (2) the  planning of  illocutionary
acts to satisfy  the speaker's goals.  This  work concentrates on  the
interaction between   these two aspects  of  language   generation and
considers the overall problem to be one of  refining the specification
of an illocutionary act into a surface syntactic form, emphasizing the
problems of achieving multiple goals in a single utterance.

     Planning utterances requires an ability to reason in detail about
what the   hearer   knows   and wants.   A   formalism,  based  on   a
possible-worlds  semantics of  an intentional  logic  of knowledge and
action,  was used for representing the  effects  of illocutionary acts
and the speaker's  beliefs about the hearer's  knowledge of the world.
Techniques are described that  enable  a planning  system to   use the
representation effectively.

     The  language-planning theory and   knowledge  representation are
embodied in  a computer system called KAMP   (Knowledge and Modalities
Planner), which plans both physical  and  linguistic  actions, given a
high-level description of the speaker's goals.

     The  research   has  application  to  the   design  of gracefully
interacting computer systems, multiple-agent planning systems, and the
planning of knowledge acquisition.}
}

@techreport{aic-tn-1981:257,
number=257,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Research On Natural-Language Processing At Sri",
author={Barbara J. Grosz},
month="November",
year=1981,
keywords={Natural Language!At SRI, DIALOGIC, TEAM},
abstract={
  Research on natural-language processing at  SRI spans a
broad spectrum  of activity.  Two of our  major current efforts  are a
pair  of  research  projects  under  the sponsorship   of  the Defense
Advanced Research Projects  Agency.  The TEAM project  is intended  to
provide  natural-language access to large  databases via  systems that
are easily adaptable to a wide range of new application domains.   The
KLAUS  project is a  longer-range  effort  to  address  basic research
problems in natural-language semantics, commonsense reasoning, and the
pragmatics   of natural-language  communication.  These  two  projects
share a common core-language-processing system called DIALOGIC.}
}

@techreport{aic-tn-1981:256,
number=256,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Probabilistic Model For Uncertain Problem Solving",
author={Arthur M. Farley},
month="December",
year=1981,
keywords={Reasoning!Probabilistic},
abstract={
  With growing interest in the application of research to
problems  that   arise  in   real-world  contexts,   issues raised  by
consideration   of  uncertain states  and    unreliable  operators are
receiving increased attention in artificial intelligence research.  In
this paper, a model is presented for  dealing with such concerns.  The
model is  a  probabilistic  generalization of the  familiar  notion of
problem space.   The specification of  uncertain states and unreliable
operators is discussed.  Problem-solving search methods are described.
The need for information gathering is established.  Search methods are
generalized to produce tree-structured plans incorporating the use  of
such operators.    Several   application domains  for   our  model are
discussed.}
}

@techreport{aic-tn-1981:255,
number=255,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Database As Model:  A Metatheoretic Approach",
author={Kurt Konolige},
month="September",
year=1981,
keywords={Relational Database Model},
abstract={
  This paper presents  a method  of formally representing
the information that is available to a user of a  relational database.
The intended application area is deductive question-answering  systems
that  must  access  an  existing relational    database.   To  respond
intelligently  to user  inquiries,  such  systems   must  have a  more
complete representation of the domain of discourse than is generally
available in the tuple sets of a relational database.  Given this more
expressive representation, the problem then arises of how to reconcile
the   information   present  in the    database  with   the   domain
representation, so that database queries can be  derived to answer the
user's inquiries.  Here we take  the formal approach  of describing  a
relational database as the model  of a  first-order language.  Another
first-order language, the metalanguage, is used both  to represent the
domain of discourse, and to describe the relationship of  the database
to the  domain.  The formal advantages  of this approach are presented
and contrasted with other work in the area.}
}

@techreport{aic-tn-1981:254,
number=254,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Team User's Guide",
author={Armar A. Archbold and Barbara Grosz and Daniel Sagalowicz},
month="December",
year=1981,
keywords={TEAM, Natural Language!Interface},
abstract={
  TEAM  (Transportable English Data Access  Manager) is a
computer system designed  to  acquire information  about a  (local  or
remote) database,  and subsequently to interpret and  answer questions
addressed to the database in a subset of natural language.

     The  system   currently includes  an   interactive  program  that
acquires information about a database from the database administrator.
The user  is  asked how the  database is  to  be  structured  and what
language   is to be used to   talk about its    contents.  The program
augments the following  components  of a  core natural language access
system:

\begin{itemize}
\item  The lexicon

\item  The conceptual model of the domain (or conceptual schema)

\item  The structural model of the database (or database schema).
\end{itemize}

     After  these steps  have   been  completed, the   system  accepts
natural-language questions   about the  database contents  and, to the
extent it is able, provides relevant responses.}
}

@techreport{aic-tn-1981:253,
number=253,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Computational Strategies For Analyzing The Organization And, Use Of Information",
author={Donald E. Walker},
month="July",
year=1981,
keywords={Information!Organization and use},
abstract={
   This    chapter   describes    new   developments   in
computer-based procedures that  can improve our  understanding  of how
people organize  and  use information.   Relevant   recent research in
information science,     computational   linguistics,  and  artificial
intelligence is reviewed.  A program of  research is presented that is
producing systems that make it possible to study  the organization and
use  of  information and, at   the same   time, provide more effective
support for people engaged in those activities.}
}

@techreport{aic-tn-1981:252,
number=252,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Machine Learning For Information Management",
author={Norman Haas and Gary Hendrix},
month="July",
year=1981,
keywords={Information!Management, Learning},
abstract={
  This paper discusses machine learning in the context of
information  management.  The core idea is  that  of a compiler system
that can hold a conversation with a user in English about his specific
domain  of  interest,  subsequently  retrieve and  display information
conveyed by the user, and  apply various types  of  external  software
systems to solve user problems.

     The specific   learning  problems discussed  is   how  to  enable
computer systems to acquire information about  domains with which they
are unfamiliar from people who  are expert  in those domains, but have
little  or no training  in  computer  science.  The information  to be
acquired  is    that needed  to   support  question-answering  or fact
retrieval tasks, and the  type of learning to  be employed is learning
by  being told.  Reflecting  the intimate connection between  language
and reasoning, this paper  is  largely concerned with the  problems of
learning concepts and language simultaneously.}
}

@techreport{aic-tn-1981:248,
number=248,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Artificial Intelligence:  Engineering, Science Or Slogan?",
author={Nils J. Nilsson},
month="July",
year=1981,
keywords={Artificial Intelligence},
abstract={
   This  paper   presents    the  view   that  artificial
intelligence (AI) is primarily  concerned with propositional languages
for representing knowledge and  with techniques for manipulating these
representations.   In  this  respect,    AI is analogous   to  applied
mathematics;  its representations and techniques can  be applied in  a
variety of other subject areas.  Typically, AI research  is (or should
be)  more  concerned  with   the  general  form    and  properties  of
representational languages  and  methods than  it is with  the content
being   described  by  these languages.     Notable exceptions involve
``commonsense'' knowledge about  the everyday world (no other  specialty
claims this subject  area as its  own), and world  (no knowledge about
the properties  and uses of  knowledge itself).  In  these areas AI is
concerned with content as  well as  form.   We also  observe that  the
technology   that   seems to    underlie peripheral sensory  and motor
activities (analogous to low-level animal  or human vision and  muscle
control) seems to be quite different from the technology that seems to
underlie cognitive reasoning and problem solving.  Some definitions of
AI would include peripheral  as well  as cognitive processes;  here we
argue against including the peripheral processes.}
}

@techreport{aic-tn-1981:246,
number=246,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Deductive Synthesis Of The Unification Algorithm",
author={Zohar Manna and Richard Waldinger},
month="July",
year=1981,
keywords={Program Synthesis},
abstract={
  The deductive approach is a formal program construction
method in which the derivation of a program from a given specification
is  regarded as a theorem-proving task.   To construct a program whose
output satisfies   the conditions  of the specification,  we   prove a
theorem  stating   the  existence of such  an   output.  The proof  is
restricted to be sufficiently constructive so that a program computing
the desired output  can  be extracted directly  from  the proof.   The
program we obtain is  applicative and may  consist of several mutually
recursive procedures.   The proof constitutes  a demonstration of  the
correctness of this program.

     To exhibit the full power of the deductive approach,  we apply it
to a nontrivial  example--the  synthesis of  a unification  algorithm.
Unification is the process  of  finding a  common  instance  of  two
expressions.  Algorithms to  perform unification have been central  to
many   theorem-proving    systems and   to some  programming-language
processors.

     The  task of  deriving a  unification  algorithm automatically is
beyond the power of existing program synthesis systems.  In this paper
we use the  deductive  approach to derive  an algorithm from a simple,
high-level specification of  the unification task.   We will identify
some  of  the  capabilities required  of  a theorem-proving  system to
perform this derivation automatically.}
}

@techreport{aic-tn-1981:244,
number=244,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Detection Of Rivers In Low-Resolution Aerial Imagery",
author={Grahame B. Smith},
month="June",
year=1981,
keywords={Vision!River detection},
abstract={
   This paper describes an  operator for detecting rivers
in low-resolution aerial imagery.  The operator provides  results that
would allow graph-traversing  routines to  delineate these structures.
The approach is  to look for the  typical river profile involving  not
only the water component of the river, but  its surrounding vegetation
as well.}
}

@techreport{aic-tn-1981:243,
number=243,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Knowledge-Engineering Techniques and Tools",
author={Rene Reboh},
month="May",
year=1981,
keywords={Knowledge Engineering, Expert Systems},
abstract={
  Techniques and tools to assist in several phases of the
knowledge-engineering process  for   developing  an expert  system are
explored.

     A  sophisticated domain-independent network   editor is described
that  uses knowledge   about the  representational  and  computational
formalisms   of the   host  consultation   system   to  watch over the
knowledge-engineering process and  to give  the knowledge  engineer a
convenient environment for developing, debugging, and  maintaining the
knowledge base.

     We also illustrate how partial matching techniques can  assist in
maintaining the consistency of   the   knowledge  base (in  form   and
content) as it grows, and can support a variety of features  that will
enhance the interaction between the  system  and the user  and make  a
knowledge-based consultation system behave more intelligently.

     Although  these techniques and features are  illustrated in terms
of the  Prospector environment, it  will be clear to   the reader  how
these techniques can be applied in other environments.}
}

@techreport{aic-tn-1981:241,
number=241,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Problems In Logical Form",
author={Robert C. Moore},
month="April",
year=1981,
keywords={Natural Language!Logical form},
abstract={
  Most   current  theories of natural-language processing
propose that the  assimilation of an  utterance involves producing  an
expression or  structure   that in some sense represents   the literal
meaning of the utterance.   It  is often maintained that understanding
what an utterance literally  means consists  in being  able to recover
such a  representation.  In  philosophy and linguistics   this sort of
representation is  usually said  to display
the \underline{logical form}  of an utterance.

     This paper   surveys  some  of the  key  problems that arise   in
defining a system of  representation for the logical  forms of English
sentences  and suggests  possible  approaches to their  solution.   We
first look  at some general  issues relating to  the notion of logical
form, explaining why it  makes sense to define such  a notion only for
sentences  in  context, not   in  isolation, and  we    discuss    the
relationship  between research on logical   form and work on knowledge
representation in artificial  intelligence.  The rest  of the paper is
devoted to examining specific problems in logical form.  These include
the following: quantifiers;  events, actions and  processes; time  and
space; collective entities and substances; propositional attitudes and
modalities; questions and imperatives.}
}

@techreport{aic-tn-1981:240,
number=240,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Natural Language Access To Medical Text",
author={Donald E. Walker and Jerry R. Hobbs},
month="March",
year=1981,
keywords={DIAGRAM, DIAMOND, Natural Language!Interface},
abstract={
  This paper describes  research on the  development of a
methodology  for   representing  the  information   in  texts and   of
procedures for relating the  linguistic structure of a  request to the
corresponding representations.  The work is being done in the  context
of a  prototype system that will allow  physicians and  other   health
professionals  to access information   in a computerized  textbook  of
hepatitis through natural  language dialogues.  The interpretation  of
natural     language  queries is    derived  from  DIAMOND/DIAGRAM,  a
linguistically  motivated,    domain-independent     natural  language
interface  developed  at  SRI.   A text  access component    is  being
developed that uses  representations of  the propositional content  of
text passages and of the hierarchical structure of the text as a whole
to retrieve relevant information.}
}

@techreport{aic-tn-1981:239,
number=239,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Automatic Deduction For Commonsense Reasoning:  An Overview",
author={Robert C. Moore},
month="April",
year=1981,
keywords={Deduction!Commonsense Reasoning, Commonsense Reasoning},
abstract={
    How   to  enable  computers    to draw    conclusions
automatically from  bodies  of facts has   long been recognized  as  a
central problem in artificial-intelligence (AI) research.  Any attempt
to address this problem requires choosing  an application (or  type of
application),  a  representation for bodies  of facts, and methods for
deriving conclusions.  This article provides an overview of the issues
involved in drawing conclusions by means of  deductive inference  from
bodies of commonsense knowledge represented by logical  formulas.   We
first briefly review the history of this enterprise: its  origins, its
fall into disfavor, and its recent revival.  We  show why applications
involving certain types  of incomplete information resist solution  by
other   techniques, and     how   supplying domain-specific    control
information  seems  to offer a solution    to  the difficulties   that
previously led to disillusionment  with automatic deduction.  Finally,
we discuss the relationship of automatic deduction to the new field of
``logic programming,'' and we survey some of the issues that arise  in
extending automatic-deduction techniques to nonstandard logics.}
}

@techreport{aic-tn-1981:237,
number=237,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Natural-Language Processing Part One:  The Field In Perspective",
author={Gary G. Hendrix and Earl D. Sacerdoti},
month="July",
year=1981,
keywords={Natural Language!Processing},
abstract={
    This article  deals  with  the  problems  of enabling
computers to  communicate with humans in  natural  languages,  such as
English and French,  as distinguished from  formal  languages, such as
BASIC  and  PASCAL.  Major issues  in  natural-language processing are
discussed by examining several experimental computer systems developed
over the  last decade.   The intent of  the authors is  to demonstrate
that natural-language processing techniques are useful  now, to reveal
the richness of the computations  performed  by human natural-language
communicators, and to explain why the  fluent use  of natural language
by machines remains an elusive aspiration.}
}

@techreport{aic-tn-1981:233,
number=233,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Computational Structures For Machine Perception",
author={Martin A. Fischler},
month="January",
year=1981,
keywords={Vision!Computational structures},
abstract={
  This note  discusses the adequacy  of  current computer
architectures to serve as a base for building machine vision  systems.
Arguments are  presented  to  show that perceptual  problems cannot be
completely formalized and dealt with in a closed abstract system.  The
conclusion  is   that  the     digital   computer,   organized  as   a
general-purpose   symbol  processor,  cannot   serve  as  an  adequate
instrument for achieving a human-like visual capability.}
}

@techreport{aic-tn-1980:232,
number=232,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A First-Order Formalization Of Knowledge and Action For A Multiagent, Planning System",
author={Kurt Konolige},
month="December",
year=1980,
keywords={Multiple Agents!Formalization},
abstract={
  We are interested  in   constructing a  computer  agent
whose behavior will be intelligent enough to perform cooperative tasks
involving other agents like itself.   The construction of such  agents
has been a major goal of artificial intelligence research.  One of the
key tasks such an agent must perform is to form plans to carry out its
intentions  in a complex  world  in   which other planning agents also
exist.  To construct such agents,  it  will be  necessary to address a
number of issues that  concern the interaction  of knowledge, actions,
and planning.   Briefly stated, an  agent at planning  time must  take
into account what his future states of  knowledge will be if he  is to
form plans that he can execute; and if he  must incorporate the  plans
of other agents  into  his own, then he must  also be able  to  reason
about the knowledge and  plans of other  agents in an appropriate way.
These ideas have been  explored  by several   researchers,  especially
McCarthy and Hayes [McCarthy and Hayes, 1969] and Moore [Moore 1980].

     Despite  the importance  of this problem, there  has  not  been a
great deal of work in the area of formalizing  a solution.  Formalisms
for both action  and knowledge separately have been  examined  in some
depth, but there have been few attempts at a synthesis.  The exception
to  this  is Moore's thesis  on  reasoning  about knowledge and action
[Moore 1980], for which  a planner has  been recently proposed [Appelt
1980].  Moore shows how a formalism based  on possible-world semantics
can be used to  reason about the  interaction of knowledge and action.
In this paper we develop an alternative  formalism for reasoning about
knowledge, belief, and action; we show how this formalism  can be used
to deal  with  several well-known problems,  and then  describe how it
could be used by a plan constructing systems.}
}

@techreport{aic-tn-1980:230,
number=230,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Klaus: A System For Managing Information and Computational Resources",
author={Gary G. Hendrix},
month="October",
year=1980,
keywords={KLAUS, Natural Language!Interface},
abstract={
  This report presents  a broad-brush description of  the
basic goals and philosophy of a research program  at SRI International
(SRI)  aimed at developing  the technology needed  to support  systems
that can be tutored in English about  new subject areas,  and that can
therefore aid the initial or subsequent user in  filing and retrieving
information, and  in  conveniently applying to the  new  subject  area
other computer software, such as data-base  management systems (DBMS),
planners,  schedulers, report  generators, simulators  and   the like.
These  systems, which we  call  Knowledge Learning and  Using  Systems
(KLAUS), are intended to act as brokers between  the user's needs,  as
expressed in the user's terms, and  the resources available  in a rich
computational environment.}
}

@techreport{aic-tn-1980:229,
number=229,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Interplay Between Experimental and Theoretical Methods, In Artificial Intelligence",
author={Nils J. Nilsson},
month="September",
year=1980,
keywords={Artificial Intelligence},
abstract={
  This note  alleges that there  is a dichotomy   between
theoretical  and experimental  work   in Artificial Intelligence (AI).
The reasons for this dichotomy are discussed, and  AI is compared with
other,  more mature disciplines in which  there is closer  cooperation
between experimental and  theoretical branches.   Some recommendations
are given for achieving this needed cooperation.}
}

@techreport{aic-tn-1981:228,
number=228,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Transportable Natural-Language Interfaces To Databases",
author={Gary G. Hendrix and William H. Lewis},
month="April",
year=1981,
keywords={Natural Language!Interface},
abstract={
  Several computer systems have now been constructed that
allow  users  to access databases  by    posing questions  in  natural
languages, such as English.  When used in  the restricted domains  for
which they have been especially designed, these  systems have achieved
reasonably high levels of performance.  However, these systems require
the encoding of knowledge about  the domain of application in  complex
data structures that typically can be created for a  new database only
with considerable effort on the  part of a  computer professional  who
has had special training in computational linguistics and  the use  of
databases.

     This paper describes initial work on  a  methodology for creating
natural-language processing capabilities for new databases without the
need for intervention  by specially trained  experts.  The approach is
to  acquire logical schemata and  lexical  information through  simple
interactive dialogues with someone who is familiar with  the  form and
content of  the  database, but   unfamiliar   with  the technology  of
natural-language  interfaces.   A   prototype    system   using   this
methodology is described and an example transcript is presented.}
}

@techreport{aic-tn-1980:227,
number=227,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Approach To Acquiring and Applying Knowledge",
author={Norman Haas and Gary G. Hendrix},
month="November",
year=1980,
keywords={Learning, Natural Language!Interface, Knowledge Acquisition},
abstract={
  The problem addressed in this paper  is how to enable a
computer system to acquire facts about new domains from tutors who are
experts in their respective fields, but who have little or no training
in computer science.  The information to be acquired is that needed to
support question-answering activities.  The basic acquisition approach
is ``learning by being  told.'' We have  been  especially interested in
exploring the notion of simultaneously learning not only new concepts,
but also the linguistic constructions used to express those  concepts.
As a research vehicle we have developed a system that is preprogrammed
with deductive algorithms and a fixed set of syntactic/semantic  rules
covering a  small  subset of  English.   It   has  been  endowed  with
sufficient seen  concepts and  seed vocabulary  to  support  effective
tutorial interaction.  Furthermore, the system is  capable of learning
new concepts and vocabulary, and can apply its acquired knowledge in a
range of problem-solving situations.}
}

@techreport{aic-tn-1980:226,
number=226,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Problematic Features Of Programming Languages: Situational-Calculus, Approach Part I:  Assignment Statements",
author={Zohar Manna and Richard Waldinger},
month="November",
year=1980,
keywords={Program Synthesis, Situational Calculus},
abstract={
  Certain features of programming languages, such as data
structure operations and procedure call mechanisms, have been found to
resist formalization by classical techniques.   An alternate  approach
is presented, based on a ``situational calculus,'' which makes explicit
reference  to  the  states  of a  computation.    For  each  state,  a
distinction is drawn  between   an  expression, its  value,   and  the
location of the value.

     Within this conceptual framework,  the features of  a programming
language can be described axiomatically.  Programs in the language can
then be synthesized, executed, verified, or transformed by  performing
deductions in this axiomatic system.  Properties of  entire classes of
programs, and of   programming languages, can also    be expressed and
proved in   this    way.   The  approach   is    amenable to   machine
implementation.

     In a   situational-calculus formalism it  is   possible  to model
precisely  many  ``problematic'' features  of   programming languages,
including operations on  such  data  structures as  arrays,  pointers,
lists, and    records,   and  such    procedure call    mechanisms  as
call-by-reference, call-by-value,   and call-by-name.   No  particular
obstacle is presented by aliasing between variables,  by declarations,
or by recursive procedures.

     The paper  is divided into  three parts, focusing respectively on
the  assignment statement,   on  data structure   operations,  and  on
procedure call  mechanisms.   In this  first  part, we  introduce  the
conceptual  framework to be     applied  throughout  and  present  the
axiomatic  definition  of  the  assignment  statement.    If  suitable
restrictions on  the programming language are imposed,  the well-known
Hoare assignment axiom can then be proved as a theorem.   However, our
definition can also describe the  assignment statement of unrestricted
programming languages, for which the Hoare axiom does not hold.}
}

@techreport{aic-tn-1980:225,
number=225,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Interpreting Discourse:  Coherence and The Analysis Of, Ethnographic Interviews",
author={Michael Agar and Jerry R. Hobbs},
month="August",
year=1980,
keywords={Natural Language!Discourse},
abstract={
  The data we  analyze is from a  series  of life history
interviews with a career heroin addict in New York, collected by  Agar
(1981).  We analyze this data  in terms  of  a combination  of two  AI
approaches to discourse.  The  first is work  on the  inferencing that
must take place in  people's  comprehension and  production of natural
language discourse.  The second approach to discourse applies  work on
planning to the planning  of individual speech  acts and  to the plans
speakers  develop for effecting  their  goals  in  larger stretches of
conversation.

     In this paper we  first outline how  we apply these approaches to
the ethnographic data.   We discuss three kinds of  coherence in terms
of which  we  analyze a  text,    and then describe   our  method more
generally.  We next give an example of the method of microanalysis  on
a short fragment of an interview, and then show how the beliefs, goals
and concerns that the microanalysis has revealed are tied  in with the
rest of the corpus.  Finally, we discuss the significance of this work
for ethnography.}
}

@techreport{aic-tn-1980:224,
number=224,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A D-Ladder User's Guide",
author={Daniel Sagalowicz},
month="September",
year=1980,
keywords={LADDER!D-LADDER, Natural Language!Interface},
abstract={
  D-LADDER  (DIAMOND-based Language Access to Distributed
Data with Error  Recovery) is  a computer  system designed  to provide
answers to questions posed at  the  terminal in   a subset of  natural
language regarding   a distributed data  base of  naval    command and
control information.  The system accepts  natural-language questions
about   the data.   For each question    D-LADDER plans a  sequence of
appropriate queries to the data base  management system, determines on
which machines  the queries are to be  processed, establishes links to
those machines  over  the  ARPANET,  monitors the   processing of  the
queries and answer to the original question.

     This user's guide is intended for the person who knows how to log
in to the host operating system, as well  as  how to enter  and edit a
line of text.  It does not explain how D-LADDER works, but rather  how
to use it on a demonstration basis.}
}

@techreport{aic-tn-1980:222,
number=222,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reconstructing Smooth Surfaces From Partial, Noisy Information",
author={Harry G. Barrow and J. Martin Tenenbaum},
month="July",
year=1980,
keywords={Vision},
abstract={
  Interpolating smooth surfaces from  boundary conditions
is a  ubiquitous problem in  early visual processing.    We describe a
solution for an important special case:  the interpolation of surfaces
that are  locally spherical  or  cylindrical  from initial orientation
values and  constraints   on orientation.  The   approach  exploits an
observation that components   of   the unit normal  vary   linearly on
surfaces  of uniform   curvature, which permits   implementation using
local parallel processes.    Experiments on spherical and  cylindrical
test cases have produced essentially  exact reconstructions, even when
boundary values were extremely sparse  or only partially  constrained.
Results on other test cases  seem in  reasonable agreement with  human
perception.}
}

@techreport{aic-tn-1980:221,
number=221,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Scene Modeling:  A Structural Basis For Image Description",
author={Jay M. Tenenbaum and Martin A. Fischler and Harry G. Barrow},
month="July",
year=1980,
keywords={Vision!Scene modeling},
abstract={
  Conventional statistical  approaches to  image  modeling are
fundamentally limited because they take  no account of  the underlying
physical structure of the  scene  nor  of the image formation process.
The image features being modeled are frequently artifacts of viewpoint
and illumination that have no  intrinsic significance for higher-level
interpretation.  In this paper a  structural approach  to modeling  is
argued for  that  explicitly relates image   appearance to  the  scene
characteristics from which it arose.  After establishing the necessity
for structural  modeling in image analysis, a  specific representation
for scene  structure  is proposed   and then a  possible computational
paradigm for recovering this description from an image is described.}
}

@techreport{aic-tn-1980:220,
number=220,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Storage Representation For Efficient Access To Large, Multidimensional Arrays",
author={Lynn H. Quam},
month="April",
year=1980,
keywords={Storage representation},
abstract={
  This paper addresses problems associated with accessing
elements of large multidimensional arrays when the order  of access is
either unpredictable or  is orthogonal to the conventional   order  of
array  storage.  Large  arrays are defined as  arrays that are  larger
than the physical  memory immediately available  to store them.   Such
arrays must be accessed either  by the virtual memory  system  of  the
computer and operating system, or by direct input and output of blocks
of the array to a file system.  In  either case, the  direct result of
an inappropriate  order of reference  to the elements  of the array is
the very time-consuming movement of data between levels  in the memory
hierarchy,   often costing factors  of  three orders  of  magnitude in
algorithm performance.

     The access to  elements of large arrays  is decomposed into three
steps: transforming the subscript   values of an  n-dimensional  array
into  the element number in a  one-dimensional virtual array,  mapping
the virtual array position to  physical memory position, and accessing
the array element in physical memory.  The virtual-to-physical mapping
step    is unnecessary on  computer  systems  with  sufficiently large
virtual address  spaces.  This paper is  primarily  concerned with the
first step.

     A subscript  transformation  is proposed that solves  many of the
order-of-access problems associated with  conventional array  storage.
This  transformation is   based  on an additive  decomposition of  the
calculation of element number in the  array into the  sum of a  set of
integer functions applied to the set of subscripts as follows:

\begin{center}
element-number(i,j,...) = fi(i) + fj(j) + ...
\end{center}

     Choices for  the transformation functions  that  minimize  access
time to the   array  elements  depend on  the   characteristics of the
computer system's memory hierarchy  and the order  of accesses  to the
array elements.  It is  conjectured that given appropriate models  for
system and algorithm access  characteristics,  a pragmatically optimum
choice  can  be  made for the subscript transformation  functions.  In
general these   models  must be stochastic,   but   in   certain cases
deterministic models are possible.

     Using tables  to    evaluate the  functions   fi  and  fj   makes
implementation very efficient with  conventional computers.  When  the
array  accesses are made in  an  order  inappropriate to  conventional
array storage order,   this scheme  requires  far less  time  than for
conventional array-accessing schemes; otherwise,  accessing times  are
comparable.

     The semantics  of   a set of procedures  for  array access, array
creation, and the  association of arrays with file   names is defined.
For computer  systems  with insufficient  virtual memory, such  as the
PDP-10, a  software   virtual-to-physical mapping scheme   is given in
Appendix C.  Implementations are also  given in the appendix  for  the
VAX and PDP-10  series   computers to access   pixels of large  images
stored as two-dimensional arrays of n bits per element.}
}

@techreport{aic-tn-1980:213,
number=213,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Random Sample Consensus: A Paradigm For Model Fitting With  To Image Analysis and Automated Cartography",
author={Martin A. Fischler and Robert C. Bolles},
month="March",
year=1980,
keywords={Vision!Random sample consensus, Vision!Cartography},
abstract={
  In  this  paper  we introduce a    new paradigm, Random
Sample Consensus (RANSAC), for fitting a  model  to experimental data.
RANSAC  is   capable  of  interpreting/smoothing  data   containing  a
significant percentage of gross errors, and thus is ideally suited for
applications in automated image analysis where interpretation is based
on the data  provided  by  error-prone  feature detectors.   A   major
portion  of  this  paper describes  the application  of RANSAC  to the
Location Determination Problem (LDP): given an  image depicting  a set
of landmarks with known locations, determine  that point in space from
which the image was obtained.  In response to a RANSAC requirement, we
derive new results on the minimum number of landmarks needed to obtain
a   solution,   and   present    algorithms    for   computing   these
minimum-landmark solutions in closed form.  These results  provide the
basis for an automatic system that can  solve the  LDP under difficult
viewing   and  analysis   conditions.    Implementation  details   and
computational examples are also presented.}
}

@techreport{aic-tn-1980:210,
number=210,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Interpreting Natural-Language Utterances In Dialogs About Tasks",
author={Ann E. Robinson, Douglas E. Appelt, Barbara J. Grosz, Gary G. Hendrix, and Jane J. Robinson},
month="March",
year=1980,
keywords={Natural Language!Dialogue},
abstract={
   This  paper  describes  the  results of  a  three-year
research effort  investigating the knowledge and  processes needed for
participation in natural-language    dialogs   about ongoing
mechanical-assembly  tasks.   Major  concerns   were the    ability to
interpret and respond  to  utterances within the  dynamic  environment
effected by  progress in the   task, as well  as  by  the concommitant
shifting dialog context.

     The research strategy  followed was to  determine  the  kinds  of
knowledge  needed,  to define  formalisms     for  encoding  them  and
procedures for reasoning with them,  to implement those formalisms and
procedures in a computer system called TDUS, and then to test  them by
exercising the system.

     Principal accomplishments include: development of a framework for
encoding knowledge about  linguistic  processes; encoding of a grammar
for  recognizing many  of    the  syntactic structures    of  English;
development of the concept of ``focusing,'' which clarifies a major role
of  context; development of  a formalism   for representing  knowledge
about processes, and procedures  for reasoning about them; development
of   an  overall  framework  for describing  how    different types of
knowledge interact in  the  communication  process; development of   a
computer system that  not only   demonstrates the  feasibility of  the
various formalisms and procedures, but  also provides a  research tool
for testing new hypotheses about the communication process.}
}

@techreport{aic-tn-1980:209,
number=209,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Plan Generation and Execution For Robotics",
author={Earl D. Sacerdoti},
month="April",
year=1980,
keywords={Planning!Generation and execution, Robot!Planning},
abstract={
  Applicability of   existing  industrial robot  systems    is
limited; truly   flexible automation must  make use    of  significant
sensory feedback to respond appropriately to each new stimulus.   This
requires fundamental research in problem solving and the monitoring of
plan execution.  A number of  problems in this  area requiring further
research  are discussed, including  dealing  with  time, planning  for
parallel  execution, planning  for information gathering, planning for
planning,  learning, interactive  planning, dynamic   plan repair, and
distributed robotics.

This paper was prepared for the NSF-sponsored workshop on the research
needed to advance the state of robotics, to be held April 15-17, 1980.}
}

@techreport{aic-tn-1980:206,
number=206,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Interpretation Of Verb Phrases In Dialogs",
author={Ann E. Robinson},
month="January",
year=1980,
keywords={Natural Language!Dialogue},
abstract={
    This  paper discusses two   problems  central to  the
interpretation of  utterances: determining   the relationship  between
actions described in  an utterance  and  events   in   the world,  and
inferring the ``state of the world'' from utterances.   Knowledge of the
language, knowledge about   the general subject  being  discussed, and
knowledge about the current situation are all necessary for this.  The
problem of  determining an action  referred to by  a  verb  phrase   is
analogous  to the problem of determining  the object referred  to by a
noun phrase.

     This paper presents  an approach to the  problems of verb phrases
resolution in which knowledge about language, the  problem domain, and
the dialog itself is combined to interpret such references.  Presented
and  discussed are the  kinds of knowledge  necessary for interpreting
references to actions, as well as algorithms  for using that knowledge
in interpreting dialog utterances about  ongoing tasks and for drawing
inferences  about  the task situation    that are   based  on a  given
interpretation.}
}

@techreport{aic-tn-1980:205,
number=205,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Diagram:  A Grammar For Dialogues",
author={Jane J. Robinson},
month="February",
year=1980,
keywords={DIAGRAM, Natural Language!Dialogue grammar},
abstract={
  This paper presents an explanatory overview  of a large
and complex grammar, DIAGRAM, that is  used in a  computer  system for
interpreting English dialogue.  DIAGRAM  analyzes  all of  the   basic
kinds of phrases  and sentences and many  quite complex  ones as well.
It is not tied to a particular domain  of application, and  it  can be
extended to analyze additional constructions,  using  the formalism in
which it is currently   written.  For every   expression it  analyzes,
DIAGRAM provides an annotated description of  the structural relations
holding  among its  constituents.   The  annotations provide important
information  for  other parts  of   the  system  that  interpret   the
expression in the context of a dialogue.

     DIAGRAM  is  an augmented  phrase  structure grammar.  Its   rule
procedures allow phrases to inherit attributes from their constituents
and  to acquire attributes from   the  larger  phrases  in which  they
themselves are constituents.   Consequently, when these attributes are
used to  set context-sensitive  constraints  on  the acceptance  of an
analysis,  the contextual constraints can  be imposed by conditions on
dominance as well as conditions on constituency.  Rule  procedures can
also assign scores to an analysis, rating some applications of a  rule
as probable or  as unlikely.  Less  likely analyses can be ignored  by
the procedures that interpret the utterance.

     In   assigning categories and  writing  the   rule statements and
procedures for DIAGRAM, decisions were guided by  consideration of the
functions  that  phrases  serve in   communication   as   well  as  by
considerations   of  efficiency  in  relating    syntactic analyses to
propositional   content.   The major  decisions  are   explained   and
illustrated with examples of the rules and the analyses they  provide.
Some contrasts with  transformational grammars  are pointed  out and
problems that motivate a plan to use  redundancy  rules  in the future
are discussed.  (Redundancy   rules  are meta-rules   that  derive new
constituent-structure rules    from a  set    of  base rules,  thereby
achieving generality of syntactic  statement without having to perform
transformations on  syntactic analyses.)    Other extensions of   both
grammar  and formalism   are  projected in  the  concluding   section.
Appendices  provide  details  and  samples of the  lexicon,  the  rule
statements, and the   procedures,  as well  as   analyses  for several
sentences that differ in type and structure.}
}

@techreport{aic-tn-1979:204,
number=204,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Metaphor, Metaphor Schemata, and Selective Inferencing",
author={Jerry R. Hobbs},
month="December",
year=1979,
keywords={Natural Language!Metaphor},
abstract={
  The  importance  of  spatial  and  other metaphors   is
demonstrated.  An approach  to  handling metaphor  in a  computational
framework is described,  based on the  idea  of selective inferencing.
Three examples of  metaphors are examined in detail  in this  light--a
simple metaphor,  a spatial metaphor  schema,  and  a novel  metaphor.
Finally, there is  a   discussion,  from  this   perspective,  of  the
analogical processes that underlie metaphor in this  approach and what
the approach says about several classical questions about metaphor.}
}

@techreport{aic-tn-1979:203,
number=203,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Conversation As Planned Behavior",
author={Jerry R. Hobbs and David A. Evans},
month="December",
year=1979,
keywords={Natural Language!Planning and Conversation},
abstract={
  Perhaps the most  promising  working hypothesis for the
study of conversation is that the participants can be viewed  as using
planning   mechanisms  much     like those   developed  in  artificial
intelligence.  In    this  paper,   a  framework    for  investigating
conversation,  which  for convenience   will  be  called  the Planning
Approach, is developed from this  hypothesis.  It suggests  a style of
analysis  to   apply  to conversation,    analysis   in terms  of  the
participants' goals, plans, and beliefs, and it indicates a consequent
program of research to  be pursued.     These are developed  in detail
in Part 2.

     Parts 3 and  4 are  devoted to  the microanalysis  of  an  actual
free-flowing  conversation,  as   an   illustration  of   the style of
analysis.  In the process, order is discovered in a conversation  that
on the surface seems   quite  incoherent.  The microanalysis  suggests
some   ways in which  the   planning mechanisms  common  in artificial
intelligence will have to be extended to  deal with  conversation, and
these are  discussed in Part  5.   In  Part 6, certain   methodological
difficulties are examined.   Part 7 addresses the  problem that arises
in this approach of what constitutes successful communication.}
}

@techreport{aic-tn-200:1979,
number=200,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Aerial Imagery Using A Multisource Knowledge Integration Technique",
author="M. A. Fischler and J. M. Tenenbaum and H. C. Wolf",
month="October",
year=1979,
keywords={Vision!Aerial Imagery},
abstract={
 This paper describes  a computer-based approach  to the
problem of  detecting and   precisely   delineating roads, and similar
``line-like''  structures,  appearing in low-resolution aerial  imagery.
The  approach  is based  on   a new    paradigm  for   combining local
information from   multiple,  and  possibly  incommensurate,  sources,
including various line and edge  detection   operators, map  knowledge
about the likely path of roads through an image, and generic knowledge
about roads (e.g.,  connectivity,  curvature, and  width  constraints).
The  final interpretation of the scene  is achieved by using either  a
graph search or  dynamic  programming technique  to optimize a  global
figure of merit.  Implementation details and  experimental results are
included.details and  experimental results are
included.}
}

@techreport{aic-tn-1979:199,
number=199,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Theoretical Foundations Of Linguistics and Automatic Text Processing",
author={Jane J. Robinson},
month="October",
year=1979,
keywords={Natural Language!Theoretical foundations},
abstract={
   Texts  are  viewed  as  purposeful  transactions whose
interpretation  requires inferences based  on extra-linguistic as well
as on linguistic information.  Text processors  are viewed as  systems
that  model  both a    theory  of text   and  a theory  of information
processing.  The  interdisciplinary research  required to design  such
systems have a common center, conceptually, in the development of  new
kinds  of lexical  information, since  words are not  only linguistics
objects,  they are also  psychological objects  that evoke experiences
from  which  meanings   can   be inferred.   Recent   developments  in
linguistic theory seem likely to promote more fruitful cooperation and
integration of linguistic research with research on test processing.}
}

@techreport{aic-tn-1979:198,
number=198,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Artificial Intelligence:  State Of The Art",
author={Harry G. Barrow},
month="October",
year=1979,
keywords={Artificial Intelligence},
}

@techreport{aic-tn-1979:196,
number=196,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Map-Guided Interpretation Of Remotely-Sensed Imagery",
author={J. Martin Tenenbaum, Harry  G. Barrow, Robert C. Bolles, Martin A. Fischler,  and Helen C.   Wolf},
month="September",
year=1979,
keywords={Vision!Map guided},
abstract={
  A  map-guided  approach
to interpretation  of  remotely sensed    imagery is described,   with
emphasis  on    applications   involving  continuous   monitoring   of
predetermined ground sites.  Geometric correspondence between a sensed
image and a symbolic reference map is  established in an initial stage
of processing by adjusting parameters of a sensor model  so that image
features predicted from the map optimally match corresponding features
extracted from the sensed image.  Information in the  map is then used
to constrain where to  look  in  an image and what to  look for.  With
such  constraints, previously  intractable  remote  sensing  tasks can
become feasible, even easy,  to automate.  Four  illustrative examples
are given,  involving the   monitoring of reservoirs, roads,  railroad
yards,                          and                         harbors.}
}

@techreport{aic-tn-1979:195,
number=195,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="An Iconic Transform For Sketch  Completion and Shape Abstraction",
author={Martin  A. Fischler   and  Phyllis Barrett},
month="October",
year=1979,
keywords={Vision!Iconic transformation},
abstract={
   This paper shows how a simple  label propagation  technique, in conjunction
with some novel ideas about how labels can  be applied to  an image to
express semantic knowledge, lead to the simplification of  a number of
diverse and difficult  image analysis  tasks (e.g., sketch  completion
and  shape  abstraction).   A  single algorithm  technique,  based  on
skeleton and distance transform concepts, is  applied to appropriately
labeled images to obtain the desired results.  A key point is that the
initial  semantic labeling  is not  required at every  location in the
image, but  only at  those  few critical  locations  where significant
changes          or               discontinuities             occur.}
}

@techreport{aic-tn-1979:191,
number=191,
price="$18.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reasoning About Knowledge and Action",
author={Robert C. Moore},
month="September",
year=1979,
keywords={Reasoning!About knowledge and action},
abstract={
  This thesis deals with the problem of
making a computer reason about the interactions  between knowledge and
action.  In  particular, we  want  to be able  to  reason about   what
knowledge a person must have in  order to  perform an action, and what
knowledge   a person may  gain  by    performing an action.  The first
problem we face in achieving this goal  is that  the basic facts about
knowledge which we need to use are most naturally expressed as a modal
logic.  There are, however, no known techniques for efficiently  doing
automatic deduction directly in  modal logics.  We  solve this problem
by taking the possible-world semantics for a modal  logic of knowledge
and axiomatizing it directly in first-order logic.  This means that we
reason not about  what facts  someone knows, but  rather what possible
worlds are compatible  with what identifying possible worlds  with the
situations before  and after an action  is   performed.  We use  these
notions to  express what knowledge  a person  must  have in  order  to
perform  a given action  and  what   knowledge  a person   acquires by
carrying    out   a  given   action.   Finally,      we consider  some
domain-specific  control  heuristics  that   are   useful  for   doing
deductions in this formalism, and   we  present  several examples   of
deductions     produced    by   applying        these    heuristics.}
}

@techreport{aic-tn-1979:188,
number=188,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Utterance  and  Objective:  Issues  In  Natural Language Communication",
author={Barbara J. Grosz},
month="June",
year=1979,
keywords={Natural Language!Utterance and objective, Multiple Agents},
abstract={
   Communication   in natural language requires  a  combination of
language-specific and general common-sense reasoning  capabilities, the ability
to represent and reason about the beliefs, goals, and plans of multiple agents,
and the recognition that utterances are multifaceted.  This paper evaluates the
capabilities of natural language processing systems against these  requirements
and  identifies crucial  areas  for future   research  in language  processing,
common-sense        reasoning,        and       their           coordination.}
}

@techreport{aic-tn-1979:187,
number=187,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Computational Models Of Beliefs and The Semantics Of  Belief-Sentences",
author={Robert C. Moore and Gary  G. Hendrix},
month="June",
year=1979,
keywords={Beliefs},
abstract={
  This paper considers  a  number of  problems   in   the
semantics of  belief sentences  from the  perspective of computational
models  of   the  psychology of     belief.   We  present a   semantic
interpretation for belief  sentences and show  how this interpretation
overcomes  some of    the  difficulties of    alternative  approaches,
especially those based on possible-world semantics.  Finally, we argue
that these difficulties arise from a mistaken attempt  to identify the
truth conditions of a   sentence with what a competent   speaker knows
about        the       meaning        of       the         sentence.}
}

@techreport{aic-tn-1979:185,
number=185,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Focusing and Description In Natural Language Dialogues",
author={Barbara J. Grosz},
month="April",
year=1979,
keywords={Natural Language!Dialogue, Natural Language!Focusing},
abstract={
  When  two  people talk, they  focus  their attention on
only  a  small portion of what each  of them knows  or believes.  Both
what is   said and   how it   is    interpreted depend on    a  shared
understanding of this narrowing  of  attention to a  small highlighted
portion of what is known.

     Focusing is  an active process.   As a dialogue  progresses,  the
participants continually shift their focus and thus form  an  evolving
context against  which  utterances are  produced  and  understood.   A
speaker provides a hearer  with clues of  what to look at and  how  to
look  at  it--what to focus on, how  to focus on it,  and  how wide or
narrow the focusing should be.   As a  result,  one of the effects  of
understanding an  utterance is that  the listener becomes  focused  on
certain  entities  (both objects and relationships) from  a particular
perspective.

     Focusing clues may be linguistic or they may  come from knowledge
about the relationships between  entities in  the  domain.  Linguistic
clues may be either explicit, deriving directly from certain words, or
implicit,  deriving  from  sentential structure   and from  rhetorical
relationships between sentences.

     This paper  examines  the   relationships  between  focusing  and
definite descriptions in  dialogue  and its implications  for  natural
language processing systems.  It describes  focusing mechanisms  based
on domain structure  clues which  have  been included   in a  computer
system and, from this perspective, indicates  future research problems
entailed   in   modeling   the   focusing  process  more  generally.}
}

@techreport{aic-tn-1979:182,
number=182,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Hierarchical Representation Of Three-Dimensional Objects  Verbal Models",
author={Gerald J. Agin},
month="March",
year=1979,
keywords={Representation!Hierarchical},
abstract={
  We present a formalism for  the computer representation
of three-dimensional  shapes,   that has as  its   goal to  facilitate
man-machine communication  using  verbal, graphic,  and  visual means.
With this method, pieces may be  assembled hierarchically using any of
several  ways  of   specifying  attachment.    The  primitives of  the
representation  are    generalized cylinders,  and  the    creating of
assemblies  may make use   of the  axes  inherent in   the primitives.
Generic models may be  described  that may leave  some  parameters  or
dimensions unspecified, so that when a specific  instance of the model
is described, those parameters may  either be explicitly  specified or
take on default values.  The  axes of  local coordinate frames may  be
given  symbolic  names.    A   set of    computer programs   translate
descriptions of objects into polyhedral models and line drawings.}
}

@techreport{aic-tn-1978:177,
number=177,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Deductive Approach To Program Synthesis",
author={Zohar Manna and Richard Waldinger},
month="December",
year=1978,
keywords={Program Synthesis, Deduction!Program synthesis},
abstract={
   Program synthesis  is the systematic   derivation of a
program from a given specification.  A deductive  approach to  program
synthesis is presented  for   the construction of recursive  programs.
This approach regards program synthesis  as a theorem-proving task and
relies on  a  theorem-proving method  that combines the  features   of
transformation rules, unification, and mathematical induction within a
single framework.}
}

@techreport{aic-tn-1978:176,
number=176,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Why Is Discourse Coherent?",
author={Jerry R. Hobbs},
month="November",
year=1978,
keywords={Natural Language!Discourse},
abstract={
  When people produce  a discourse, what  needs are  they
responding to when  they make it  coherent,  and what  form  does this
coherence take?  In  this paper, it  is argued  that coherence  can be
characterized in  terms  of a  set of  ``coherence   relations'' between
segments of a discourse.  It is shown, from an abstract description of
the discourse situation, that these  relations correspond to the kinds
of   communicative work that needs  to    get done in discourse.    In
particular, four    requirements  for  successful    communication are
isolated: the  message itself must  be  conveyed; the message must  be
related to the goals of the discourse; what  is  new and unpredictable
in the  message  must  be  related  to what  the listener's  inference
processes toward   the    full intended   meaning   of   the  message.
Corresponding to  each requirement is a class   of coherence relations
that   help  the  speaker  satisfy  the  requirements.  The  coherence
relations in each class are  discussed and defined formally.  Finally,
a fragment of a conversation  is analyzed in  detail to illustrate the
problems that face a speaker in trying to  satisfy these requirements,
and to  demonstrate the   role  the coherence relations   play in  the
solution.}
}

@techreport{aic-tn-1978:175,
number=175,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Prospects For Industrial Vision",
author={J. Martin Tenenbaum and Harry G. Barrow and Robert C. Bolles},
month="November",
year=1978,
keywords={Vision!Prospects},
abstract={
  Most current industrial vision systems  are designed to
recognize   known objects seen  from  standard   view points  in  high
contrast scenes.  Their performance and reliability are marginal; many
tasks including such as bin picking,  recognition of parts on overhead
conveyors, and implicit inspection of surface flaws are beyond current
competence.

     Recent image understanding research suggests that the limitations
of    current    industrial  vision  systems   stem   from  inadequate
representations   for   describing    scenes;    physical   attributes
(reflectance, texture, etc.)  and three-dimensional pictorial features
and  object models.  This paper  builds a  case  for needed additional
levels of representation and outlines the design  of a general-purpose
computer-vision system  capable of high performance  in a wide variety
of industrial vision tasks.

     This  paper  was  originally  presented  at  the  General  Motors
Symposium on ``Computer   Vision and  Sensor-Based   Robots,'' September
1978, the proceedings  of which will  be published by  Plenum Press in
1979.}
}

@techreport{aic-tn-1978:173,
number=173,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Scene-Analysis Approach To Remote Sensing",
author={J. Martin Tenenbaum and Martin A. Fischler and Helen C. Wolf},
month="October",
year=1978,
keywords={Vision!Scene analysis},
abstract={
  A scene-analysis approach to interpretation of remotely
sensed imagery is described, with  emphasis  on applications involving
continuous monitoring of predetermined ground sites.  A key concept is
the use of knowledge contained  in various kinds of maps  to guide the
extraction of relevant information from the image.

     Geometric correspondence between a  sensed image and  a  symbolic
map  is  established in an  initial  stage of  processing by adjusting
parameters of a sensor model so that image features predicted from the
optimally  match corresponding   features extracted from   the  sensed
image.  Information in the map is then used to constrain where to look
in an image, what to look for, and how to interpret what is seen.  For
simple monitoring  tasks involving multispectral classification, these
constraints can    significantly      reduce  computation,    simplify
interpretation, and improve the  utility of the resulting information.
Moreover, previously intractable tasks requiring spatial and  textural
analysis may become straightforward in  the context established by the
map   knowledge.  Three  such  tasks  are demonstrated: monitoring the
volume of water in a reservoir, monitoring the number of  boxcars in a
rail yard, and monitoring the number of ships in a harbor.

     The conceptual approach of map-guided image analysis is described
in  sufficient  generality to   suggest  numerous  other applications.
Details   of the map-image  correspondence  procedure  and  a  general
technique for locating  boundaries    to subpixel accuracy  using  map
knowledge are described in appendices.}
}

@techreport{aic-tn-1978:171,
number=171,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Natural Language Access To A Melanoma Data Base",
author={Martin N. Epstein and Donald E. Walker},
month="September",
year=1978,
keywords={Natural Language!Interface},
abstract={
    This paper describes    ongoing    research   towards
developing a  system  that will  allow physicians  personal  access to
patient medical data through natural  language queries to support both
patient management and clinical  research.  A prototype system  has
been  implemented  for a small  data base on malignant  melanoma.  The
physician can input  queries in  English that retrieve specified  data
for particular patients or for  groups of  patients satisfying certain
characteristics, that perform simple calculations, that allow browsing
through the data base, and that assist in identifying relations  among
attributes.  The system supports  dialogue interactions; that  is, the
user can follow a line of inquiry  to test a particular  hypothesis by
entering a sequence of queries that depend on each other.  Classes  of
questions that can be  processed are described  and examples using the
system are given.}
}

@techreport{aic-tn-1979:170,
number=170,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Handling Complex Queries In A Distributed Data Base",
author={Robert C. Moore},
month="October",
year=1979,
keywords={LADDER, Natural Language!Interface},
abstract={
  As  part of the continuing  development of  the  LADDER
system [1] [2], we have substantially expanded the capabilities of the
data base access component  that serves  as the interface  between the
natural-language front  end of LADDER  and the  data base  management
systems on which the data is actually stored.  SODA, the new data base
access  component,  goes beyond its  predecessor IDA [3],  in that  it
accepts  a wider range of  queries and accesses  multiple DBMSs.  This
paper is concerned with the first  of  these areas, and  discusses how
the  expressive power of the  query  language was increased, how these
changes affected query processing in a  distributed data base, as well
as what are some limitations of and planned extensions to the  current
system.}
}

@techreport{aic-tn-1978:169,
number=169,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Why Ask?",
author={Jerry R. Hobbs and Jane J. Robinson},
month="October",
year=1978,
keywords={Natural Language!Appropriateness},
abstract={
  In this paper, we address  the  problem, ``What makes an
answer appropriate?''  We  do so  by investigating indirect answers  to
questions in task-oriented dialogues.  Three cases  are distinguished:
(1) The response, though indirect, answers the question asked; (2) the
response denies a presupposition of the question; and (3) the response
answers to  higher   goals  the questioner   was  trying to   achieve.
Detailed analysis shows  the need for  knowledge about  the task,  the
role of the participants, and communication goals, in the construction
of appropriate answers.  We conclude with a preliminary formulation of
the  appropriateness of  an answer in   terms  of  the  goals  of  the
questioner and the knowledge of the respondent.}
}

@techreport{aic-tn-1978:168,
number=168,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Coherence and Coreference",
author={Jerry R. Hobbs},
month="August",
year=1978,
keywords={Natural Language!Coherence, Natural Language!Coreference},
abstract={
    Coherence  in  conversations and in    texts   can be
partially characterized by  a set  of  coherence relations,  motivated
ultimately  by the  speaker's or writer's need to   be understood.  In
this  paper,  formal  definitions   are  given  for  several coherence
relations, based on  the operations  of  an inference system; that is,
the  relations  between   successive portions   of  a  discourse   are
characterized in terms of the inferences that  can be drawn from each.
In analyzing a discourse, it  is frequently the   case  that we  would
recognize it   as  coherent,  in that it    would satisfy  the  formal
definition of some coherence relation, if only we could assume certain
noun  phrases to be  coreferential.  In  such  cases, we will   simply
assume the  identity of  the  entities referred to,  in what  might be
called  a ``petty conversational   implicature,'' thereby   solving  the
coherence and coreference problems simultaneously.   Three examples of
different kinds of reference  problems are  presented.  In each, it is
shown  how the coherence of  the discourse are  solved,  almost  as  a
by-product, by means of these petty conversational implicatures.}
}

@techreport{aic-tn-1978:165,
number=165,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Investigating The Process Of Natural Language Communication",
author={Ann E. Robinson},
month="June",
year=1978,
keywords={Natural Language!Investigations},
abstract={
  This paper provides an overview  of an ongoing research
program on  natural language communications,  indicating its status as
of   June 1978, and  its short  term goals.   This   research seeks to
identify  and  computationally  formalize the knowledge  and processes
needed  for participation in  natural  language dialogs  about ongoing
tasks.  The paper describes (1) the  knowledge embodied in an existing
system that interprets utterances in such dialogs,  (2) the formalisms
developed for encoding this knowledge, and (3) the framework in  which
the  knowledge is combined  and  coordinated during the interpretation
process.   The paper also indicates  anticipated extensions that  will
lead to refinements of interpretations.   These extensions include the
concept of modality, the use  of the focus and  goals of the dialog in
the identification of  the  referents  of  pronouns, and the   use  of
knowledge  about the goals of  the  other dialog   participants in the
interpretation of utterances.}
}

@techreport{aic-tn-1978:164,
number=164,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Encoding Knowledge In Partitioned Networks",
author={Gary G. Hendrix},
month="June",
year=1978,
keywords={Networks!Partition},
abstract={
  This paper  discusses network notations for  encoding a
number    of  different   kinds  of   knowledge, including   taxonomic
information; general statements  involving quantification; information
about processes and  procedures; the delineation   of local  contexts,
beliefs, and wishes; and the relationships between syntactic units and
their interpretations.

     Many   of the  encodings  appeal   to  the concept  of    network
partitioning,  in which  a large  net is partitioned into subnets  and
higher-order relationships among the subnets are defined.

     Procedural mechanisms  for  constructing and   using the  various
network formalisms   are    discussed as   equal partners with     the
declarative structures.}
}

@techreport{aic-tn-1980:163R,
number=163R,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Ladder User's Guide (Revised)",
author={Earl D. Sacerdoti and Daniel Sagalowicz},
month="March",
year=1980,
keywords={LADDER, Natural Language!Interface},
abstract={
  LADDER (Language Access to  Distributed Data with Error
Recovery) is   a  computer system  designed   to   provide answers  to
questions  posed at  the  terminal in  a  subset  of natural  language
regarding a distributed data  base   of  naval command   and   control
information.      The system  accepts a   fairly  wide   range  of
natural-language questions about the data.  For  each question LADDER
plans a sequence  of appropriate queries to the  data  base management
system, determines on which machine the queries  are  to be processed,
establishes links  to  those machines over the   Arpanet, monitors the
processing  of  the  queries and recovers   from    certain errors  in
execution, and prepares a relevant answer to the original question.

The user's guide is intended for the person who knows how to log in to
the host operating system, as well as how to enter and edit a line of
text.  It does not explain how LADDER works, but rather how to use it
on a demonstration basis.}
}

@techreport{aic-tn-1979:158,
number=158,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Road Tracking and Anomaly Detection In Aerial Imagery",
author={Lynn H. Quam},
month="March",
year=1979,
keywords={Vision!Road tracking},
abstract={
  This report describes a new procedure for tracking road
segments and finding potential vehicles in imagery  of approximately 1
to 3 feet per pixel ground resolution.  This work is part of a  larger
effort by SRI International to construct an image understanding system
for monitoring roads in aerial imagery.}
}

@techreport{aic-tn-1978:157,
number=157,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Recovering Intrinsic Scene Characteristics From Images",
author={Harry G. Barrow and J. Martin Tenenbaum},
month="April",
year=1978,
keywords={Vision!Intrinsic characteristics},
abstract={
  We suggest  that  an appropriate role  of  early visual
processing is to  describe a scene in terms   of intrinsic  (vertical)
characteristics--such as range, orientation, reflectance, and incident
illumination--of  the surface element  visible at  each point  in  the
image.  Support for  this  idea comes from  three sources: the obvious
utility of intrinsic characteristics for higher-level scene  analysis;
the  apparent  ability of humans   to determine these characteristics,
regardless of viewing conditions or familiarity with  the scene; and a
theoretical argument  that   such a description  is  obtainable,  by a
noncognitive and  nonpurposive process,  at  least, for   simple scene
domains.  The central  problem in recovering   intrinsic value encodes
all the characteristics of  the corresponding scene   point.  Recovery
depends on exploiting constraints, derived from  assumptions about the
nature of the scene and the physics of the imaging process.}
}

@techreport{aic-tn-1977:156,
number=156,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Synthesis:  Dreams ==      Programs",
author={Richard Waldinger and Zohar Manna},
month="November",
year=1977,
keywords={Program Synthesis, Deduction!Program Synthesis},
abstract={
  Deductive   techniques are    presented   for  deriving
programs systematically from given specifications.  The specifications
express the purpose of the desired program without giving any  hint of
the algorithm to be employed.  The basic approach is to  transform the
specifications   repeatedly according   to  certain  rules,  until   a
satisfactory  program  is  produced.  These   techniques   have   been
incorporated in a running program-synthesis system, called DEDALUS.

     Many of the  transformation rules represent  knowledge  about the
program's subject domain (e.g., numbers, lists,  sets); some represent
the meaning of the  constructs of the specification  language and  the
target   programming  language;   and  a  few   rules  represent basic
programming  principles.  Two  of these  principles,  the conditional-formation
rule  and the  recursion-formation rule,  account   for  the
introduction of  conditional expressions and  of  recursive calls into
the synthesized program.  The  termination of the programs is  ensured
as new recursive calls are formed.

     Two  extensions of the recursion-formation rule  are discussed; a
procedure-formation rule, which  admits the introduction  of auxiliary
subroutines in the course of the synthesis process,  and a
generalization rule, which causes the specifications to be  altered to
represent a more general problem that is  nevertheless easier to solve.  Special
techniques  are  introduced for  the formation of programs   with side
effects.

     The techniques of this paper are illustrated with a sequence of
examples of increasing complexity; programs are constructed for list
processing, numerical calculation, and array computation.

     The  methods  of program   synthesis  can be applied   to various
aspects  of programming   methodology--program  transformation,   data
abstraction, program modification, and structured programming.

     The    DEDALUS system  accepts  specifications  expressed  in   a
high-level language, including  set notation,  logical quantification,
and a  rich vocabulary drawn from a  variety of subject domains.   The
system  attempts  to transform  the specifications   into a recursive,
LISP-like target    program.    Over   one hundred    rules  have been
implemented, each expressed as a small program in a QLISP language.}
}

@techreport{aic-tn-1977:155,
number=155,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Prospector--A Computer Based Consultation System For Mineral Exploration",
author={Peter E. Hart and Richard O. Duda},
month="October",
year=1977,
keywords={Prospector, Expert Systems},
abstract={
  This   paper reviews the    principles  and  status  of
Prospector,  a    computer-based  consultation program   for   mineral
exploration.  The  mechanisms for representing  ore deposit models  by
networks of inference rules are described, and the overall approach is
compared to alternative decision making methodologies.}
}

@techreport{aic-tn-1977:154,
number=154,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Logic Of Computer Programming",
author={Richard Waldinger and Zohar Manna},
month="August",
year=1977,
keywords={Program Synthesis},
abstract={
  Techniques derived from  mathematical logic promise  to
provide  an   alternative   to   the  conventional   methodology   for
constructing,     debugging,   and    optimizing    computer programs.
Ultimately, these techniques are intended to lead to the automation of
many of the facets of the programming process.

     This paper provides a unified  tutorial exposition of the logical
techniques,  illustrating  each with    examples.   The strengths  and
limitations of  each technique  as  a   practical programming  aid are
assessed and attempts  to implement  these  methods  in   experimental
systems are discussed.}
}

@techreport{aic-tn-1977:152,
number=152,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Developing A Natural Language Interface To Complex Data",
author={Gary G. Hendrix, Earl D. Sacerdoti, Daniel Sagalowicz, and Jonathan Slocum},
month="August",
year=1977,
keywords={Natural Language!Interface},
abstract={
   This   paper describes  aspects  of   an  intelligence
interface  that provides natural language access  to a  large body  of
data  distributed over  a   computer  network.    The overall   system
architecture is presented,  showing how a user is   buffered  from the
actual data base management   systems  (DBMSs) by   three   layers  of
insulating components.  These layers  operate   in  series to  convert
natural  language queries   into  calls   to  DBMSs   at remote sites.
Attention is then focused  on the  first of the insulating components,
the natural language system.  A pragmatic approach  to language access
that  has   proved useful for  building interfaces  to  data  bases is
described and illustrated by examples.  Special language features that
increase system usability, such as spelling correction, processing  of
incomplete   inputs, and  run-time  system  personalization, are  also
discussed.}
}

@techreport{aic-tn-1977:151,
number=151,
price="$20.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="The Representation and Use Of Focus In Dialogue",
author={Barbara J. Grosz},
month="July",
year=1977,
keywords={Natural Language!Focus, Natural Language!Dialogue},
abstract={
  This report develops a representation of focus of attention that
circumscribes discourse contexts within a general representation of knowledge.
Focus of attention is essential to any comprehension process because what and
how a person understands is strongly influenced by where his attention is
directed at a given moment.  To formalize the notion of focus, the need for and
the use of focus mechanisms are considered from the standpoint of building a
computer system that can participate in a natural language dialogue with a
user.  Two ranges of focus, global and immediate, are investigated, and
representations for incorporating them in a computer system as developed.

     The global  focus  in    which an   utterance  is interpreted  is
determined by  the total  discourse   and situational setting   of the
utterance.  It influences what is talked about, how different concepts
are introduced, and  how concepts are referenced.    To  encode global
focus computationally,  a representation is  developed that highlights
those items that are relevant  at a given  place in  a dialogue.   The
underlying knowledge representation is segmented into subunits, called
focus spaces,  that contain those items that   are in  the  focus   of
attention of a dialogue participant during  a  particular part of  the
dialogue.

     Mechanisms are required  for updating the  focus  representation,
because,  as a  dialogue progresses, the  objects and actions that are
relevant to the conversation, and therefore in the participants' focus
of attention, change.  Procedures are  described for deciding when and
how to shift focus  in task-oriented dialogues,  i.e., in dialogues in
which  the participants  are  cooperating   in a shared   task.  These
procedures are guided by a representation of the task being performed.

     The ability to represent focus of attention in  a language understanding
system results in a  new approach to an  important problem in
discourse comprehension--the  identification  of    the  referents  of
definite  noun phrases.   Procedures   for identifying  referents  are
developed  that   take discourse structure   into account and  use the
distinction   between  highlighted  items  and those     that  are not
highlighted to  constrain the search  for the referent  of a  definite
noun phrase.

     Interpretation of  an   utterance also depends on  the  immediate
focus  established by the linguistic  form of the preceding utterance.
The  interpretation of elliptical  sentence  fragments illustrates the
effect  of  immediate  focus.   Procedures  that interpret  elliptical
sentence  fragments are developed.  The  use of a representation  that
superimposes  syntactic  information   about    an  utterance   on the
interpretation of the underlying meaning of that utterance to minimize
the processing required to expand a fragment into a complete sentence.}
}

@techreport{aic-tn-1977:148,
number=148,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Production System For Automatic Deduction",
author={Nils J. Nilsson},
month="July",
year=1977,
keywords={Deduction!Production system},
abstract={
  A new predicate calculus deduction system based on production
rules is proposed.  The system combines several developments in Artificial
Intelligence and Automatic Theorem Proving research including the use of
domain-specific inference rules and separate mechanisms for forward and
backward reasoning.  It has a clean separation between the data base, the
production rules, and the control system. Goals and subgoals are maintained in
an AND/OR tree structure.  We introduce here a structure that is the dual of
the AND/OR tree to represent assertions.  The production rules modify these
structures until they ``connect'' in a  fashion that proves the goal theorem.
Unlike some previous systems that used production rules, ours is not limited to
rules in Horn Clause form.  Unlike previous PLANNER-like systems, ours can
handle the full range of predicate calculus expressions including those with
quantified variables, disjunctions and negations.}
}

@techreport{aic-tn-1977:147,
number=147,
price="$5.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Network-Based Knowledge Representation and Its Natural  Systems",
author={Gary G. Hendrix and Richard Fikes},
month="July",
year=1977,
keywords={K-NET, SNIFFER, Deduction!Network based, Knowledge Representation!Network based},
abstract={
  We describe a knowledge representation scheme called
K-NET and a problem solving system called SNIFFER designed to answer
queries using a K-NET knowledge base.  K-NET uses a partitioned
semantic net to combine the expressive capabilities of the first-order
predicate calculus with linkage to procedural knowledge and with full
indexing of objects to the relationships in which they participate.
Facilities are also included for representing taxonomies of sets and for
maintaining hierarchies of contexts.  SNIFFER is a manager and
coordinator of deductive and problem-solving processes.  The basic
system includes a logically complete set of natural deduction
facilities that do not require statements to be converted into clause
or prenex normal form.  Using SNIFFER's coroutine-based control
structure, alternative proofs may be constructed in pseudo-parallel
and results shared among them.  In addition, SNIFFER can also manage
application of specialist procedures that have specific knowledge
about a particular domain or about the topology of the K-NET
structures.  For example, specialist procedures are used to manipulate
taxonomic information and to link the system to information in
external data bases.}
}

@techreport{aic-tn-1977:145,
number=145,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Ida",
author={Daniel Sagalowicz},
month="June",
year=1977,
keywords={IDA, Natural Language!Interface},
abstract={
  IDA was  developed at  SRI  to  allow a casual  user to
retrieve information from a  data base, knowing the  fields present in
the data base, but not the structure of the  data base itself.  IDA is
part  of a system  that  allows  the user to   express queries  in   a
restricted subset  of English, about  a  data  base  of fourteen files
stored  on CCA's Datacomputer.   IDA's input is a  very simple, formal
query language which is essentially  a list of restrictions  on fields
and queries about fields, with no mention of the structure of the data
base.   It produces a  series of DBMS  queries, which are  transmitted
over the ARPA network.  The results  of these queries are combined  by
IDA to provide the answer to the user's query.  In this paper, we will
define  the input language, and  give examples of  IDA's behavior.  We
will also present our representation of the ``structural schema,'' which
is the information needed by IDA to know how the data base is actually
organized.  We will give an idea  of some of the  heuristics which are
used to produce a  program  in the language of the  DBMS.  Finally, we
will discuss  the limitations  of  this  approach,  as  well as future
research areas.}
}

@techreport{aic-tn-1977:142,
number=142,
price="$18.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Framework For Speech Understanding",
author={William H. Paxton},
month="June",
year=1977,
keywords={Speech Understanding},
abstract={
  This paper reports the author's  results in  designing,
implementing,  and testing  a   framework for a   speech-understanding
system.  The work  was done  as  part  of  a multi-disciplinary effort
based  on   state-of-the-art  advances in  computational  linguistics,
artificial intelligence, systems programming, and speech science.  The
overall project goal was to develop one or  more computer systems that
would recognize continuous speech uttered in the context or some well-specified
task  by making extensive use of  grammatical, semantic, and
and contextual  constraints.   We  call  a  system   emphasizing  such
linguistic constraints a `speech-understanding system' to  distinguish
it from speech-recognition systems which rely  on acoustic information
alone.

     Two major  aspects of a  framework for speech   understanding are
integration  of the process of forming   a unified  system  out of the
collection of  components--and control--the  dynamic  direction of the
overall  activity  of  the system  during the processing   of an input
utterance.  Our method of system  integration  gives a central role to
the input-language definition,  which  is  based on  augmented phrase-structure  
rules.  A rule  consists of  a phrase-structure declaration
which specifies the  possible for computing 'attributes' and `factors.'
Attribute statements determine the  properties  of  particular phrases
constructed by  the  rule;  factor    statements make    acceptability
judgments on  phrases.  Together these statements   contain specifications 
for most of the potential interactions among system components.

     Our  approach to system  control centers on a  system `Executive'
applying  the rules of  the language  definition organizing hypotheses
and results, and assigning priorities.  Phrases  with their attributes
and factors are the basic entities manipulated by the Executive, which
takes on  the role of a parser  in   carrying out its  integration and
control functions.  The Executive controls the overall activity of the
system by setting priorities on the basis of acoustics  and linguistic
acceptability  judgments.  These data are  combined to form scores and
ratings.  A phrase score reflects  a quality judgment  independent  of
the phrase's context and gives useful local information concerning the
sentential context.   To   get  early and  efficient   access to   the
contextual information, we have developed a technique for  calculating
phrase ratings by a heuristic  search of possible interpretations that
would use   the  phrase.   One  of our  experiments   shows  that this
context-checking method results  in significant improvements in system
performance.}
}

@techreport{aic-tn-1977:140,
number=140,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Language Access To Distributed Data With Error Recovery",
author={Earl D. Sacerdoti},
month="April",
year=1977,
keywords={Natural Language!Interface},
abstract={
  This paper  discusses an effort  in the application  of
artificial   intelligence  to the access   of  data    from  a  large,
distributed data  base over a  computer network.  A  running system is
described that provides  access to multiple  instances of a  data base
management system over the ARPANET in real time.  The system accepts a
rather wide range of  appropriate queries to  the data base management
system  to answer the question, determines  on which machine to  carry
out the queries, establishes links to those machines over the ARPANET,
monitors  the prosecution of the   queries and  recovers from  certain
errors in  execution, and prepares a relevant   answer to the original
question.  In addition to  the functional components that make  up the
demonstration  system, equivalent  functional  components with  higher
levels of sophistication are discussed and proposed.}
}

@techreport{aic-tn-1977:139,
number=139,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Human Engineering For Applied Natural Language Processing",
author={Gary G. Hendrix},
month="March",
year=1977,
keywords={LIFER, Natural Language!Interface},
abstract={
  Human engineering features for  enhancing the usability
of practical  natural language systems are described.    Such features
include  spelling  correction, processing  of incomplete  (elliptical)
inputs, interrogation  of the  underlying  language definition through
English  queries, and  an  ability for  casual users  to  extend   the
language accepted   by  the system through  the  use  of  synonyms and
paraphrases.  All of the features described are incorporated in LIFER,
an   applications-oriented  system   for   creating  natural  language
interfaces between  computer programs    and casual   users.   LIFER's
methods for realizing the more complex human engineering  features are
presented.}
}

@techreport{aic-tn-1977:138,
number=138,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Lifer Manual:  A Guide To Building Practical Natural Language Interface",
author={Gary G. Hendrix},
month="February",
year=1977,
keywords={LIFER, Natural Language!Interface},
abstract={
  This document describes  an application-oriented system
for  creating natural language  interfaces between existing   computer
programs (such as data base management systems) and casual users.  The
system is easy to use and flexible,  offering  a range of capabilities
that  support both  simple and  complex interfaces.    This  range  of
capabilities allows  beginning  interface builders to   rapidly define
worktable sublets of English and gives more  advanced language
definitions.  The  system  includes  an automatic    mechanism for  handling
certain   classes  of elliptical  (incomplete)  inputs,    a  spelling
corrector, a grammar editor, and a mechanism that allows even novices,
through the use of  paraphrase, to extend  the language recognized  by
the system.   Experience with the  system has   shown  that  for  many
applications,  very practicable interfaces may  be  created  in a  few
days.}
}

@techreport{aic-tn-1977:137,
number=137,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Interactive Aids For Cartography and Photo Interpretation",
author={Harry Barrow and Thomas Garvey and Jan Kremers and J. Martin Tenenbaum and Helen C. Wolf},
month="January",
year=1977,
keywords={Vision!Cartographic aids},
abstract={
  This report covers the six-month period October 1975 to
April 1976.  In this report, the application areas  of  ARPA-supported
Machine   Vision   work at    SRI  were  changed    to Cartography and
Photointerpretation.   This change entailed general    familiarization
with the new domains, exploration of their current practices and uses,
and  determination  of    outstanding  problems.  In  addition,   some
preliminary tool-building and experimentation have been performed with
a view  to determining feasibility  of various  AI  approaches to  the
identified  problems.   The   work of  this   period resulted  in  the
production and  submission to  ARPA of  a  proposal for  research into
Interactive Aids for Cartography and Photointerpretation.  This report
will not reiterate in detail  the content  of  the  proposal, but will
refer the reader to it for further information.}
}

@techreport{aic-tn-1977:136,
number=136,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Semantic Network Representation In Rule Based Inference System",
author={Richard O. Duda},
month="January",
year=1977,
keywords={Semantic Nets, Knowledge Representation!Semantic Nets, Rule-based Inference Systems},
abstract={
  Rule-based inference systems allow judgmental knowledge
about a specific problem domain  to be  represented as a collection of
discrete rules.  Each rule states that if  certain premises are known,
then certain conclusions can  be inferred.  An  important design issue
concerns the representational form for the premises and conclusions of
the rules.   We describe a  rule-based system that  uses a partitioned
semantic network representation for the premises and conclusions.

     Several advantages can be cited for the semantic network representation.
The most important of these concern the ability to represent subset and element
taxonomic information, the ability to include the potential for smooth
interface with natural language subsystems.  This representation is being used
in a system currently under development at SRI to aid a geologist in the
evaluation of the mineral potential of exploration sites.  The principles
behind this system and its current implementation are described in the paper.}
}

@techreport{aic-tn-1976:134,
number=134,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Experiments In Speech Understanding System Control",
author={William H. Paxton},
month="August",
year=1976,
keywords={Speech Understanding},
abstract={
  A series of experiments was performed concerning control
strategies for a speech understanding system.  The main experiment tested the
effects on performance of four major choices: focus attention by inhibition or
use an unbiased best-first method, ``island-drive'' or process left or right,
use context checks in priority setting or do not, and map words all at once or
map only as called for.  Each combination of choices was tested with 60
simulated utterances of lengths varying from 0.8 to 2.3 seconds.  The results
include analysis of the effects and interactions of the design choices with
respect to aspects of system performance such as overall sentence accuracy,
processing time, and storage.  Other experiments include tests of acoustic
processing performance and a study of the effects of increased vocabulary and
improved acoustic accuracy.}
}

@techreport{aic-tn-1976:132,
number=132,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Is ``Sometime'' Sometimes Better Than ``Always?'' Intermittent, Assertion In Proving Program Correctness",
author={Zohar Manna and Richard Waldinger},
month="June",
year=1976,
keywords={Program correctness},
abstract={
  This paper explores a technique for proving the correctness and
termination of programs simultaneously.  This approach, which we call the
intermittent-assertion method, involves documenting the program with assertions
that must be true at some time when control is passing through the
corresponding point, but that need not be true every time.  The method,
introduced by Knuth and further developed by Burstall, promises to provide a
valuable complement to the more conventional methods.

     We first introduce and illustrate the technique with a number of examples.
We then show that a correctness proof using the invariant assertion method or
the subgoal induction method can always be expressed using intermittent
assertions instead, but that the reverse is not always the case.  The method
can also be used just to prove termination, and any proof of termination using
the conventional well-founded sets approach can be rephrased as a proof using
intermittent assertions.  Finally, we show how the method can be applied to
prove the validity of program transformations and the correctness of
continuously operating programs.}
}

@techreport{aic-tn-1976:127,
number=127,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Application Of Interactive Scene Analysis Techniques To Cartography",
author={Thomas D. Garvey and J. Martin Tenenbaum},
month="September",
year=1976,
keywords={Vision!Cartography, Vision!Interactive scene analysis},
abstract={
  One  of  the most  time-consuming  and  labor-intensive
steps in map production involves the delineation  of  cartographic and
cultural features such as   lakes, rivers, roads,   and drainages   in
aerial photographs.  These features are usually  traced manually on  a
digitizing table  in  painstaking detail.  This  paper investigates an
alternative approach, an  interactive graphically designates a feature
of  interest  by pointing at  or   crudely tracing it  with a  display
cursor.  Using this input as a guide, the system employs context-dependent,
scene-analysis techniques to extract a  detailed outline of
the feature.  The    results are  displayed  so  that  errors   can be
corrected by further  interaction,  for  example, by  tracing    small
sections of the boundary in detail.  This interactive approach appears
applicable to many other problem domains involving large quantities of
graphic or pictorial data, which are difficult  to extract in  digital
form by either strictly manual or strictly automatic means.}
}

@techreport{aic-tn-1976:124,
number=124,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Subjective Bayesian Methods For Rule-Based Inference Systems",
author={Richard O. Duda and Peter E. Hart and Nils J. Nilsson},
month="January",
year=1976,
keywords={Reasoning!Bayesian, Rule-based Inference Systems},
abstract={
  The general   problem   of  drawing  inferences    from
uncertain or  incomplete evidence has invited a  variety of  technical
approaches, some mathematically rigorous and some largely informal and
intuitive.  Most current inference systems  in artificial intelligence
have emphasized  intuitive methods,  because the  absence  of adequate
statistical  samples forces a  reliance on the subjective judgment  of
human  experts.  We  describe   in this paper  a subjective   Bayesian
inference method that realizes  some of the advantages  of both formal
and informal approaches.  Of particular interest are the modifications
needed to deal with  the inconsistencies usually  found in collections
of subjective statements.}
}

@techreport{aic-tn-1976:123,
number=123,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Experiments In Interpretation-Guided Segmentation",
author={J. Martin Tenenbaum and Harry G. Barrow },
month="March",
year=1976,
keywords={Vision!Segmentation},
abstract={
  This paper presents a new approach for integrating the
segmentation and interpretation phases of scene analysis.  Knowledge from a
variety of sources is used to make inferences about the interpretations of
regions, and regions are merged in accordance with their possible
interpretations.

     The deduction of region interpretations is performed using a
generalization of Waltz's filtering algorithm.  Deduction proceeds by
eliminating possible region interpretations that are not consistent with any
possible interpretation of an adjacent region.  Different sources of knowledge
are expressed uniformly as constraints on the possible interpretations of
regions.  Multiple sources of knowledge can thus be combined in a
straightforward way such that incremental additions of knowledge (or
equivalently, human guidance) will effect incremental improvements in
performance.

     Experimental results are reported in three scene domains, landscapes,
mechanical equipment, and rooms, using, respectively, a human collaborator, a
geometric model and a set of relational constraints as sources of knowledge.
These experiments demonstrate that segmentation is much improved when
integrated computational overhead over unguided segmentation.

     Applications of the approach in cartography, photointerpretation, vehicle
guidance, medicine, and motion picture analysis are suggested.}
}

@techreport{aic-tn-1976:121,
number=121,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Msys:  A System For Reasoning About Scenes",
author={Harry G. Barrow and J. Martin Tenenbaum},
month="April",
year=1976,
keywords={MSYS, Reasoning!Uncertain, Vision!Scene reasoning},
abstract={
  MSYS is a system for reasoning with uncertain information and
inexact rules of inference.  Its major application, to date, has been to the
interpretation of visual features (such as regions) in scene analysis.  In this
application, features are assigned sets of possible interpretations with
associated likelihoods based on local attributes (e.g., color, size, and
shape).  Interpretations are related by rules of inference that adjust the
likelihoods up or down in accordance with interpretation likelihoods of related
features.  An asynchronous relaxation process repeatedly applies the rules
until a consistent set of likelihood values is attained.  At this point,
several alternative interpretations still exist for each feature.  One feature
is chosen and the most likely of its alternatives is assumed.  the rules are
then used in this more precise context to determine likelihoods for the
interpretations of remaining features by a further round of relaxation.  The
selection and relaxation steps are repeated until all features have been
interpreted.

     Some interpretation typifies constraint optimization problems involving
the assignment of values to a set of mutually constrained variables.  For an
interesting class of constraints, MSYS is guaranteed to find the optimal
solution with less branching than conventional heuristic search methods.

     MSYS is implemented as a network of asynchronous parallel processes.  The
implementation provides an effective way of using data driven systems with
distributed control for optimal stochastic search.}
}

@techreport{aic-tn-1976:120,
number=120,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Qlisp: A Language For The Interactive Development Of Complex Systems",
author={Earl Sacerdoti and Richard Fikes and Rene Reboh and Daniel Sagalowicz and Richard Waldinger and B. Michael Wilber},
month="March",
year=1976,
keywords={QLISP, QA4},
abstract={
  This paper presents a functional overview of the features and
capabilities of QLISP, one of the newest of the current generation of very high
level languages developed for use in artificial intelligence (AI) research.

     QLISP is both a programming language and an interactive programming
environment.  It embeds an extended version of QA4, an earlier AI language, in
INTERLISP, a widely available version of LISP with a variety of sophisticated
programming aids.

     The language features provided by QLISP include a variety of useful data
types, an associative data base for the storage and retrieval of expressions, a
powerful pattern matcher based on a unification algorithm, pattern-directed
function invocation, ``teams'' of pattern involved functions, a sophisticated
mechanism for breaking a data base into contexts, generators for associative
data retrieval, and easy extensibility.  System features available in QLISP
include a very smooth interaction with the underlying INTERLISP language, a
facility for aggregating multiple pattern matches, and features for interactive
control of programs.

     A number of the implemented applications of QLISP are briefly discussed,
and some directions for future development are presented.}
}

@techreport{aic-tn-1976:117,
number=117,
price="$20.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Perceptual Strategies For Purposive Vision",
author={Thomas D. Garvey},
month="September",
year=1976,
keywords={Vision!Purposive},
abstract={
  This report describes a computer program that approaches
perception as a problem-solving task.  The system uses information about the
appearances of objects, about their interrelationships, and about available
sensors to produce a plan for locating specified objects in images of room
scenes.  The strategies produced allow for cost-effective processing by
utilizing ``cheap'' features in the early stages, reserving more complex
operations for later in the process, when the content has been sufficiently
restricted.

     The general strategy paradigm used by the system is to acquire image
samples expected to belong to the target object; validate the hypothesis that
the acquired samples do belong to the target; and finally to bound the image of
the object by outlining it in a display picture.

     Sensors used by the system include a vidicon with three color filters and
a master-scanned, laser-rangefinder capable of producing a ``range image.''  In
addition, the range-finder measures the reflectivity at the laser wavelength,
at each post, producing a gray-scale image in perfect registration with the
range image.  The primitive attributes of brightness, line, saturation, height,
and local surface orientation at specified range locations can be computed from
these images.  These attributes represent the new data available to the system
for recognizing and identifying objects.  Object descriptions are provided
initially by indicating the object to the system, and allowing the system to
measure attributes.  Other data, such as typical object relationships, are
provided interactively by the user.

     When required to locate an object, the system computes a distinguishing
features representation that will serve to separate parts of the target from
those of other objects.  These distinguishing features are combined into a
strategy represented as a planning graph.  This graph contains optimal subgoals
for achieving the goal of locating the object. An execution routine selects the
``best'' subgoal, executes it, rates its effect, and selects the next best
goal, continuing with the progress until either the object is located, or there
are no options remaining.

     This approach offers several contributions to perception research.  By
capitalizing on the goal-directed aspects of the problem, the system is able to
select relevant information from a mass of irrelevant data.  The system is able
to organize its processing in such a way as to optimize the use of sensor data.
By generating strategies when needed, the program allows for the easy
introduction of new objects and new sensors.  The system allows for the logical
introduction of new information deriving algorithms.}
}

@techreport{aic-tn-1975:113,
number=113,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Semantic Processing For Speech Understanding",
author={Gary G. Hendrix},
month="September",
year=1975,
keywords={Speech Understanding!Semantic processing},
abstract={
  The semantic component of the speech understanding system being
developed jointly by SRI and SDC rules out phrase combinations that are not
meaningful and produces semantic interpretations for combination that are.  The
system consists of a semantic network model and routines that interact with it.
The net is partitioned into a set of hierarchically ordered subnets,
facilitating the encoding of higher-order predicates and the maintenance of
multiple parsing hypotheses.  Composition routines, combining utterance
components into phrases, consult network descriptions of prototype situations
and surface-to-deep-case maps.  Outputs from these routines are network
fragments consisting of several subnets that in aggregate capture the
interrelationships between a phrase's syntax and semantics.}
}

@techreport{aic-tn-1975:112,
number=112,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Tuneable Performance Grammar",
author={Jane J. Robinson},
month="September",
year=1975,
keywords={Speech Understanding!Tuneable performance grammar},
abstract={
  This paper describes a tuneable performance grammar currently
being developed for speech understanding.  It shows how attributes of words are
defined and propagated to successively larger phrases, how other attributes are
acquired, how ``factors'' reference them to help the parser choose among
competing definitions in order to interpret the utterance correctly, and how
these factors can easily be changed to adapt the grammar to other discourses
and contexts. Factors that might be classified as ``syntactic'' are emphasized,
but the attributes they reference need not be, and seldom are, purely
syntactic.}
}

@techreport{aic-tn-1975:109,
number=109,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="A Structure For Plans and Behavior",
author={Earl D. Sacerdoti},
month="August",
year=1975,
keywords={NOAH, Planning!Structure, Procedural nets},
abstract={
  This report describes progress that has been made in the ability
of a computer system to understand and reason actions.  A new method for
representing actions within a computer memory has been developed, and this new
representation, called the procedural net, has been employed in developing new
strategies for solving problems and monitoring the execution of the resulting
solutions.

     A set of running computer programs, called the NOAH (Nets of Action
Hierarchies) systems, embodies the representation and strategies discussed
above.  Its major goal is to provide a framework for storing expertise about
the actions of a particular task domain and to impart that expertise to a human
in the cooperative achievement of nontrivial tasks.

     A problem is presented to NOAH as a statement that is to be made true by
applying a sequence of actions in an initial state of the world.  The actions
are drawn from a set of actions previously defined to the system.  NOAH first
creates a one-step solution to the problem (essentially ``Solve the given
goal'').  Then it progressively expands the level of detail of the solution,
filling in ever more detailed actions.  All the individual actions, composed
into plans at differing levels of detail, are stored in a data structure called
the procedural net.  The system avoids imposing unnecessary constraints on the
order of the actions, rather than as linear sequences.

     The same data structure is used to guide the human user through a task.
Since the system has planned the task at varying levels of detail, it can issue
requests for action to the user at varying levels of detail, depending on his
competence and understanding of the higher-level actions.  If more detail
should be needed than was originally planned for, or if an unexpected event
causes the plan to go awry, the system can continue to plan from any point
during execution.

The key ideas that are explored include:

\begin{enumerate}

\item Planning at many levels of detail

\item  Representing a plan as a partial ordering of actions with respect to
     time.

\item  Execution monitoring and error recovery using hierarchical plans.

\item Using procedures that represent a task domain to generate
declarative (frame-like) structures that represent individual actions.

\item Using abstract actions to model iterative operators.

\end{enumerate}

     The major point of the report is that the structure of a plan of actions
is as important for problem solving and execution monitoring as the nature of
the actions themselves.}
}

@techreport{aic-tn-1975:107,
number=107,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Achieving Several Goals Simultaneously",
author={Richard Waldinger},
month="July",
year=1975,
keywords={Planning},
abstract={
  In the synthesis of a plan or computer program, the problem of
achieving several goals simultaneously presents special difficulties, since a
plan to achieve one goal may interfere with attaining the others.  This paper
develops the following strategy: to achieve two goals simultaneously, develop a
plan to achieve one of them and then modify that plan to achieve the second as
well.  A systematic program modification technique is presented to support this
strategy.  The technique requires the introduction of a special ``skeleton
model'' to represent a changing world that can accommodate modifications in the
plan.  This skeleton model also provides a novel approach to the ``frame
problem.''

     The strategy is illustrated by its application to three examples.  Two
examples involve synthesizing the following programs: interchanging the values
of two variables and sorting three variables.  The third entails formulating a
tricky blocks world plan.  The strategy has been implemented in a simple QLISP
program.

     It is argued that skeleton modeling is valuable as a planning technique
apart from its use in plan modification, particularly because it facilitates
the representation of ``influential actions'' whose effects may be far
reaching.

     The second part of the paper is a critical survey of contemporary planning
literature, which compares our approach with other techniques for facing the
same problems.}
}

@techreport{aic-tn-1974:98,
number=98,
price="$10.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Knowledge and Reasoning In Program Synthesis",
author={Zohar Manna and R. J. Waldinger},
month="November",
year=1974,
keywords={Program Synthesis},
abstract={
  Program synthesis is the construction of a computer program from
given specifications.  An automatic program synthesis system must combine
reasoning and programming ability with a good deal of knowledge about the
subject matter of the program.  This ability and knowledge must be manifested
both procedurally (by programs) and structurally (by choice of representation).

     We describe some of the reasoning and programming capabilities of a
projected synthesis system.  Special attention is paid to the introduction of
conditional tests, loops, and instructions with wide effects in the program
being constructed.  The ability to satisfy several interacting goals
simultaneously proves to be important in many contexts.  The modification of an
already existing program to solve a somewhat different problem has been found
to be a powerful approach.

     We illustrate these concepts with hand simulations of the synthesis of a
number of pattern-matching programs.  Some of these techniques have already
been implemented, some are in the course of implementation, while others seem
equivalent to well-known unsolved problems in artificial intelligence.}
}

@techreport{aic-tn-1973:86,
number=86,
price="$15.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Reasoning About Programs",
author={R. J. Waldinger and K. N. Levitt},
month="October",
year=1973,
keywords={Program verification},
abstract={
  This paper  describes a theorem prover that   embodies knowledge
about programming constructs, such as numbers,  arrays, lists, and expressions.
The program can  reason about these concepts  and is used as  part of a program
verification system that uses the Floyd-Naur explication of  program semantics.
It is implemented  in the  QA4 language; the  QA4 system allows many pieces  of
strategic knowledge, each  expressed as a  small program, to be coordinated  so
that a program stands forward when it is relevant to the problem  at hand.  The
language allows clear, concise representation  of this sort  of knowledge.  The
QA4 system also has special facilities for dealing with  commutative functions,
ordering relations, and equivalence relations; these features are  heavily used
in this deductive system.   The  program interrogates the  user  and  asks  his
advice in the course of  a proof.   Verifications have  been found for  Hoare's
FIND program a real-number division algorithm, and some sort  programs, as well
as for many simpler algorithms.   Additional theorems have  been proved about a
pattern  matcher  and a  version  of   Robinson's  unification algorithm.   The
appendix contains  a complete, annotated  listing  of the deductive system  and
annotated  traces  of several  of the   deductions  performed by  the system.}
}

@techreport{aic-tn-1972:73,
number=73,
price="$20.00",
institution="AI Center, SRI International",
address="333 Ravenswood Ave., Menlo Park, CA 94025",
title="Qa4:  A Procedural Calculus For Intuitive Reasoning",
author={J. F. Rulifson and J. A. Derksen and R. J. Waldinger},
month="November",
year=1972,
keywords={QA4},
abstract={
  This report presents a language, called QA4, designed to facilitate
the construction  of problem-solving systems used  for  robot planning, theorem
proving, and automatic  program synthesis and  verification.  QA4 integrates an
omega-order logic  language with canonical  composition, associative retrieval,
and  pattern     matching  of expressions;     process-structure   programming;
goal-directed searching; and demons.  Thus it provides many useful  programming
aids.  More importantly, however, it provides  a semantic framework for  common
sense reasoning about these problem domains.  The interpreter for the  language
is extraordinary general, and is therefore an adaptable tool for developing the
specialized   techniques of    intuitive,  symbolic  reasoning   used   by  the
intelligence system.

     Chapter  One is  an  informal  introduction to the    unusual  programming
concepts  available  in the  QA4  language.  Chapter  Two  is a primer for  the
language.  It  informally presents the  language through the  use of  examples.

Most of the unusual or complicated features of the language are  not discussed.
The chapter concludes with a presentation of a small robot planning system that
uses  only the   language features  presented in the    chapter.  Chapter Three
presents a  series of  examples chosen to    illustrate solutions to  automatic
programming problems.  The QA4 programs used in Chapter Three rely on  language
features  not presented in  the primer.  They  are, however,  explained as they
occur.  These programs illustrate  most   of the programming   concepts    just
discussed.  Chapter  Four  is a complete reference  guide to  the language.  It
provides the semantics of all  the features of the language  together with many
implementation notes and design  rationale.  Chapter  Five discusses extensions
to  the  language  that  will  probably   be  done during  the   next   year.}
}

